{"title":"120B runs awesome on just 8GB VRAM!","byline":"Wrong-Historian","lang":"en","content":"<div id=\"readability-page-1\" class=\"page\"><div><p>Here is the thing, the expert layers run amazing on CPU  (<del>~17T/s</del> 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .</p>\n\n<p>You can offload just the attention layers to GPU  (requiring about 5 to 8GB of VRAM) for fast prefill.</p>\n\n<ul>\n<li>KV cache for the sequence</li>\n<li>Attention weights &amp; activations</li>\n<li>Routing tables</li>\n<li>LayerNorms and other “non-expert” parameters</li>\n</ul>\n\n<p>No giant MLP weights are resident on the GPU, so memory use stays low.</p>\n\n<p>This yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.</p>\n\n<p>64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)</p>\n\n<blockquote>\n<p>prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)</p>\n\n<p>eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)</p>\n</blockquote>\n\n<p>with 5GB of vram usage!</p>\n\n<p>Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.</p>\n\n<p>edit: with this latest PR: <a href=\"https://github.com/ggml-org/llama.cpp/pull/15157\">https://github.com/ggml-org/llama.cpp/pull/15157</a></p>\n\n<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \\\n    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n    --n-cpu-moe 36 \\    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.\n    --n-gpu-layers 999 \\   #everything else on the GPU, about 8GB\n    -c 0 -fa \\   #max context (128k), flash attention\n    --jinja --reasoning-format none \\\n    --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n\n\n\nprompt eval time =   94593.62 ms / 12717 tokens (    7.44 ms per token,   134.44 tokens per second)\n       eval time =   76741.17 ms /  1966 tokens (   39.03 ms per token,    25.62 tokens per second)\n</code></pre>\n\n<p>Hitting above 25T/s with only 8GB VRAM use!</p>\n\n<p>Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :</p>\n\n<pre><code>~/build/llama.cpp/build-cuda/bin/llama-server \\\n    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n    --n-cpu-moe 28 \\\n    --n-gpu-layers 999 \\\n    -c 0 -fa \\\n    --jinja --reasoning-format none \\\n    --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n\nprompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)\n       eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)\n</code></pre>\n\n<p>Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!</p>\n</div></div>","textContent":"Here is the thing, the expert layers run amazing on CPU  (~17T/s 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .\n\nYou can offload just the attention layers to GPU  (requiring about 5 to 8GB of VRAM) for fast prefill.\n\n\nKV cache for the sequence\nAttention weights & activations\nRouting tables\nLayerNorms and other “non-expert” parameters\n\n\nNo giant MLP weights are resident on the GPU, so memory use stays low.\n\nThis yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.\n\n64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)\n\n\nprompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)\n\neval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)\n\n\nwith 5GB of vram usage!\n\nHonestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.\n\nedit: with this latest PR: https://github.com/ggml-org/llama.cpp/pull/15157\n\n~/build/llama.cpp/build-cuda/bin/llama-server \\\n    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n    --n-cpu-moe 36 \\    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.\n    --n-gpu-layers 999 \\   #everything else on the GPU, about 8GB\n    -c 0 -fa \\   #max context (128k), flash attention\n    --jinja --reasoning-format none \\\n    --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n\n\n\nprompt eval time =   94593.62 ms / 12717 tokens (    7.44 ms per token,   134.44 tokens per second)\n       eval time =   76741.17 ms /  1966 tokens (   39.03 ms per token,    25.62 tokens per second)\n\n\nHitting above 25T/s with only 8GB VRAM use!\n\nCompared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :\n\n~/build/llama.cpp/build-cuda/bin/llama-server \\\n    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n    --n-cpu-moe 28 \\\n    --n-gpu-layers 999 \\\n    -c 0 -fa \\\n    --jinja --reasoning-format none \\\n    --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n\nprompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)\n       eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)\n\n\nHonestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!\n","length":2778,"excerpt":"Here is the thing, the expert layers run amazing on CPU (~~\\~17T/s~~ 25T/s on a 14900K) and you can force that with this new llama-cpp option:...","siteName":"reddit","url":"https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"}