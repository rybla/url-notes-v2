{"title":"LLMs aren’t world models","lang":"en","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n<p>I believe that language models aren’t world models. It’s a weak claim — I’m not saying they’re useless, or that we’re done\nmilking them. It’s also a fuzzy-sounding claim — with its trillion weights, who can prove that there’s something an LLM isn't a\nmodel of? But I hope to make my claim clear and persuasive enough with some examples.</p>\n<p>A friend who plays better chess than me — and knows more math &amp; CS than me - said that he played some moves against a\nnewly released LLM, and it must be at least as good as him. I said, no way, I’m going to cRRRush it, in my best Russian accent.\nI make a few moves – but unlike him, I don't make good moves<a href=\"#fn1\" id=\"fnref1\" role=\"doc-noteref\"><sup>1</sup></a>, which would be opening book moves it has seen a million times; I make weak moves, which it\nhasn't <a href=\"#fn2\" id=\"fnref2\" role=\"doc-noteref\"><sup>2</sup></a>. The thing makes decent moves in\nresponse, with cheerful commentary about how we're attacking this and developing that — until about move 10, when it tries to\nmove a knight which isn't there, and loses in a few more moves. This was a year or two ago; I’ve just tried this again, and it\nlost track of the board state by move 9.</p>\n<p>When I’m saying that LLMs have no world model, I don’t mean that they haven't seen enough photos of chess knights, or held a\nknight in their greasy fingers; I don’t mean the physical world, necessarily. And I obviously don’t mean that a machine can’t\nlearn a model of chess, when all leading chess engines use machine learning. I only mean that, <strong>having read a trillion\nchess games, <em>LLMs</em>, specifically, have not learned that to make legal moves, you need to know where the pieces are on\nthe board</strong>. Why would they? For predicting the moves or commentary in chess games, which is what they’re optimized for,\nthis would help very marginally, if at all.</p>\n<p>Of course, nobody uses LLMs as chess engines — so whatever they did learn about chess, they learned entirely “by accident”,\nwithout any effort by developers to improve the process for this kind of data. And we could say that the whole argument that\nLLMs learn about the world is that they <em>have </em>to understand the world <em>as a side effect of modeling the distribution\nof text</em> — which is soundly refuted by them literally failing to learn the first thing about chess. But maybe we could\ncharitably assume that LLMs fail this badly with chess for silly reasons you could easily fix, but nobody bothered. So let’s\nlook at something virtual enough to learn a model of without having greasy fingers to touch it with, but also relevant enough\nfor developers to try to make it work.</p>\n<p>So, for my second example, we will consider the so-called “normal blending mode” in image editors like <a href=\"https://krita.org/\">Krita</a> — what happens when you put a layer with some partially transparent pixels on top of another\nlayer? What’s the mathematical formula for blending 2 layers? An LLM replied roughly like so:</p>\n<blockquote>\n<p>In Krita Normal blending mode, colors are <strong>not blended using a mathematical formula</strong>. The \"Normal\" mode\n<strong>simply</strong> displays the upper layer's color, <strong>potentially affected</strong> by its transparency,\n<strong>without any interaction or calculation</strong> with the base layer's color. <em>(It then said how other blending modes\nwere different and involved mathematical formulas.)</em></p>\n</blockquote>\n<p>This answer tells us the LLM doesn't know things such as:</p>\n<ul>\n<li>Computers work with numbers. A color is represented by a number in a computer.</li>\n<li>Therefore, a color cannot be blended by something other than a mathematical formula — nor can it be “affected” without a\n“calculation” by transparency, another number.</li>\n<li>“Transparency” is when you can see through something.</li>\n<li>“Seeing” works by sampling the color at various points, and processing that signal.</li>\n<li>Therefore, if you can see something through something, like, say, a base layer through an upper layer, then by definition,\nthe color you will see is affected not only by the color of the upper layer and its degree of transparency, but also by the\ncolor of the base layer — or you wouldn’t be <em>seeing</em> the base layer, which means that the upper layer is <em>not at all\n</em>transparent, because you’re not seeing through it.</li>\n</ul>\n<p>I mean, it sounds stupid to break it down like that, but I’m not wrong, am I? It really doesn’t know any of these things,\ndoes it.</p>\n<p>Can you prompt the LLM to explain <a href=\"https://en.wikipedia.org/wiki/Alpha_compositing\">alpha blending</a> properly?\nSure. But that just shows the LLM knows to put the words explaining it after the words asking the question. This capability does\nnot make the answer above into lesser evidence of the LLM not knowing <em>the things </em>as opposed to <em>the words.</em></p>\n<p>And of course people can be like that, too - eg <a href=\"https://danluu.com/algorithms-interviews/\">much better at the big O\nnotation and complexity analysis in interviews than on the job</a>. But I guarantee you that <a href=\"https://yosefk.com/blog/the-cardinal-programming-jokes.html#expanding-your-skill-set\">if you put a gun to their head or\noffer them a million dollar bonus for getting it right</a>, they will do well enough on the job, too. And with 200 billion\nthrown at LLM hardware last year, the thing can't complain that it wasn't incentivized to perform<a href=\"#fn3\" id=\"fnref3\" role=\"doc-noteref\"><sup>3</sup></a>.</p>\n<p>Of course, these are simple examples. An LLM triumphalist will observe that they often stop reproducing; an LLM denialist\nwill assume they stopped reproducing through some conspiracy, like a chess engine tool having been given to the LLM, or it\nhaving been drenched with synthetic data similar to your question. (I used to ask LLMs to prove 2+2=4; they'd very pompously\nenumerate various notable properties of 2 and 4, and proudly declare that 2+2 must equal 4 based on these properties, and I had\na good laugh. Then LLMs were flogged to become “good at math,” and now they might say something about “Peano axioms,” and some\ntotal garbage about set theory — but they emit enough S(S(2)) and such that it probably counts as a proof, though I am yet to\nsee the simple “2+2 = 2+(1+1) = (2+1)+1 = 3+1 = 4” which I’d expect from an entity understanding the question.)</p>\n<p>For a more complex example, we can take associativity (which, as we’ve seen in 2+2=4, LLMs understand vaguely at best),\ncombine it with alpha blending and transparency (which apparently they don’t understand at all), and see how well LLMs do. I’ve\nhad an exchange with an LLM asking whether alpha blending, as implemented in commonly used libraries, is associative, or whether\nit isn’t due to precision loss or whatever — and if it’s not associative, how does caching work in drawing programs (where the\nprogram must be precomputing the blending of the layers above and below the currently edited one, to avoid recomputing the\nblending of 10 or 100 layers upon every brush stroke.)</p>\n<p>Sure enough, it said that alpha blending wasn’t associative — probably because I suggested that it might not be — and that\nthis is “solved with caching instead of mathematical elegance” — probably because I suggested that caching was involved. And\nthen I ask, but how can caching work if blending is not associative? If layer 6 is selected, and you blend the cached blending\nof {1…5}, the selected layer 6, and the cached blending of {7…10}, you would get different results from blending {1…4}, 5, and\n{6…10}, if blending is not in fact associative? And then if you selected layer 5 in the program, you would see a different\npicture compared to selecting layer 6 - but in practice you see the same picture?</p>\n<p>“You got me,” says the LLM, more or less. So their not knowing what any of the words actually mean very much does extend to\ncomplex examples.</p>\n<p>You could say that the LLM was a victim of its agreeableness<a href=\"#fn4\" id=\"fnref4\" role=\"doc-noteref\"><sup>4</sup></a>, since it might have been influenced by my contradictory implications that blending might\nnot be associative, yet caching must be implemented that counts on it being associative. I could say that, well, my whole\nquestion was about which parts of my suspicions are incorrect, and saying they’re all correct is an abject failure — but let’s\nassume it could be a character flaw more than an intellectual weakness. So in our last example, we’ll see the LLM having its own\nopinion and sticking to it, despite being told repeatedly that it can’t be true.</p>\n<p>I ask it about the thread safety of appending to a Python list from multiple threads, and whether I can tell the number of\ntimes append was called with len(myList), and whether it will work once the <a href=\"https://en.wikipedia.org/wiki/Global_interpreter_lock\">GIL</a> is <a href=\"https://peps.python.org/pep-0703/\">removed</a>.\nIt says that without the GIL, the program could corrupt memory. I say, no way, this is not C, it must be more like Java? And it\ngoes, no, <em>CPython is a C program</em>, and without the GIL your racy code can crash like C does. Java is different, <em>it\nhas a memory model</em>, and look at these crash reports from GIL-less Python. And I’m like, but these are bug reports, it’s not\n<em>by design</em>, is there evidence that this is by design? — and it goes, it’s too early for the kind of evidence you’re\nlooking for to exist, no-GIL is too new, but here’s how a C program could crash in such scenarios… and on and on and on.</p>\n<p>It does not know that <a href=\"https://yosefk.com/blog/a-100x-speedup-with-unsafe-python.html\">(pure) Python is a memory-safe\nlanguage</a>, and that no suggestion making it memory-unsafe would ever be accepted, and I found no way of persuading it to take\nthis notion into account — or to acknowledge that the evidence it’s citing in support of its claims is more like evidence to the\ncontrary (if all the crashes upon races you find are bug reports, it points to the requirement being that races don’t lead to\ncrashes.)</p>\n<p>So it can be either kinda agreeable or very stubborn — and it might obviously not know what it just said in both modes.</p>\n<p><strong>Can this be quantified?</strong></p>\n<p>I don't see how.</p>\n<p>I mean, I wish it could be. It's clear that LLMs do learn <em>some</em> things about the world. For instance, even just the\ntoken embeddings contain the representation of the concept of gender learned without any specific effort to teach the model what\ngender is, as evidenced by “king - man + woman ~= queen” in the embedding space.</p>\n<p>Ideally, you would want to quantify \"how much of the world LLMs model.\" But even if you resolve the difficulty of defining\nwhat this means, you'll run into the ease with which LLMs memorize answers to specific questions, so the vendor can celebrate\nthe new bar having been cleared.</p>\n<p>All I can confidently claim is that they don't learn a world model except by accident, and there's neither a theoretical\nreason nor empirical evidence for your being able to count on this accident in any defined and broad set of circumstances.</p>\n<p><strong>So-called conclusions<a href=\"#fn5\" id=\"fnref5\" role=\"doc-noteref\"><sup>5</sup></a></strong></p>\n<p>A guy who made $100 million from being an early employee of some startup came to give a lecture for that startup, and said “a\nfundamentally incorrect approach to a problem can be taken very far in practice with sufficient engineering effort.” (He then\ncheered up his listeners, most of whom had $100 million less than him, with the addendum “That's what I think is happening in\nthis company!”)</p>\n<p>It is therefore not one of my conclusions that you can’t take LLMs very, very far just because they demonstrably do not learn\na model of the many worlds described by the words they’re trained on (which, BTW, is exactly as it says on the tin; nobody ever\ncalled them LWMs.) I will, however, predict a few things — something you shouldn’t do if you don’t want to look stupid in the\nfuture, but here goes.</p>\n<p><strong>There will be at least one big breakthrough in machine learning around “world models”</strong>. I have no idea what\nthis breakthrough will look like; I predict that it will happen because some important kinds of thinking cannot be done without\nit, and I trust the Church-Turing thesis when it comes to these kinds of thinking, and I think someone will figure this out,\nsame as people have come up with deep learning, convnets and transformers. And of course you already have “world models”, such\nas systems recovering object classes and positions from images — by a breakthrough, I mean a “generic” ability to build models\nof “novel worlds” (even if the model isn’t as good as a specially tailored one), much like you throw any text into an LLM and\nhave it learn “something” without much tuning for this kind of text.</p>\n<p>(In fact, I would guess there will be at least 2 more breakthroughs, the other one being around needing far less training\ndata — again, not because I know how machines could use less training data, but because I know you and I get by with less.\nFeeding these algorithms gobs of data is another example of how an approach that must be fundamentally incorrect at least in\nsome sense, as evidenced by how data-hungry it is, can be taken very far by engineering efforts — as long as something is useful\nenough to fund such efforts and isn’t outcompeted by a new idea, it can persist.)</p>\n<p><strong>LLMs are not by themselves sufficient as a path to general machine intelligence; in some sense they are a distraction\nbecause of how far you can take them despite the approach being fundamentally incorrect</strong>. This should make “AI risk”\npeople happy; but “AI risk” is its own hilarity best left to another time.</p>\n<p><strong>LLMs will never<a href=\"#fn6\" id=\"fnref6\" role=\"doc-noteref\"><sup>6</sup></a> manage to deal\nwith large code bases “autonomously”</strong>, because they would need to have a model of the program, and they don’t even learn\nto track chess pieces having read everything there is to read about chess.</p>\n<p><strong>LLMs will never reliably know what they don’t know, or stop making things up. </strong>You need some sort of a world\nmodel to have notions of knowledge, truth and falsehood. Any mechanism that is supposed to make LLMs “safe”, trustworthy or\nother such is a mix of snake oil and honest efforts to somehow steer it away from text that spooks users — which can be done,\nsince users are spooked by form more than substance. For example, when it says some politically related nonsense, people drag it\nfor having the wrong politics, and you can “fix” it by making its output less politically charged — without making it less\nnonsensical, which there’s no way to reliably achieve.</p>\n<p><strong>LLMs will always be able to teach a student complex (standard) curriculum, answer an expert’s question with a useful\n(known) insight, and yet fail at basic (novel) questions on the same subject, all at the same time</strong>. This is not\nsurprising — this is exactly what you would expect from a language model that isn’t a world model. This fuzzy insight is more\ncute than useful, however, since it’s hard to know what is and is not novel — in part because you come to the LLM in the first\nplace with things you don’t already know everything about.</p>\n<p>(Some sources, such as <a href=\"https://mindmatters.ai/2025/01/some-lessons-from-deepseek-compared-with-other-chatbots/\">this\nwonderful writeup about LLMs not knowing that rotating a tic-tac-toe board doesn’t change the game</a>, take this point to its\nlogical conclusion: “<em>If you know the answer, you don’t need to ask an LLM; if you don’t know the answer, you can’t trust an\nLLM.</em>” But this conveys a true insight into LLMs together with unwarranted pessimism about their utility. In fact, sometimes\nyou know the answer, but it’s quicker to proofread an LLM’s output than to type it out; sometimes you don’t know the answer, but\nknow to check if the LLM’s answer is correct, etc. etc.)</p>\n<p><strong>LLM-style language processing is definitely a part of how human intelligence works — and how human stupidity works.\n</strong>I agree with Dijkstra that “can machines think?” and “can submarines swim?” are poor questions to ask, and I hate it\nwhen people say that neural networks “work like the brain” and such. But I can’t help feeling that LLMs are a mirror into a part\nof how people such as myself think — and I don’t like what I’m seeing in that mirror. “Thinking” by guessing what words to say\nnext based on words we’ve previously heard might actually help find a good idea — and it’s also how know-nothings get through\nwork meetings, and how people come to think they know stuff they really don’t, and how they internalize the stupidest notions. I\nam starting to think that in today’s environment, high cognitive skills are an actual risk factor for stupidity, and that\nlearning words without learning a model of what they refer to is one big part of the problem.</p>\n<p><strong>P.S.</strong> I wish I could say something about how to best use LLMs for programming — something I would like to be\nqualified to speak about, and that I am kinda supposed to learn enough to be qualified to speak about. I don’t think I am; I can\nonly say that I tried Cursor and it failed every time, including at replacing f(obj) and g(obj) with obj.f() and obj.g() (it\nwould occasionally mix up f and g, and I got tired of reviewing its output), and I went back to simply copying code into and out\nof chat windows. I would say that I use LLMs like I use SIMD — sometimes it’s a good fit for leaf functions whose behavior is\nrelatively easy to specify and test, and it has no business being anywhere else.</p>\n<p>I have conflicting theories about why some people do great things with “agentic AI” while I think it’s hopelessly useless for\nme; I am waiting for someone to write something crisp and well-researched about this to teach me the truth, or a useful\napproximation. I console myself with the idea that I can’t be missing out on too much, given how terrible the output I’m getting\nfrom LLMs often is.</p>\n<p><strong>P.P.S. </strong>Here’s a somewhat rough Soviet-era joke about a school for kids with special needs that illustrates\nmy point. Rachel says that a Russian joke isn’t a joke, but a story of pain. To this I reply that some people like them.</p>\n<blockquote>\n<p>An inspector comes to a school for kids with developmental issues. He asks a kid riding a wooden horsie his name, and the kid\nsays “MMMM.” He says, what do you want to be when you grow up? — and the kid says “MMMM.” The inspector turns to the principal\nand says, “you’re doing nothing for these kids. I’ll be back in a month — if there’s no improvement, we’ll close the\nschool.”</p>\n<p>He comes back a month later and finds the kid swinging on the wooden horsie, same as last time; if you want to tell this\nmasterpiece of Soviet humor at parties — the perfect conversation starter — you should be swinging wildly when saying the kid’s\nlines:</p>\n<p>— What’s your name?<br> — MMMMikey!!<br> — Mikey? Nice to meet you, Mikey. What do you want to be when you grow up,\nMikey?<br> — MMMMastrounaut!! <br> — An astronaut? Good stuff, good stuff! And how old are you? <br> — MMMMikey!! <br></p>\n</blockquote>\n<p>The moral of the story being that you can learn to predict the next word without learning much about the world — at least up\nto a point.</p>\n<p><em>Thanks to Dan Luu for reviewing a draft of this post.</em></p>\n</div></div>","textContent":"\nI believe that language models aren’t world models. It’s a weak claim — I’m not saying they’re useless, or that we’re done\nmilking them. It’s also a fuzzy-sounding claim — with its trillion weights, who can prove that there’s something an LLM isn't a\nmodel of? But I hope to make my claim clear and persuasive enough with some examples.\nA friend who plays better chess than me — and knows more math & CS than me - said that he played some moves against a\nnewly released LLM, and it must be at least as good as him. I said, no way, I’m going to cRRRush it, in my best Russian accent.\nI make a few moves – but unlike him, I don't make good moves1, which would be opening book moves it has seen a million times; I make weak moves, which it\nhasn't 2. The thing makes decent moves in\nresponse, with cheerful commentary about how we're attacking this and developing that — until about move 10, when it tries to\nmove a knight which isn't there, and loses in a few more moves. This was a year or two ago; I’ve just tried this again, and it\nlost track of the board state by move 9.\nWhen I’m saying that LLMs have no world model, I don’t mean that they haven't seen enough photos of chess knights, or held a\nknight in their greasy fingers; I don’t mean the physical world, necessarily. And I obviously don’t mean that a machine can’t\nlearn a model of chess, when all leading chess engines use machine learning. I only mean that, having read a trillion\nchess games, LLMs, specifically, have not learned that to make legal moves, you need to know where the pieces are on\nthe board. Why would they? For predicting the moves or commentary in chess games, which is what they’re optimized for,\nthis would help very marginally, if at all.\nOf course, nobody uses LLMs as chess engines — so whatever they did learn about chess, they learned entirely “by accident”,\nwithout any effort by developers to improve the process for this kind of data. And we could say that the whole argument that\nLLMs learn about the world is that they have to understand the world as a side effect of modeling the distribution\nof text — which is soundly refuted by them literally failing to learn the first thing about chess. But maybe we could\ncharitably assume that LLMs fail this badly with chess for silly reasons you could easily fix, but nobody bothered. So let’s\nlook at something virtual enough to learn a model of without having greasy fingers to touch it with, but also relevant enough\nfor developers to try to make it work.\nSo, for my second example, we will consider the so-called “normal blending mode” in image editors like Krita — what happens when you put a layer with some partially transparent pixels on top of another\nlayer? What’s the mathematical formula for blending 2 layers? An LLM replied roughly like so:\n\nIn Krita Normal blending mode, colors are not blended using a mathematical formula. The \"Normal\" mode\nsimply displays the upper layer's color, potentially affected by its transparency,\nwithout any interaction or calculation with the base layer's color. (It then said how other blending modes\nwere different and involved mathematical formulas.)\n\nThis answer tells us the LLM doesn't know things such as:\n\nComputers work with numbers. A color is represented by a number in a computer.\nTherefore, a color cannot be blended by something other than a mathematical formula — nor can it be “affected” without a\n“calculation” by transparency, another number.\n“Transparency” is when you can see through something.\n“Seeing” works by sampling the color at various points, and processing that signal.\nTherefore, if you can see something through something, like, say, a base layer through an upper layer, then by definition,\nthe color you will see is affected not only by the color of the upper layer and its degree of transparency, but also by the\ncolor of the base layer — or you wouldn’t be seeing the base layer, which means that the upper layer is not at all\ntransparent, because you’re not seeing through it.\n\nI mean, it sounds stupid to break it down like that, but I’m not wrong, am I? It really doesn’t know any of these things,\ndoes it.\nCan you prompt the LLM to explain alpha blending properly?\nSure. But that just shows the LLM knows to put the words explaining it after the words asking the question. This capability does\nnot make the answer above into lesser evidence of the LLM not knowing the things as opposed to the words.\nAnd of course people can be like that, too - eg much better at the big O\nnotation and complexity analysis in interviews than on the job. But I guarantee you that if you put a gun to their head or\noffer them a million dollar bonus for getting it right, they will do well enough on the job, too. And with 200 billion\nthrown at LLM hardware last year, the thing can't complain that it wasn't incentivized to perform3.\nOf course, these are simple examples. An LLM triumphalist will observe that they often stop reproducing; an LLM denialist\nwill assume they stopped reproducing through some conspiracy, like a chess engine tool having been given to the LLM, or it\nhaving been drenched with synthetic data similar to your question. (I used to ask LLMs to prove 2+2=4; they'd very pompously\nenumerate various notable properties of 2 and 4, and proudly declare that 2+2 must equal 4 based on these properties, and I had\na good laugh. Then LLMs were flogged to become “good at math,” and now they might say something about “Peano axioms,” and some\ntotal garbage about set theory — but they emit enough S(S(2)) and such that it probably counts as a proof, though I am yet to\nsee the simple “2+2 = 2+(1+1) = (2+1)+1 = 3+1 = 4” which I’d expect from an entity understanding the question.)\nFor a more complex example, we can take associativity (which, as we’ve seen in 2+2=4, LLMs understand vaguely at best),\ncombine it with alpha blending and transparency (which apparently they don’t understand at all), and see how well LLMs do. I’ve\nhad an exchange with an LLM asking whether alpha blending, as implemented in commonly used libraries, is associative, or whether\nit isn’t due to precision loss or whatever — and if it’s not associative, how does caching work in drawing programs (where the\nprogram must be precomputing the blending of the layers above and below the currently edited one, to avoid recomputing the\nblending of 10 or 100 layers upon every brush stroke.)\nSure enough, it said that alpha blending wasn’t associative — probably because I suggested that it might not be — and that\nthis is “solved with caching instead of mathematical elegance” — probably because I suggested that caching was involved. And\nthen I ask, but how can caching work if blending is not associative? If layer 6 is selected, and you blend the cached blending\nof {1…5}, the selected layer 6, and the cached blending of {7…10}, you would get different results from blending {1…4}, 5, and\n{6…10}, if blending is not in fact associative? And then if you selected layer 5 in the program, you would see a different\npicture compared to selecting layer 6 - but in practice you see the same picture?\n“You got me,” says the LLM, more or less. So their not knowing what any of the words actually mean very much does extend to\ncomplex examples.\nYou could say that the LLM was a victim of its agreeableness4, since it might have been influenced by my contradictory implications that blending might\nnot be associative, yet caching must be implemented that counts on it being associative. I could say that, well, my whole\nquestion was about which parts of my suspicions are incorrect, and saying they’re all correct is an abject failure — but let’s\nassume it could be a character flaw more than an intellectual weakness. So in our last example, we’ll see the LLM having its own\nopinion and sticking to it, despite being told repeatedly that it can’t be true.\nI ask it about the thread safety of appending to a Python list from multiple threads, and whether I can tell the number of\ntimes append was called with len(myList), and whether it will work once the GIL is removed.\nIt says that without the GIL, the program could corrupt memory. I say, no way, this is not C, it must be more like Java? And it\ngoes, no, CPython is a C program, and without the GIL your racy code can crash like C does. Java is different, it\nhas a memory model, and look at these crash reports from GIL-less Python. And I’m like, but these are bug reports, it’s not\nby design, is there evidence that this is by design? — and it goes, it’s too early for the kind of evidence you’re\nlooking for to exist, no-GIL is too new, but here’s how a C program could crash in such scenarios… and on and on and on.\nIt does not know that (pure) Python is a memory-safe\nlanguage, and that no suggestion making it memory-unsafe would ever be accepted, and I found no way of persuading it to take\nthis notion into account — or to acknowledge that the evidence it’s citing in support of its claims is more like evidence to the\ncontrary (if all the crashes upon races you find are bug reports, it points to the requirement being that races don’t lead to\ncrashes.)\nSo it can be either kinda agreeable or very stubborn — and it might obviously not know what it just said in both modes.\nCan this be quantified?\nI don't see how.\nI mean, I wish it could be. It's clear that LLMs do learn some things about the world. For instance, even just the\ntoken embeddings contain the representation of the concept of gender learned without any specific effort to teach the model what\ngender is, as evidenced by “king - man + woman ~= queen” in the embedding space.\nIdeally, you would want to quantify \"how much of the world LLMs model.\" But even if you resolve the difficulty of defining\nwhat this means, you'll run into the ease with which LLMs memorize answers to specific questions, so the vendor can celebrate\nthe new bar having been cleared.\nAll I can confidently claim is that they don't learn a world model except by accident, and there's neither a theoretical\nreason nor empirical evidence for your being able to count on this accident in any defined and broad set of circumstances.\nSo-called conclusions5\nA guy who made $100 million from being an early employee of some startup came to give a lecture for that startup, and said “a\nfundamentally incorrect approach to a problem can be taken very far in practice with sufficient engineering effort.” (He then\ncheered up his listeners, most of whom had $100 million less than him, with the addendum “That's what I think is happening in\nthis company!”)\nIt is therefore not one of my conclusions that you can’t take LLMs very, very far just because they demonstrably do not learn\na model of the many worlds described by the words they’re trained on (which, BTW, is exactly as it says on the tin; nobody ever\ncalled them LWMs.) I will, however, predict a few things — something you shouldn’t do if you don’t want to look stupid in the\nfuture, but here goes.\nThere will be at least one big breakthrough in machine learning around “world models”. I have no idea what\nthis breakthrough will look like; I predict that it will happen because some important kinds of thinking cannot be done without\nit, and I trust the Church-Turing thesis when it comes to these kinds of thinking, and I think someone will figure this out,\nsame as people have come up with deep learning, convnets and transformers. And of course you already have “world models”, such\nas systems recovering object classes and positions from images — by a breakthrough, I mean a “generic” ability to build models\nof “novel worlds” (even if the model isn’t as good as a specially tailored one), much like you throw any text into an LLM and\nhave it learn “something” without much tuning for this kind of text.\n(In fact, I would guess there will be at least 2 more breakthroughs, the other one being around needing far less training\ndata — again, not because I know how machines could use less training data, but because I know you and I get by with less.\nFeeding these algorithms gobs of data is another example of how an approach that must be fundamentally incorrect at least in\nsome sense, as evidenced by how data-hungry it is, can be taken very far by engineering efforts — as long as something is useful\nenough to fund such efforts and isn’t outcompeted by a new idea, it can persist.)\nLLMs are not by themselves sufficient as a path to general machine intelligence; in some sense they are a distraction\nbecause of how far you can take them despite the approach being fundamentally incorrect. This should make “AI risk”\npeople happy; but “AI risk” is its own hilarity best left to another time.\nLLMs will never6 manage to deal\nwith large code bases “autonomously”, because they would need to have a model of the program, and they don’t even learn\nto track chess pieces having read everything there is to read about chess.\nLLMs will never reliably know what they don’t know, or stop making things up. You need some sort of a world\nmodel to have notions of knowledge, truth and falsehood. Any mechanism that is supposed to make LLMs “safe”, trustworthy or\nother such is a mix of snake oil and honest efforts to somehow steer it away from text that spooks users — which can be done,\nsince users are spooked by form more than substance. For example, when it says some politically related nonsense, people drag it\nfor having the wrong politics, and you can “fix” it by making its output less politically charged — without making it less\nnonsensical, which there’s no way to reliably achieve.\nLLMs will always be able to teach a student complex (standard) curriculum, answer an expert’s question with a useful\n(known) insight, and yet fail at basic (novel) questions on the same subject, all at the same time. This is not\nsurprising — this is exactly what you would expect from a language model that isn’t a world model. This fuzzy insight is more\ncute than useful, however, since it’s hard to know what is and is not novel — in part because you come to the LLM in the first\nplace with things you don’t already know everything about.\n(Some sources, such as this\nwonderful writeup about LLMs not knowing that rotating a tic-tac-toe board doesn’t change the game, take this point to its\nlogical conclusion: “If you know the answer, you don’t need to ask an LLM; if you don’t know the answer, you can’t trust an\nLLM.” But this conveys a true insight into LLMs together with unwarranted pessimism about their utility. In fact, sometimes\nyou know the answer, but it’s quicker to proofread an LLM’s output than to type it out; sometimes you don’t know the answer, but\nknow to check if the LLM’s answer is correct, etc. etc.)\nLLM-style language processing is definitely a part of how human intelligence works — and how human stupidity works.\nI agree with Dijkstra that “can machines think?” and “can submarines swim?” are poor questions to ask, and I hate it\nwhen people say that neural networks “work like the brain” and such. But I can’t help feeling that LLMs are a mirror into a part\nof how people such as myself think — and I don’t like what I’m seeing in that mirror. “Thinking” by guessing what words to say\nnext based on words we’ve previously heard might actually help find a good idea — and it’s also how know-nothings get through\nwork meetings, and how people come to think they know stuff they really don’t, and how they internalize the stupidest notions. I\nam starting to think that in today’s environment, high cognitive skills are an actual risk factor for stupidity, and that\nlearning words without learning a model of what they refer to is one big part of the problem.\nP.S. I wish I could say something about how to best use LLMs for programming — something I would like to be\nqualified to speak about, and that I am kinda supposed to learn enough to be qualified to speak about. I don’t think I am; I can\nonly say that I tried Cursor and it failed every time, including at replacing f(obj) and g(obj) with obj.f() and obj.g() (it\nwould occasionally mix up f and g, and I got tired of reviewing its output), and I went back to simply copying code into and out\nof chat windows. I would say that I use LLMs like I use SIMD — sometimes it’s a good fit for leaf functions whose behavior is\nrelatively easy to specify and test, and it has no business being anywhere else.\nI have conflicting theories about why some people do great things with “agentic AI” while I think it’s hopelessly useless for\nme; I am waiting for someone to write something crisp and well-researched about this to teach me the truth, or a useful\napproximation. I console myself with the idea that I can’t be missing out on too much, given how terrible the output I’m getting\nfrom LLMs often is.\nP.P.S. Here’s a somewhat rough Soviet-era joke about a school for kids with special needs that illustrates\nmy point. Rachel says that a Russian joke isn’t a joke, but a story of pain. To this I reply that some people like them.\n\nAn inspector comes to a school for kids with developmental issues. He asks a kid riding a wooden horsie his name, and the kid\nsays “MMMM.” He says, what do you want to be when you grow up? — and the kid says “MMMM.” The inspector turns to the principal\nand says, “you’re doing nothing for these kids. I’ll be back in a month — if there’s no improvement, we’ll close the\nschool.”\nHe comes back a month later and finds the kid swinging on the wooden horsie, same as last time; if you want to tell this\nmasterpiece of Soviet humor at parties — the perfect conversation starter — you should be swinging wildly when saying the kid’s\nlines:\n— What’s your name? — MMMMikey!! — Mikey? Nice to meet you, Mikey. What do you want to be when you grow up,\nMikey? — MMMMastrounaut!!  — An astronaut? Good stuff, good stuff! And how old are you?  — MMMMikey!! \n\nThe moral of the story being that you can learn to predict the next word without learning much about the world — at least up\nto a point.\nThanks to Dan Luu for reviewing a draft of this post.\n","length":17969,"excerpt":"I believe that language models aren’t world models. It’s a weak claim — I’m not saying they’re useless, or that we’re done\nmilking them. It’s also a fuzzy-sounding claim — with its trillion weights, who can prove that there’s something an LLM isn't a\nmodel of? But I hope to make my claim clear and persuasive enough with some examples.","url":"https://yosefk.com/blog/llms-arent-world-models.html","addedTime":"2025-08-12,16:49"}