{"title":"Training language models to be warm and empathetic makes them less reliable and more sycophantic","byline":"[Submitted on 29 Jul 2025 (v1), last revised 30 Jul 2025 (this version, v2)]","lang":"en","content":"<div id=\"readability-page-1\" class=\"page\"><div id=\"content-inner\">\n    \n    \n                \n    <p><a href=\"https://arxiv.org/pdf/2507.21919\">View PDF</a>\n    <a href=\"https://arxiv.org/html/2507.21919v2\">HTML (experimental)</a></p><blockquote>\n            <span>Abstract:</span>Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.\n    </blockquote>\n\n    <!--CONTEXT-->\n    <div>\n      <table summary=\"Additional metadata\"><tbody><tr>\n          <td>Subjects:</td>\n          <td>\n            <span>Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)</td>\n        </tr><tr>\n          <td>Cite as:</td>\n          <td><span><a href=\"https://arxiv.org/abs/2507.21919\">arXiv:2507.21919</a> [cs.CL]</span></td>\n        </tr>\n        <tr>\n          <td>&nbsp;</td>\n          <td>(or <span>\n              <a href=\"https://arxiv.org/abs/2507.21919v2\">arXiv:2507.21919v2</a> [cs.CL]</span> for this version)\n          </td>\n        </tr>\n        <tr>\n          <td>&nbsp;</td>\n          <td>              <a href=\"https://doi.org/10.48550/arXiv.2507.21919\" id=\"arxiv-doi-link\">https://doi.org/10.48550/arXiv.2507.21919</a><div>\n              <!-- tooltip description -->\n              <p><span></span>                  arXiv-issued DOI via DataCite</p>\n            </div>\n          </td>\n        </tr></tbody></table>\n    </div>\n  </div><div>\n      <h2>Submission history</h2><p> From: Lujain Ibrahim [<a href=\"https://arxiv.org/show-email/8508a95d/2507.21919\" rel=\"nofollow\">view email</a>]      <br>            <strong><a href=\"https://arxiv.org/abs/2507.21919v1\" rel=\"nofollow\">[v1]</a></strong>\n        Tue, 29 Jul 2025 15:33:20 UTC (1,073 KB)<br>\n    <strong>[v2]</strong>\n        Wed, 30 Jul 2025 10:11:59 UTC (1,071 KB)<br>\n</p></div></div>","textContent":"\n    \n    \n                \n    View PDF\n    HTML (experimental)\n            Abstract:Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)\n        \n          Cite as:\n          arXiv:2507.21919 [cs.CL]\n        \n        \n           \n          (or \n              arXiv:2507.21919v2 [cs.CL] for this version)\n          \n        \n        \n           \n                        https://doi.org/10.48550/arXiv.2507.21919\n              \n                                arXiv-issued DOI via DataCite\n            \n          \n        \n    \n  \n      Submission history From: Lujain Ibrahim [view email]                  [v1]\n        Tue, 29 Jul 2025 15:33:20 UTC (1,073 KB)\n    [v2]\n        Wed, 30 Jul 2025 10:11:59 UTC (1,071 KB)\n","length":2185,"excerpt":"Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.","siteName":"arXiv.org","url":"https://arxiv.org/abs/2507.21919","addedTime":"2025-08-12,16:46"}