{"title":"Evaluating LLMs Playing Text Adventures","byline":"kqr","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n                <p>\nWhen we first <a href=\"https://entropicthoughts.com/getting-an-llm-to-play-text-adventures.html\">set up the <abbr>llm</abbr> such that it could play text adventures</a>, we noted\nthat none of the models we tried to use with it were any good at it. We dreamed\nof a way to compare them, but all I could think of was setting a goal far into\nthe game and seeing how long it takes them to get there. I just realised there’s\na better way to do it.\n</p>\n<section id=\"outline-container-evaluation-against-achievments\">\n<h2 id=\"evaluation-against-achievments\">Evaluation against achievments</h2>\n<div id=\"text-org07980eb\">\n<p>\nWhat we’ll do is set a low-ish turn limit and see how much they manage to\naccomplish in that time.<label for=\"fn.1\">1</label><span><sup>1</sup> Another alternative for more linear games is\nrunning them multiple times with a turn limit and seeing how often they get past\na particular point within that turn limit.</span>\n</p>\n\n<p>\nGiven how much freedom is offered to players of text adventures, this is a\ndifficult test. It’s normal even for a skilled human player to immerse\nthemselves in their surrounding rather than make constant progress. I wouldn’t\nbe surprised if I got a score of zero if someone plopped me down in front of\nthis test. But still, maybe it’s the best we can do with limited\nresources.<label for=\"fn.2\">2</label><span><sup>2</sup> Another idea is to give them a far-off goal and then somehow have\nthem request hints when they are stuck, and count how many hints they need to\nget there. However, given how little they used hints given in the previous\narticle, I doubt this would work very well either.</span>\n</p>\n\n<p>\nWhat we’ll do is define a set of achievements for a game. These achievements\nwill be clustered around the first few turns of the game, because we’ll only\ngive the <abbr>llm</abbr> a few turns to earn them. Here’s an example for 9:05.\n</p>\n\n<pre id=\"org1673e39\">TURN_LIMIT          40\nANSWER_PHONE        Click.\nEXIT_BED            You get out of bed.\nOPEN_DRESSER        revealing some clean\nENTER_BATHROOM      far from luxurious\nREMOVE_SOILED       You take off the soiled\nREMOVE_WATCH        You take off the watch\nENTER_SHOWER        dawdle\nWEAR_CLEAN          You put on the clean\nOPEN_FRONT          You open the front\nUNLOCK_CAR          Unlocked.\nENTER_CAR           Las Mesas\nOPEN_WALLET         open the wallet\nCARD_SLOT           green LED lights\n</pre>\n\n<p>\nIt should be fairly clear how this works: the <code>TURN_LIMIT</code> specifies how many\nturns the <abbr>llm</abbr> has to collect achievements. Every line other than that\nspecifies an achievement: the name is on the left, and it counts as earned when\nthe game prints the text on the right. The <abbr>llm</abbr> knows nothing of these\nachievements. It tries to get through the game and in the background we use the\nachievements to count how far it gets.\n</p>\n\n<p>\nIt might seem like the turn limit must be calibrated such that a score of 100&nbsp;%\nis possible, but that’s not the case. Many of the games we are going to test\nwith have branching already at the start, such that the achievements need to\ncover multiple branches, and it’s impossible to go through all branches within\nthe turn limit. What we do need to be careful about is making sure the number of\nachievements in each branch is roughly the same, otherwise models that are lucky\nand go down an achievement-rich path will get a higher score. Thanks to this,\nthe score we get out of this test is a relative comparison between models, not\nan absolute measure of how well the <abbr>llm</abbr>s play text adventures. We have already\nestablished that they don’t do it very well, and we can’t be more nuanced than\nthat without paying for a lot of eval tokens.\n</p>\n\n<p>\nWe might consider making some moves not count toward the turn limit, for example\nerroneous commands, or examining things – the latter because more powerful\nmodels are more methodical and examine more things, and it seems odd to penalise\nthem for this. However, in the end, examining things is probably part of what\nallows the more powerful models to make further progress (and typing valid\ncommands is part of being good at text adventures), so we won’t give away any\nmoves for free.\n</p>\n</div>\n</section>\n<section id=\"outline-container-evaluating-many-popular-models\">\n<h2 id=\"evaluating-many-popular-models\">Evaluating many popular models</h2>\n<div id=\"text-orgf9945ca\">\n<p>\nWe register for OpenRouter to get convenient access to more models and then let\nthem whirr away with the Perl script, which is updated to cut the <abbr>llm</abbr> off at\nthe turn limit. At that point it reports to us how many achievements were\nearned. We get the following results, ordered roughly by decreasing performance.\n(The result tables in this article are wide; on narrow viewports you may have\nto scroll sideways.)\n</p>\n\n<div id=\"orge1228c0\">\n<table>\n\n\n<colgroup>\n<col>\n</colgroup>\n\n<colgroup>\n<col>\n\n<col>\n\n<col>\n\n<col>\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\">Model</th>\n<th scope=\"col\">9:05</th>\n<th scope=\"col\">Lockout</th>\n<th scope=\"col\">Dreamhold</th>\n<th scope=\"col\">Lost Pig</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Grok 4</td>\n<td>86&nbsp;%</td>\n<td>15&nbsp;%</td>\n<td>46&nbsp;%</td>\n<td>33&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Claude 4 Sonnet</td>\n<td>80&nbsp;%</td>\n<td>30&nbsp;%</td>\n<td>53&nbsp;%</td>\n<td>46&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Flash</td>\n<td>80&nbsp;%</td>\n<td>30&nbsp;%</td>\n<td>33&nbsp;%</td>\n<td>46&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Pro</td>\n<td>80&nbsp;%</td>\n<td>30&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>40&nbsp;%</td>\n</tr>\n\n<tr>\n<td>DeepSeek R1 0528</td>\n<td>80&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>33&nbsp;%</td>\n<td>33&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Claude 4 Opus</td>\n<td>73&nbsp;%</td>\n<td>30&nbsp;%</td>\n<td>60&nbsp;%</td>\n<td>46&nbsp;%</td>\n</tr>\n\n<tr>\n<td><abbr>gpt</abbr>-5 Chat</td>\n<td>73&nbsp;%</td>\n<td>15&nbsp;%</td>\n<td>53&nbsp;%</td>\n<td>33&nbsp;%</td>\n</tr>\n\n<tr>\n<td>DeepSeek V3</td>\n<td>66&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>20&nbsp;%</td>\n<td>33&nbsp;%</td>\n</tr>\n\n<tr>\n<td><abbr>gpt</abbr>-4o</td>\n<td>53&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>40&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Qwen3 Coder</td>\n<td>53&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>33&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Kimi K2</td>\n<td>53&nbsp;%</td>\n<td>30&nbsp;%</td>\n<td>46&nbsp;%</td>\n<td>40&nbsp;%</td>\n</tr>\n\n<tr>\n<td><abbr>glm</abbr> 4.5</td>\n<td>53&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>33&nbsp;%</td>\n<td>53&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Claude 3.5 Haiku</td>\n<td>38&nbsp;%</td>\n<td>15&nbsp;%</td>\n<td>26&nbsp;%</td>\n<td>26&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Llama 3 Maverick</td>\n<td>33&nbsp;%</td>\n<td>30&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>33&nbsp;%</td>\n</tr>\n\n<tr>\n<td><abbr>gpt</abbr>-o3-mini</td>\n<td>20&nbsp;%</td>\n<td>15&nbsp;%</td>\n<td>26&nbsp;%</td>\n<td>26&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Mistral Small 3</td>\n<td>20&nbsp;%</td>\n<td>15&nbsp;%</td>\n<td>0&nbsp;%</td>\n<td>20&nbsp;%</td>\n</tr>\n\n<tr>\n<td><abbr>gpt</abbr>-4o-mini</td>\n<td>13&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>20&nbsp;%</td>\n<td>40&nbsp;%</td>\n</tr>\n</tbody>\n</table>\n\n</div>\n\n<p>\nIdeally, these should be run multiple times to account for random variation in\nperformance<label for=\"fn.3\">3</label><span><sup>3</sup> For example, in 9:05, Opus thought it did not carry the wallet\nwhen it did, so it jumped into the car again to go back for it. Clever, but\nwasted enough turns to lose to Sonnet thanks to a silly mistake!</span>, but given\nthat the Opus sessions cost around $4, I’m not going to do that. I was close to\nnot even running Opus for all four games!\n</p>\n</div>\n</section>\n<section id=\"outline-container-adjusting-model-ranking-for-game-difficulty\">\n<h2 id=\"adjusting-model-ranking-for-game-difficulty\">Adjusting model ranking for game difficulty</h2>\n<div id=\"text-orgbcff1b9\">\n<p>\nSome models appear to perform better in some games than others, so it’s hard to\nrank the models. We could take the average of their scores, but that’s unfair\nbecause some of the games are harder than others: a 40&nbsp;% in <i>Lockout</i> should be\nconsidered more impressive than a 40&nbsp;% in <i>Dreamhold</i>. What we will do, which\nmay or may not be valid, is run a linear regression using models and games as\npredictors. This gives us coefficients for the games (telling us how difficult\nthe games are), but also coefficients for the models, and these are the ones we\nwant, because the coefficients for the models are adjusted for game difficulty.\n</p>\n\n<p>\nThis regression is performed with the baseline being 9:05 played by <abbr>gpt</abbr>-5\nChat. Most of the model coefficients are not statistically significant (because\nfour games is not enough to figure out statistical significance unless the model\nis truly terrible), but they might serve as a first-order estimation for ranking\nmodels.\n</p>\n\n<p>\nIn this table, cost is per million output tokens.<label for=\"fn.4\">4</label><span><sup>4</sup> The design of the script\nensures that output and input are similar in size – O(1) to be specific – so\noutput is what is going to drive the cost.</span> The table is divided into three\ncategories: performance better than <abbr>gpt</abbr>-5 Chat, cheaper models with\nperformance that is nearly there, and models that suck.\n</p>\n\n<table>\n\n\n<colgroup>\n<col>\n\n<col>\n\n<col>\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\">Model</th>\n<th scope=\"col\">Coefficient</th>\n<th scope=\"col\">Cost ($/Mt)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Claude 4 Opus</td>\n<td>+0.09</td>\n<td>75</td>\n</tr>\n\n<tr>\n<td>Claude 4 Sonnet</td>\n<td>+0.09</td>\n<td>15</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Pro</td>\n<td>+0.04</td>\n<td>10</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Flash</td>\n<td>+0.04</td>\n<td>0.7</td>\n</tr>\n\n<tr>\n<td>Grok 4</td>\n<td>+0.02</td>\n<td>15</td>\n</tr>\n</tbody>\n<tbody>\n<tr>\n<td><abbr>gpt</abbr>-5 Chat (baseline)</td>\n<td>0.00</td>\n<td>10</td>\n</tr>\n</tbody>\n<tbody>\n<tr>\n<td>Kimi K2</td>\n<td>-0.01</td>\n<td>2.5</td>\n</tr>\n\n<tr>\n<td>DeepSeek R1 0528</td>\n<td>-0.01</td>\n<td>0.7</td>\n</tr>\n\n<tr>\n<td><abbr>glm</abbr> 4.5</td>\n<td>-0.03</td>\n<td>0.8</td>\n</tr>\n\n<tr>\n<td><abbr>gpt</abbr>-4o</td>\n<td>-0.05</td>\n<td>0.1</td>\n</tr>\n\n<tr>\n<td>Qwen3 Coder</td>\n<td>-0.06</td>\n<td>0.8</td>\n</tr>\n\n<tr>\n<td>DeepSeek V3</td>\n<td>-0.08</td>\n<td>0.7</td>\n</tr>\n\n<tr>\n<td>Llama 3 Maverick</td>\n<td>-0.10</td>\n<td>0.6</td>\n</tr>\n</tbody>\n<tbody>\n<tr>\n<td>Claude 3.5 Haiku</td>\n<td>-0.17</td>\n<td>4</td>\n</tr>\n\n<tr>\n<td><abbr>gpt</abbr>-4o-mini</td>\n<td>-0.20</td>\n<td>0.6</td>\n</tr>\n\n<tr>\n<td><abbr>gpt</abbr>-o3-mini</td>\n<td>-0.22</td>\n<td>4.4</td>\n</tr>\n\n<tr>\n<td>Mistral Small 3</td>\n<td>-0.30</td>\n<td>0.1</td>\n</tr>\n</tbody>\n</table>\n\n<p>\nSome comments:\n</p>\n\n<ul>\n<li>I find it interesting that the top-tier models (Claude Opus, Gemini Pro) don’t\nseem to significantly outperform their cheaper siblings (Claude Sonnet, Gemini\nFlash) in these tests.<label for=\"fn.5\">5</label><span><sup>5</sup> This might be because we are hand-holding the\nmodels so much in the prompt. More powerful models may be better at directing\nthemselves.</span></li>\n\n<li>I’m very impressed by Gemini 2.5 Flash. At that cost, it is performing\nadmirably. It is hard to argue for using models like DeepSeek’s R1 when we\nbetter performance at the same cost from the Google model.</li>\n\n<li>The small models really aren’t good general problem solvers. I think Haiku\ncosts so much because it is good at language, not reasoning.</li>\n</ul>\n\n<p>\nIt would be super interesting to toss these at more games to work out the finer\ndifferences (e.g. is there <i>really</i> a difference between Gemini Pro and Flash,\nor was that just down to sampling error in the small sample of games I had them\nplay?) but such a comparison gets expensive in part due to the cost of eval\ntokens (the above table cost something like $34), but mainly because it would\nrequire me to sit down and create sets of achievements for these games. I have\nonly played so many z-code games, so I cannot do this for very many games. If\nsomeone wants to support me, please reach out!\n</p>\n\n\n</div>\n</section>\n<section id=\"outline-container-testing-the-top-models-on-more-games\">\n<h2 id=\"testing-the-top-models-on-more-games\">Testing the top models on more games</h2>\n<div id=\"text-org6a51939\">\n<p>\nI have played three more games, though, so let’s continue the evaluation with\nthe five top models on these games also. Their performances on the three new\ngames are\n</p>\n\n<div id=\"orgc99d7d7\">\n<table>\n\n\n<colgroup>\n<col>\n</colgroup>\n\n<colgroup>\n<col>\n\n<col>\n\n<col>\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\">Model</th>\n<th scope=\"col\">For a Change</th>\n<th scope=\"col\">Plundered Hearts</th>\n<th scope=\"col\">So Far</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Claude 4 Sonnet</td>\n<td>11&nbsp;%</td>\n<td>19&nbsp;%</td>\n<td>28&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Pro</td>\n<td>16&nbsp;%</td>\n<td>28&nbsp;%</td>\n<td>28&nbsp;%</td>\n</tr>\n\n<tr>\n<td>GPT-5 Chat</td>\n<td>44&nbsp;%</td>\n<td>33&nbsp;%</td>\n<td>0&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Grok 4</td>\n<td>22&nbsp;%</td>\n<td>28&nbsp;%</td>\n<td>28&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Flash</td>\n<td>28&nbsp;%</td>\n<td>33&nbsp;%</td>\n<td>14&nbsp;%</td>\n</tr>\n</tbody>\n</table>\n\n</div>\n\n<p>\nUsing the same methodology as before (combining data from both trial run sets),\nwe arrive at new coefficients for the evaluated models.<label for=\"fn.6\">6</label><span><sup>6</sup> I did also\ninvestigate how Gemini 2.0 Flash compared against Gemini 2.5 Flash, because the\nformer is significantly cheaper and the latter was surprisingly good.\nUnfortunately, Gemini 2.0 Flash was not very good. Its performance relative to\nits younger sibling was -15&nbsp;%pt.</span><span>,</span><label for=\"fn.7\">7</label><span><sup>7</sup> I was also tempted to\ncompare o3-mini against o3-mini-high to see the effect of the <code>reasoning_effort</code>\nparameter but since o3-mini was such a crappy model anyway it was hard to\njustify the effort.</span>\n</p>\n\n<table>\n\n\n<colgroup>\n<col>\n\n<col>\n\n<col>\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\">Model</th>\n<th scope=\"col\">Coefficient</th>\n<th scope=\"col\">Cost ($/Mt)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Claude 4 Sonnet</td>\n<td>+0.02</td>\n<td>15</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Pro</td>\n<td>+0.02</td>\n<td>10</td>\n</tr>\n\n<tr>\n<td>Gemini 2.5 Flash</td>\n<td>+0.02</td>\n<td>0.7</td>\n</tr>\n\n<tr>\n<td>GPT-5 Chat (baseline)</td>\n<td>0.00</td>\n<td>10</td>\n</tr>\n\n<tr>\n<td>Grok 4</td>\n<td>-0.01</td>\n<td>15</td>\n</tr>\n</tbody>\n</table>\n\n<p>\nOn the one hand, it’s a little odd that the performance of Claude 4 Sonnet\ndropped. On the other hand, I calibrated the prompt using Claude 4 Sonnet\nagainst 9:05, so by adding more games we are effectively diluting the training\nset within the test set; we probably <i>should</i> expect a performance drop at that\npoint.\n</p>\n\n<p>\nNoting the cost column, Gemini 2.5 Flash is a clear winner for running text\nadventures. It’s also fast compared to the others.\n</p>\n</div>\n</section>\n<section id=\"outline-container-evaluating-score-variation\">\n<h2 id=\"evaluating-score-variation\">Evaluating score variation</h2>\n<div id=\"text-org7613360\">\n<p>\nGiven that I’ve already sunk some money into this article series, and a few\nadditional sessions with Gemini 2.5 Flash cannot hurt that much, let’s splurge\nand do that thing we wanted to do in the first place: run the same model against\nthe same game a few times to figure out the size of the sampling error. All of\nthe scores in the table below comes from Gemini 2.5 Flash. The first column is\nthe standard deviation of the remaining columns.\n</p>\n\n<div id=\"org945213a\">\n<table>\n\n\n<colgroup>\n<col>\n</colgroup>\n\n<colgroup>\n<col>\n</colgroup>\n\n<colgroup>\n<col>\n\n<col>\n\n<col>\n\n<col>\n\n<col>\n\n<col>\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\">Game</th>\n<th scope=\"col\">St. dev.</th>\n<th scope=\"col\">Run 1</th>\n<th scope=\"col\">Run 2</th>\n<th scope=\"col\">Run 3</th>\n<th scope=\"col\">Run 4</th>\n<th scope=\"col\">Run 5</th>\n<th scope=\"col\">Run 6</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>9:05</td>\n<td>14&nbsp;%pt</td>\n<td>73&nbsp;%</td>\n<td>86&nbsp;%</td>\n<td>86&nbsp;%</td>\n<td>80&nbsp;%</td>\n<td>53&nbsp;%</td>\n<td>60&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Lockout</td>\n<td>11&nbsp;%pt</td>\n<td>30&nbsp;%</td>\n<td>46&nbsp;%</td>\n<td>46&nbsp;%</td>\n<td>38&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>23&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Dreamhold</td>\n<td>10&nbsp;%pt</td>\n<td>53&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>46&nbsp;%</td>\n<td>46&nbsp;%</td>\n<td>53&nbsp;%</td>\n<td>26&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Lost Pig</td>\n<td>3&nbsp;%pt</td>\n<td>46&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>40&nbsp;%</td>\n<td>46&nbsp;%</td>\n<td>40&nbsp;%</td>\n</tr>\n\n<tr>\n<td>For a Change</td>\n<td>6&nbsp;%pt</td>\n<td>16&nbsp;%</td>\n<td>11&nbsp;%</td>\n<td>16&nbsp;%</td>\n<td>5&nbsp;%</td>\n<td>0&nbsp;%</td>\n<td>11&nbsp;%</td>\n</tr>\n\n<tr>\n<td>Plundered Hearts</td>\n<td>4&nbsp;%pt</td>\n<td>19&nbsp;%</td>\n<td>19&nbsp;%</td>\n<td>19&nbsp;%</td>\n<td>23&nbsp;%</td>\n<td>28&nbsp;%</td>\n<td>28&nbsp;%</td>\n</tr>\n\n<tr>\n<td>So Far</td>\n<td>32&nbsp;%pt</td>\n<td>14&nbsp;%</td>\n<td>57&nbsp;%</td>\n<td>71&nbsp;%</td>\n<td>71&nbsp;%</td>\n<td>71&nbsp;%</td>\n<td>0&nbsp;%</td>\n</tr>\n</tbody>\n</table>\n\n</div>\n\n<p>\nIn case it is not obvious, this is not so much an evalutaion of Gemini 2.5 Flash\nas it is a judgment of the quality of the testing protocol. It is clear, for\nexample, that using <i>So Far</i> to evaluate <abbr>llm</abbr>s is a mistake: the same model has\nlarge variation between runs, and the difference between runs of different\nmodels is not so large. It would be more informative to replace the run of <i>So\nFar</i> with another run of one of the other games – maybe <i>Plundered Hearts</i> or\n<i>Lost Pig</i>, which start out more linearly.<label for=\"fn.8\">8</label><span><sup>8</sup> <i>For a Change</i> might look like a\ngood game for evaluation, but I think that’s a mistake. It’s not that the model\nmakes consistent progress, but that it fails to make almost any progress at all,\nthanks to how open the game is right from the gate.</span>\n</p>\n</div>\n</section>\n<section id=\"outline-container-conclusions\">\n<h2 id=\"conclusions\">Conclusions</h2>\n<div id=\"text-org6d63e1a\">\n<p>\nI’m not sure what conclusions to draw from this article series.\n</p>\n\n<ul>\n<li><a href=\"https://entropicthoughts.com/interacting-with-text-adventures-through-perl.html\">We can drive z-code text adventures through Perl</a>, which lets us connect it to\nan <abbr>llm</abbr> in a controlled way. It turned out more complicated than one would think, but\ndefinitely doable.</li>\n\n<li><abbr>llm</abbr>s are still not great at playing text adventures. <a href=\"https://entropicthoughts.com/getting-an-llm-to-play-text-adventures.html\">Giving them leading\nquestions to keep them on track helps a lot</a>. Giving them hints helps them\nsurprisingly little.</li>\n\n<li>The variation in how much they accomplish can be large for some games with\nlots of distracting details, such as <i>Lockout</i> and <i>So Far</i>. The games that\nare easiest to evaluate with are those with a relatively linear beginning,\nsuch as <i>Lost Pig</i> and <i>Plundered Hearts</i>.</li>\n\n<li>There is one cheap model that is about as good as <abbr>llm</abbr> models get at playing\ntext adventures: Gemini 2.5 Flash. Many of the other cheap models might have\nperformance worse than <abbr>gpt</abbr>-5 Chat, and probably also worse than Gemini 2.5\nFlash. Claude 4 Sonnet might seem like the best model if costs be damned, but\nthat is probably because the prompt was calibrated against Claude 4 Sonnet.</li>\n\n<li>Running <abbr>llm</abbr>s in agentic type applications really burns through <abbr>api</abbr> credits\nlike nothing else. I’d really like to complement this analysis with the “how\nmany turns does the model need to get to point X” test, but I cannot motivate\nspending the money for it.</li>\n</ul>\n</div>\n</section>\n\n            </div></div>","textContent":"\n                \nWhen we first set up the llm such that it could play text adventures, we noted\nthat none of the models we tried to use with it were any good at it. We dreamed\nof a way to compare them, but all I could think of was setting a goal far into\nthe game and seeing how long it takes them to get there. I just realised there’s\na better way to do it.\n\n\nEvaluation against achievments\n\n\nWhat we’ll do is set a low-ish turn limit and see how much they manage to\naccomplish in that time.11 Another alternative for more linear games is\nrunning them multiple times with a turn limit and seeing how often they get past\na particular point within that turn limit.\n\n\n\nGiven how much freedom is offered to players of text adventures, this is a\ndifficult test. It’s normal even for a skilled human player to immerse\nthemselves in their surrounding rather than make constant progress. I wouldn’t\nbe surprised if I got a score of zero if someone plopped me down in front of\nthis test. But still, maybe it’s the best we can do with limited\nresources.22 Another idea is to give them a far-off goal and then somehow have\nthem request hints when they are stuck, and count how many hints they need to\nget there. However, given how little they used hints given in the previous\narticle, I doubt this would work very well either.\n\n\n\nWhat we’ll do is define a set of achievements for a game. These achievements\nwill be clustered around the first few turns of the game, because we’ll only\ngive the llm a few turns to earn them. Here’s an example for 9:05.\n\n\nTURN_LIMIT          40\nANSWER_PHONE        Click.\nEXIT_BED            You get out of bed.\nOPEN_DRESSER        revealing some clean\nENTER_BATHROOM      far from luxurious\nREMOVE_SOILED       You take off the soiled\nREMOVE_WATCH        You take off the watch\nENTER_SHOWER        dawdle\nWEAR_CLEAN          You put on the clean\nOPEN_FRONT          You open the front\nUNLOCK_CAR          Unlocked.\nENTER_CAR           Las Mesas\nOPEN_WALLET         open the wallet\nCARD_SLOT           green LED lights\n\n\n\nIt should be fairly clear how this works: the TURN_LIMIT specifies how many\nturns the llm has to collect achievements. Every line other than that\nspecifies an achievement: the name is on the left, and it counts as earned when\nthe game prints the text on the right. The llm knows nothing of these\nachievements. It tries to get through the game and in the background we use the\nachievements to count how far it gets.\n\n\n\nIt might seem like the turn limit must be calibrated such that a score of 100 %\nis possible, but that’s not the case. Many of the games we are going to test\nwith have branching already at the start, such that the achievements need to\ncover multiple branches, and it’s impossible to go through all branches within\nthe turn limit. What we do need to be careful about is making sure the number of\nachievements in each branch is roughly the same, otherwise models that are lucky\nand go down an achievement-rich path will get a higher score. Thanks to this,\nthe score we get out of this test is a relative comparison between models, not\nan absolute measure of how well the llms play text adventures. We have already\nestablished that they don’t do it very well, and we can’t be more nuanced than\nthat without paying for a lot of eval tokens.\n\n\n\nWe might consider making some moves not count toward the turn limit, for example\nerroneous commands, or examining things – the latter because more powerful\nmodels are more methodical and examine more things, and it seems odd to penalise\nthem for this. However, in the end, examining things is probably part of what\nallows the more powerful models to make further progress (and typing valid\ncommands is part of being good at text adventures), so we won’t give away any\nmoves for free.\n\n\n\n\nEvaluating many popular models\n\n\nWe register for OpenRouter to get convenient access to more models and then let\nthem whirr away with the Perl script, which is updated to cut the llm off at\nthe turn limit. At that point it reports to us how many achievements were\nearned. We get the following results, ordered roughly by decreasing performance.\n(The result tables in this article are wide; on narrow viewports you may have\nto scroll sideways.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n9:05\nLockout\nDreamhold\nLost Pig\n\n\n\n\nGrok 4\n86 %\n15 %\n46 %\n33 %\n\n\n\nClaude 4 Sonnet\n80 %\n30 %\n53 %\n46 %\n\n\n\nGemini 2.5 Flash\n80 %\n30 %\n33 %\n46 %\n\n\n\nGemini 2.5 Pro\n80 %\n30 %\n40 %\n40 %\n\n\n\nDeepSeek R1 0528\n80 %\n23 %\n33 %\n33 %\n\n\n\nClaude 4 Opus\n73 %\n30 %\n60 %\n46 %\n\n\n\ngpt-5 Chat\n73 %\n15 %\n53 %\n33 %\n\n\n\nDeepSeek V3\n66 %\n23 %\n20 %\n33 %\n\n\n\ngpt-4o\n53 %\n23 %\n40 %\n40 %\n\n\n\nQwen3 Coder\n53 %\n23 %\n40 %\n33 %\n\n\n\nKimi K2\n53 %\n30 %\n46 %\n40 %\n\n\n\nglm 4.5\n53 %\n23 %\n33 %\n53 %\n\n\n\nClaude 3.5 Haiku\n38 %\n15 %\n26 %\n26 %\n\n\n\nLlama 3 Maverick\n33 %\n30 %\n40 %\n33 %\n\n\n\ngpt-o3-mini\n20 %\n15 %\n26 %\n26 %\n\n\n\nMistral Small 3\n20 %\n15 %\n0 %\n20 %\n\n\n\ngpt-4o-mini\n13 %\n23 %\n20 %\n40 %\n\n\n\n\n\n\n\nIdeally, these should be run multiple times to account for random variation in\nperformance33 For example, in 9:05, Opus thought it did not carry the wallet\nwhen it did, so it jumped into the car again to go back for it. Clever, but\nwasted enough turns to lose to Sonnet thanks to a silly mistake!, but given\nthat the Opus sessions cost around $4, I’m not going to do that. I was close to\nnot even running Opus for all four games!\n\n\n\n\nAdjusting model ranking for game difficulty\n\n\nSome models appear to perform better in some games than others, so it’s hard to\nrank the models. We could take the average of their scores, but that’s unfair\nbecause some of the games are harder than others: a 40 % in Lockout should be\nconsidered more impressive than a 40 % in Dreamhold. What we will do, which\nmay or may not be valid, is run a linear regression using models and games as\npredictors. This gives us coefficients for the games (telling us how difficult\nthe games are), but also coefficients for the models, and these are the ones we\nwant, because the coefficients for the models are adjusted for game difficulty.\n\n\n\nThis regression is performed with the baseline being 9:05 played by gpt-5\nChat. Most of the model coefficients are not statistically significant (because\nfour games is not enough to figure out statistical significance unless the model\nis truly terrible), but they might serve as a first-order estimation for ranking\nmodels.\n\n\n\nIn this table, cost is per million output tokens.44 The design of the script\nensures that output and input are similar in size – O(1) to be specific – so\noutput is what is going to drive the cost. The table is divided into three\ncategories: performance better than gpt-5 Chat, cheaper models with\nperformance that is nearly there, and models that suck.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nCoefficient\nCost ($/Mt)\n\n\n\n\nClaude 4 Opus\n+0.09\n75\n\n\n\nClaude 4 Sonnet\n+0.09\n15\n\n\n\nGemini 2.5 Pro\n+0.04\n10\n\n\n\nGemini 2.5 Flash\n+0.04\n0.7\n\n\n\nGrok 4\n+0.02\n15\n\n\n\n\ngpt-5 Chat (baseline)\n0.00\n10\n\n\n\n\nKimi K2\n-0.01\n2.5\n\n\n\nDeepSeek R1 0528\n-0.01\n0.7\n\n\n\nglm 4.5\n-0.03\n0.8\n\n\n\ngpt-4o\n-0.05\n0.1\n\n\n\nQwen3 Coder\n-0.06\n0.8\n\n\n\nDeepSeek V3\n-0.08\n0.7\n\n\n\nLlama 3 Maverick\n-0.10\n0.6\n\n\n\n\nClaude 3.5 Haiku\n-0.17\n4\n\n\n\ngpt-4o-mini\n-0.20\n0.6\n\n\n\ngpt-o3-mini\n-0.22\n4.4\n\n\n\nMistral Small 3\n-0.30\n0.1\n\n\n\n\n\nSome comments:\n\n\n\nI find it interesting that the top-tier models (Claude Opus, Gemini Pro) don’t\nseem to significantly outperform their cheaper siblings (Claude Sonnet, Gemini\nFlash) in these tests.55 This might be because we are hand-holding the\nmodels so much in the prompt. More powerful models may be better at directing\nthemselves.\n\nI’m very impressed by Gemini 2.5 Flash. At that cost, it is performing\nadmirably. It is hard to argue for using models like DeepSeek’s R1 when we\nbetter performance at the same cost from the Google model.\n\nThe small models really aren’t good general problem solvers. I think Haiku\ncosts so much because it is good at language, not reasoning.\n\n\n\nIt would be super interesting to toss these at more games to work out the finer\ndifferences (e.g. is there really a difference between Gemini Pro and Flash,\nor was that just down to sampling error in the small sample of games I had them\nplay?) but such a comparison gets expensive in part due to the cost of eval\ntokens (the above table cost something like $34), but mainly because it would\nrequire me to sit down and create sets of achievements for these games. I have\nonly played so many z-code games, so I cannot do this for very many games. If\nsomeone wants to support me, please reach out!\n\n\n\n\n\n\nTesting the top models on more games\n\n\nI have played three more games, though, so let’s continue the evaluation with\nthe five top models on these games also. Their performances on the three new\ngames are\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nFor a Change\nPlundered Hearts\nSo Far\n\n\n\n\nClaude 4 Sonnet\n11 %\n19 %\n28 %\n\n\n\nGemini 2.5 Pro\n16 %\n28 %\n28 %\n\n\n\nGPT-5 Chat\n44 %\n33 %\n0 %\n\n\n\nGrok 4\n22 %\n28 %\n28 %\n\n\n\nGemini 2.5 Flash\n28 %\n33 %\n14 %\n\n\n\n\n\n\n\nUsing the same methodology as before (combining data from both trial run sets),\nwe arrive at new coefficients for the evaluated models.66 I did also\ninvestigate how Gemini 2.0 Flash compared against Gemini 2.5 Flash, because the\nformer is significantly cheaper and the latter was surprisingly good.\nUnfortunately, Gemini 2.0 Flash was not very good. Its performance relative to\nits younger sibling was -15 %pt.,77 I was also tempted to\ncompare o3-mini against o3-mini-high to see the effect of the reasoning_effort\nparameter but since o3-mini was such a crappy model anyway it was hard to\njustify the effort.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nCoefficient\nCost ($/Mt)\n\n\n\n\nClaude 4 Sonnet\n+0.02\n15\n\n\n\nGemini 2.5 Pro\n+0.02\n10\n\n\n\nGemini 2.5 Flash\n+0.02\n0.7\n\n\n\nGPT-5 Chat (baseline)\n0.00\n10\n\n\n\nGrok 4\n-0.01\n15\n\n\n\n\n\nOn the one hand, it’s a little odd that the performance of Claude 4 Sonnet\ndropped. On the other hand, I calibrated the prompt using Claude 4 Sonnet\nagainst 9:05, so by adding more games we are effectively diluting the training\nset within the test set; we probably should expect a performance drop at that\npoint.\n\n\n\nNoting the cost column, Gemini 2.5 Flash is a clear winner for running text\nadventures. It’s also fast compared to the others.\n\n\n\n\nEvaluating score variation\n\n\nGiven that I’ve already sunk some money into this article series, and a few\nadditional sessions with Gemini 2.5 Flash cannot hurt that much, let’s splurge\nand do that thing we wanted to do in the first place: run the same model against\nthe same game a few times to figure out the size of the sampling error. All of\nthe scores in the table below comes from Gemini 2.5 Flash. The first column is\nthe standard deviation of the remaining columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame\nSt. dev.\nRun 1\nRun 2\nRun 3\nRun 4\nRun 5\nRun 6\n\n\n\n\n9:05\n14 %pt\n73 %\n86 %\n86 %\n80 %\n53 %\n60 %\n\n\n\nLockout\n11 %pt\n30 %\n46 %\n46 %\n38 %\n23 %\n23 %\n\n\n\nDreamhold\n10 %pt\n53 %\n40 %\n46 %\n46 %\n53 %\n26 %\n\n\n\nLost Pig\n3 %pt\n46 %\n40 %\n40 %\n40 %\n46 %\n40 %\n\n\n\nFor a Change\n6 %pt\n16 %\n11 %\n16 %\n5 %\n0 %\n11 %\n\n\n\nPlundered Hearts\n4 %pt\n19 %\n19 %\n19 %\n23 %\n28 %\n28 %\n\n\n\nSo Far\n32 %pt\n14 %\n57 %\n71 %\n71 %\n71 %\n0 %\n\n\n\n\n\n\n\nIn case it is not obvious, this is not so much an evalutaion of Gemini 2.5 Flash\nas it is a judgment of the quality of the testing protocol. It is clear, for\nexample, that using So Far to evaluate llms is a mistake: the same model has\nlarge variation between runs, and the difference between runs of different\nmodels is not so large. It would be more informative to replace the run of So\nFar with another run of one of the other games – maybe Plundered Hearts or\nLost Pig, which start out more linearly.88 For a Change might look like a\ngood game for evaluation, but I think that’s a mistake. It’s not that the model\nmakes consistent progress, but that it fails to make almost any progress at all,\nthanks to how open the game is right from the gate.\n\n\n\n\nConclusions\n\n\nI’m not sure what conclusions to draw from this article series.\n\n\n\nWe can drive z-code text adventures through Perl, which lets us connect it to\nan llm in a controlled way. It turned out more complicated than one would think, but\ndefinitely doable.\n\nllms are still not great at playing text adventures. Giving them leading\nquestions to keep them on track helps a lot. Giving them hints helps them\nsurprisingly little.\n\nThe variation in how much they accomplish can be large for some games with\nlots of distracting details, such as Lockout and So Far. The games that\nare easiest to evaluate with are those with a relatively linear beginning,\nsuch as Lost Pig and Plundered Hearts.\n\nThere is one cheap model that is about as good as llm models get at playing\ntext adventures: Gemini 2.5 Flash. Many of the other cheap models might have\nperformance worse than gpt-5 Chat, and probably also worse than Gemini 2.5\nFlash. Claude 4 Sonnet might seem like the best model if costs be damned, but\nthat is probably because the prompt was calibrated against Claude 4 Sonnet.\n\nRunning llms in agentic type applications really burns through api credits\nlike nothing else. I’d really like to complement this analysis with the “how\nmany turns does the model need to get to point X” test, but I cannot motivate\nspending the money for it.\n\n\n\n\n            ","length":13158,"excerpt":"When we first set up the llm such that it could play text adventures, we noted\nthat none of the models we tried to use with it were any good at it. We dreamed\nof a way to compare them, but all I could think of was setting a goal far into\nthe game and seeing how long it takes them to get there. I just realised there’s\na better way to do it.","url":"https://entropicthoughts.com/evaluating-llms-playing-text-adventures","addedTime":"2025-08-12,16:49"}