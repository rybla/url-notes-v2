{"title":"Why Tail-Recursive Functions are Loops","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n        <article itemscope=\"\" itemtype=\"http://schema.org/BlogPosting\">\n\n  \n\n  <div itemprop=\"articleBody\">\n    <p>One story every computing enthusiast should hear is the lesson of\nhow loops and tail-recursion are equivalent. We like recursive\nfunctions because they’re amenable to induction, and we can derive\nthem in a way that is in direct correspondence with the definition of\nthe datatype over which they recur. We like loops because they’re\nfast and make intuitive sense as long as variables don’t change in too\ntricky a way.</p>\n\n<p>In general, recursive functions are slower than loops because they\npush stack frames: the performance of most programs today is dominated\nby memory reads/writes. The data we touch the most lives in the\ncache–we do <em>not</em> want to evict a ton of stuff from the cache, under\nany circumstance. In a direct-style implementation of a recursive\nfunction, the recursive call <em>has</em> to push a stack frame to remember\nwhat to do once the function returns:</p>\n\n<div><pre><code>;; Racket\n(define (sum l)\n  (if (empty? l)\n      0\n      (+ (first l) (sum (rest l)))))\n\n// C \nint sum(int *l, int length) {\n    if (length == 0)\n        return 0;\n    else\n        return l[0] + sum(l + 1, length - 1);\n}\n</code></pre></div>\n\n<p>When we get to <code>(+ (first l) (sum (rest l)))</code>, we first call <code>(first\nl)</code> (which returns the first element). While we’re making that call,\nwe have to remember to come back and do <code>(sum (rest l))</code>–to be fully\nprecise, we remember that we need to do <code>(rest l)</code>, then take its\nresult <code>x</code> and call <code>(sum x)</code>, remembering to come back and finally\ntake <em>that</em> result and add it to the result of <code>(first l)</code>. The reason\nwe have to do this is because we need to remember those partial\nresults (in this case the result of <code>(first l)</code>): we have to store\nthem somewhere after all, and each time we make the recursive call, we\nneed to remember the result of <code>(first l)</code> from <em>this</em> call–we need\nO(n) stack space for a list of size n.</p>\n\n<p>Of course, if we use iteration this all goes away:</p>\n\n<div><pre><code>;; Racket\n(define (sum l)\n  (define x 0)\n  (for ([elt l])\n    (set! x (+ x elt)))\n  x)\n\n// C\nint sum(const int *l, int length) {\n    int x = 0;\n    for (int i = 0; i &lt; length; i++) {\n        x += l[i];\n    }\n    return x;\n}\n</code></pre></div>\n\n<p>We all have an intuitive sense of what the loop is doing: once we hit\nthe end of the loop, we do <em>not</em> make a recursive call (we never issue\na <code>call</code> instruction in assembly), we simply <em>jump</em> up to the\nbeginning of the loop. The key is that <code>x</code> is being used as an\n<em>accumulator</em>, growing a partial result in a <em>bottom-up</em> fashion as\nthe computation proceeds, eventually yielding the final value at the\nend. Instead of keeping partial results on the stack, the loop takes a\n<em>constant</em> amount of space but linear time.</p>\n\n<p>In a tail-recursive implementation, the rule is that every recursive\ncall must be a <em>tail</em> call. Intuitively, a tail call is a call which\nis “immediately returned.” More formally, a subexpression of an\nexpression is in tail position if the return value from that\nexpression is the return value from the whole expression. For example,\nin <code>(if guard e-t e-f)</code>, both <code>e-t</code> and <code>e-f</code> are in tail position,\nbut the guard is not: after we decide which branch to take, we’re\ncommitted:</p>\n\n<div><pre><code>(define (foo ...)\n  ...\n  (if guard\n    (f x y ...)\n    (g z ...)))\n</code></pre></div>\n\n<p>Once we finish executing <code>guard</code>, it would be <em>useless</em> (but\n<strong>correct</strong>) to (a) push a stack frame, (b) wait on the result of the\nsubordinate call, and (c) merely return <em>that same result</em>, because\nall we’d be doing is <em>copying</em> the return value from the callee and\npropagating it back as the return value of the caller.  Being a tail\ncall is a syntactic property of a callsite: we (and the compiler) can\neasily look at a piece of code and cheaply decide when a call is a\ntail call versus not.</p>\n\n<p>This reasoning above generalizes to <em>any</em> call expression in tail\nposition: <em>because a tail call will necessarily evaluate to its\nresult, administratively copying it up/down the stack is extensionally\na no-op</em>. Now, the tail-recursive version uses a simple trick I teach\nto all of my students: (a) identify an accumulator variable, (b)\ninstead of computing with the <em>result</em> of the recursive call, compute\nwith the <em>current accumulator</em>, (c) return the accumulator in the base\ncase:</p>\n\n<div><pre><code>;; Racket\n(define (sum l acc) ;; note: acc got added\n  (if (empty? l)\n      acc ;; this is the *true* return!\n      (sum (rest l) (+ acc (first l)))))\n\n// C -- we pass in length manually because we're using arrays\nint sum(const int* l, int length, int acc) {\n    if (length == 0) return acc;\n    return sum(l + 1, length - 1, acc + l[0]);\n}\n</code></pre></div>\n\n<p>Both of these functions are tail recursive: because the only recursive\ncall to <code>sum</code> is <em>also</em> the return value from <code>sum</code> (or, more\ndirectly: because both calls to <code>sum</code> are in tail position). Since the\ncompiler knows that these are tail calls, a compiler with tail-call\noptimization will ensure that both of these tail calls compile into\n<code>jmp</code> statements–with zero implication on stack usage–rather than\nthe more burdensome (on the cache, stack, etc.) direct-style\ncalls. Something that should concern you is this: if the function is\nusing constant stack space, how are the variables being updated /\nrepresented!? The answer is that the arguments get <strong>stomped over</strong>,\nand <strong>mutably updated</strong>, yielding the <em>exact same performance profile\nas a loop!</em>.</p>\n\n<p>Now time for an exercise, what about this program, can you convert it\nto using tail-recursion?</p>\n\n<div><pre><code>;; return a pair (cons cell) of the number of even numbers,\n;; and the number of odd numbers.\n;; HINT: use multiple accumulators. \n(define (even-odd l)\n  (if (empty? l)\n      (cons 0 0)\n      (let ([v (even-odd (rest l))])\n            (if (first l)\n                (cons (add1 (car v)) (cdr v))\n                (cons (car v) (add1 (cdr v))))))))\n</code></pre></div>\n\n<p>What about <em>this</em> program?</p>\n\n<div><pre><code>;; flattens a tree? into a list\n;; It's hard because there are *two* calls to linearize--can you do anything?\n(define (linearize t)\n  (match t\n    ['empty '()]\n    [`(node ,v ,t0 ,t1) (append (list v) (linearize t0) (linearize t1))]))\n</code></pre></div>\n\n<p>One surprising fact is that we can systematically compile <em>any</em>\nprogram so that <em>every</em> call is a tail-call, by completely\ntransforming the program into <a href=\"https://matt.might.net/articles/cps-conversion/\">continuation-passing-style\n(CPS)</a>, this\nessentially <em>eliminates</em> the stack. Indeed, some compilers for\nfunctional languages work precisely this way: those languages\n<strong>cannot</strong> fall prey to a stack overflow, because they have\nessentially traded a monolithic (efficient, array-like) stack for a\ndeeply-nested stack, strewn throughout the heap—because the\ncontinuations will be heap-allocated and nested. There are various\nexciting trade-offs here, but for now I will leave this as is–we will\ncontinue next week.</p>\n\n\n  </div>\n\n</article>\n\n\n      </div></div>","textContent":"\n        \n\n  \n\n  \n    One story every computing enthusiast should hear is the lesson of\nhow loops and tail-recursion are equivalent. We like recursive\nfunctions because they’re amenable to induction, and we can derive\nthem in a way that is in direct correspondence with the definition of\nthe datatype over which they recur. We like loops because they’re\nfast and make intuitive sense as long as variables don’t change in too\ntricky a way.\n\nIn general, recursive functions are slower than loops because they\npush stack frames: the performance of most programs today is dominated\nby memory reads/writes. The data we touch the most lives in the\ncache–we do not want to evict a ton of stuff from the cache, under\nany circumstance. In a direct-style implementation of a recursive\nfunction, the recursive call has to push a stack frame to remember\nwhat to do once the function returns:\n\n;; Racket\n(define (sum l)\n  (if (empty? l)\n      0\n      (+ (first l) (sum (rest l)))))\n\n// C \nint sum(int *l, int length) {\n    if (length == 0)\n        return 0;\n    else\n        return l[0] + sum(l + 1, length - 1);\n}\n\n\nWhen we get to (+ (first l) (sum (rest l))), we first call (first\nl) (which returns the first element). While we’re making that call,\nwe have to remember to come back and do (sum (rest l))–to be fully\nprecise, we remember that we need to do (rest l), then take its\nresult x and call (sum x), remembering to come back and finally\ntake that result and add it to the result of (first l). The reason\nwe have to do this is because we need to remember those partial\nresults (in this case the result of (first l)): we have to store\nthem somewhere after all, and each time we make the recursive call, we\nneed to remember the result of (first l) from this call–we need\nO(n) stack space for a list of size n.\n\nOf course, if we use iteration this all goes away:\n\n;; Racket\n(define (sum l)\n  (define x 0)\n  (for ([elt l])\n    (set! x (+ x elt)))\n  x)\n\n// C\nint sum(const int *l, int length) {\n    int x = 0;\n    for (int i = 0; i < length; i++) {\n        x += l[i];\n    }\n    return x;\n}\n\n\nWe all have an intuitive sense of what the loop is doing: once we hit\nthe end of the loop, we do not make a recursive call (we never issue\na call instruction in assembly), we simply jump up to the\nbeginning of the loop. The key is that x is being used as an\naccumulator, growing a partial result in a bottom-up fashion as\nthe computation proceeds, eventually yielding the final value at the\nend. Instead of keeping partial results on the stack, the loop takes a\nconstant amount of space but linear time.\n\nIn a tail-recursive implementation, the rule is that every recursive\ncall must be a tail call. Intuitively, a tail call is a call which\nis “immediately returned.” More formally, a subexpression of an\nexpression is in tail position if the return value from that\nexpression is the return value from the whole expression. For example,\nin (if guard e-t e-f), both e-t and e-f are in tail position,\nbut the guard is not: after we decide which branch to take, we’re\ncommitted:\n\n(define (foo ...)\n  ...\n  (if guard\n    (f x y ...)\n    (g z ...)))\n\n\nOnce we finish executing guard, it would be useless (but\ncorrect) to (a) push a stack frame, (b) wait on the result of the\nsubordinate call, and (c) merely return that same result, because\nall we’d be doing is copying the return value from the callee and\npropagating it back as the return value of the caller.  Being a tail\ncall is a syntactic property of a callsite: we (and the compiler) can\neasily look at a piece of code and cheaply decide when a call is a\ntail call versus not.\n\nThis reasoning above generalizes to any call expression in tail\nposition: because a tail call will necessarily evaluate to its\nresult, administratively copying it up/down the stack is extensionally\na no-op. Now, the tail-recursive version uses a simple trick I teach\nto all of my students: (a) identify an accumulator variable, (b)\ninstead of computing with the result of the recursive call, compute\nwith the current accumulator, (c) return the accumulator in the base\ncase:\n\n;; Racket\n(define (sum l acc) ;; note: acc got added\n  (if (empty? l)\n      acc ;; this is the *true* return!\n      (sum (rest l) (+ acc (first l)))))\n\n// C -- we pass in length manually because we're using arrays\nint sum(const int* l, int length, int acc) {\n    if (length == 0) return acc;\n    return sum(l + 1, length - 1, acc + l[0]);\n}\n\n\nBoth of these functions are tail recursive: because the only recursive\ncall to sum is also the return value from sum (or, more\ndirectly: because both calls to sum are in tail position). Since the\ncompiler knows that these are tail calls, a compiler with tail-call\noptimization will ensure that both of these tail calls compile into\njmp statements–with zero implication on stack usage–rather than\nthe more burdensome (on the cache, stack, etc.) direct-style\ncalls. Something that should concern you is this: if the function is\nusing constant stack space, how are the variables being updated /\nrepresented!? The answer is that the arguments get stomped over,\nand mutably updated, yielding the exact same performance profile\nas a loop!.\n\nNow time for an exercise, what about this program, can you convert it\nto using tail-recursion?\n\n;; return a pair (cons cell) of the number of even numbers,\n;; and the number of odd numbers.\n;; HINT: use multiple accumulators. \n(define (even-odd l)\n  (if (empty? l)\n      (cons 0 0)\n      (let ([v (even-odd (rest l))])\n            (if (first l)\n                (cons (add1 (car v)) (cdr v))\n                (cons (car v) (add1 (cdr v))))))))\n\n\nWhat about this program?\n\n;; flattens a tree? into a list\n;; It's hard because there are *two* calls to linearize--can you do anything?\n(define (linearize t)\n  (match t\n    ['empty '()]\n    [`(node ,v ,t0 ,t1) (append (list v) (linearize t0) (linearize t1))]))\n\n\nOne surprising fact is that we can systematically compile any\nprogram so that every call is a tail-call, by completely\ntransforming the program into continuation-passing-style\n(CPS), this\nessentially eliminates the stack. Indeed, some compilers for\nfunctional languages work precisely this way: those languages\ncannot fall prey to a stack overflow, because they have\nessentially traded a monolithic (efficient, array-like) stack for a\ndeeply-nested stack, strewn throughout the heap—because the\ncontinuations will be heap-allocated and nested. There are various\nexciting trade-offs here, but for now I will leave this as is–we will\ncontinue next week.\n\n\n  \n\n\n\n\n      ","length":6541,"excerpt":"One story every computing enthusiast should hear is the lesson ofhow loops and tail-recursion are equivalent. We like recursivefunctions because they’re amen...","url":"https://kmicinski.com/functional-programming/2025/08/01/loops/"}