{"title":"Qodo Command scores 71.2% on SWE-bench Verified","byline":"Tomer Yanay","lang":"en-US","content":"<div id=\"readability-page-1\" class=\"page\"><div data-post-content=\"\">\n\t\t\t\t\t\t\t\t\t<figure>\n\t\t\t\t\t\t<img width=\"900\" height=\"604\" src=\"https://www.qodo.ai/wp-content/uploads/2025/04/Qodo-Command-scores-71.2-on-SWE-Bench-Verified.png\" alt=\"\" loading=\"lazy\">\t\t\t\t\t</figure>\n\t\t\t\t\t<p>We’re excited to announce that Qodo Command, our CLI agent, achieved a scored of 71.2% on SWE-bench Verified (submission pending review), the leading benchmark for evaluating AI agents on real-world software engineering tasks.</p>\n<p><img loading=\"lazy\" src=\"https://www.qodo.ai/wp-content/uploads/2025/04/SWE-bench.png\" alt=\"\" width=\"900\" height=\"604\" srcset=\"https://www.qodo.ai/wp-content/uploads/2025/04/SWE-bench.png 900w, https://www.qodo.ai/wp-content/uploads/2025/04/SWE-bench-300x201.png 300w, https://www.qodo.ai/wp-content/uploads/2025/04/SWE-bench-768x515.png 768w, https://www.qodo.ai/wp-content/uploads/2025/04/SWE-bench-48x32.png 48w, https://www.qodo.ai/wp-content/uploads/2025/04/SWE-bench-58x39.png 58w, https://www.qodo.ai/wp-content/uploads/2025/04/SWE-bench-72x48.png 72w\" sizes=\"auto, (max-width: 900px) 100vw, 900px\"></p>\n<p><span>This achievement is a strong signal that Qodo’s agents are built for the realities of production development. For use cases like reviewing code, writing tests, fixing bugs, and generating features, our CLI agent goes beyond autocomplete to deliver thoughtful, context-aware, and high-integrity code.&nbsp;</span></p>\n<h2 id=\"one-shot-real-world-execution\">One-Shot, Real-World Execution</h2>\n<p><span>Most AI benchmarks evaluate agents in isolated, simplified environments. However, SWE-bench Verified tests coding agents in messy, complex, real-world software engineering scenarios. Each test case in SWE-bench is built from a real GitHub issue in one of 12 widely-used, open-source Python repositories. Agents are given the GitHub issue and the codebase in the state it was in when the issue was opened, and must reason, plan, and edit code, iterating over many turns as a developer would – without shortcutting the problem.</span></p>\n<p><span>Qodo Command scored 71.2% using a single run of the production version of Qodo Command—no finetuning or benchmark-specific adjustments—exactly the way any developer would by running it out-of-the-box with the simple install package</span>: <code>npm install -g @qodo/command.&nbsp;</code></p>\n<h3 id=\"llm-model-flexibility-amp-claude-partnership\">LLM Model Flexibility &amp; Claude Partnership</h3>\n<p>While Qodo Command is designed to support all top-tier LLMs, Claude 4 emerged as our model of choice for SWE Bench Verified results. Thanks to a strong partnership with Anthropic—Qodo is a “<a href=\"https://www.anthropic.com/partners/powered-by-claude\">Powered by Claude</a>” solution, we’re collaboratively building the world’s most adaptive and learning-oriented coding agents, leveraging one of the most advanced language models available today.</p>\n<h2 id=\"the-architecture-behind-our-71-2-swe-bench-success\">The Architecture Behind Our 71.2% SWE-bench success</h2>\n<p>Achieving high performance on SWE-bench wasn’t about optimizing for the benchmark–it was the natural result of engineering Qodo Command to excel at real-world software engineering challenges. Here’s how our architectural decisions directly contributed to its performance:</p>\n<h3 id=\"context-summarization\">Context Summarization</h3>\n<p>SWE-bench Verified tests AI on complex, muli-file codebases where understanding interdependencies is crucial for success. Succeeding in this environment requires more than feeding raw files to an LLM, pattern-matching or autocomplete.</p>\n<p><span>Qodo Command solves this by distilling multi-layered code into precise, high-signal summaries—ensuring that language models receive only the most relevant, structured context at every step. This enables deep reasoning, accurate generation, and high-quality reviews without hitting context limits or losing essential detail.</span></p>\n<h3 id=\"execution-planning\">Execution Planning</h3>\n<p>Qodo’s default plan-first approach ensures that implementation begins only after a structured breakdown of user intent. Instead of rushing into action, we first deeply analyze the user’s goal, then decompose it into clear, actionable subtasks arranged for optimal execution. This creates a roadmap for the LLM, enabling precise task tracking and reliable validation. Completion is judged not just by output but by strict adherence to the original plan—gaps trigger feedback and retry loops until full alignment is achieved</p>\n<h3 id=\"retry-and-fall-back-mechanisms\">Retry and Fall-back Mechanisms</h3>\n<p>When a tool call fails, Qodo agents don’t stop—they adapt. The system extracts error feedback, invokes the LLM to diagnose the failure, and intelligently adjusts the tool parameters or structure. Agents are empowered to retry up to three times if needed, refining their calls each round. If resolution isn’t possible through retries, the agent pivots to alternative strategies, ensuring progress continues despite initial breakdowns.</p>\n<h3 id=\"powered-by-langgraph\">Powered by LangGraph</h3>\n<p>Qodo Command uses LangGraph, a framework for agents and agentic workflows that require structure, modularity and state management, giving Qodo Command modularity and speed. LangGraph enables graph-based orchestration, where each step is a configurable node. This foundation allowed us to reuse and extend proven components from Qodo Gen, our IDE extension—including code analysis, summarization and security scanning—while giving us the flexibility to split, extend and repurpose workflows effortlessly.</p>\n<h3 id=\"agent-tools\">Agent Tools</h3>\n<p><span>Qodo Command combines agentic reasoning with a powerful set of execution tools. These tools allow Qodo’s agents to operate more like expert developers—interacting with real environments, scanning large codebases, and thinking in structured steps.</span></p>\n<ul>\n<li><strong>FileSystem: </strong>Standard tools for reading, writing, and editing files and directories. Since even state-of-the-art (SOTA) LLMs may produce errors when using exact matches with the edit file tool, we have implemented a fallback mechanism that allows fuzzy matching to improve the tool’s success rate.</li>\n<li><strong>Shell Tool:</strong> executing like a real developer, Qodo agents can interact with the system shell to run build scripts and linters, execute test suites and validate hypotheses in real-time</li>\n<li><b>Ripgrep</b>: for deep codebase understanding, Qodo Command is natively designed for optimized usage of ripgrep recursive search tool to locate relevant code across large repositories</li>\n<li><b>Sequential Thinking</b><span>: structured agent reasoning helped contribute to the benchmark results by breaking down tasks into actionable steps. This shows the importance of interacting with ai coding agents in a step-by-step iteration, and how well structured tickets or PRDs can yield better code results. While this tool is not enabled by default, it can be easily added via MCP to any custom agent with Qodo Command.</span></li>\n<li><strong>Web Search:</strong> this tool has been disabled for SWE-bench run to prevent data leakage in solutions</li>\n</ul>\n<h2 id=\"what-makes-qodo-command-exceptional-for-complex-codebases-code-quality\">What Makes Qodo Command Exceptional for Complex Codebases Code Quality</h2>\n<p><a href=\"https://www.qodo.ai/blog/introducing-qodo-gen-cli-build-run-and-automate-agents-anywhere-in-your-sdlc/\"><span>We recently announced Qodo Command</span></a><span> and it’s already transforming how we develop software at Qodo. What makes Qodo Command unique is our foundational focus on automation with integrity. Here’s what you can do with Qodo Command:</span></p>\n<h3 id=\"code-integrity-automations\">Code Integrity Automations</h3>\n<p>Since launch, the Qodo team, our customers and community contributors have been actively building agents using Qodo Command,enabling teams to automate high-impact tasks such as:</p>\n<ul>\n<li>Code review automation</li>\n<li>Test generation</li>\n<li>Documentation generation</li>\n</ul>\n<p>And many more agents that enhance code quality, which&nbsp; you can explore in Qodo Command <a href=\"https://github.com/qodo-ai/agents\">agents</a> repository.</p>\n<h3 id=\"ui-mode-for-reviewing-code\">UI Mode for Reviewing Code</h3>\n<p>Code quality doesn’t stop at generation—it depends on consistent, structured review. That’s why Qodo Command includes a dedicated UI mode with Qodo Merge, our advanced code review agent, built in.</p>\n<p>This integration enables developers to generate and review code in a single, streamlined flow. Every AI-assisted task is automatically routed through a review process that checks for correctness, completeness, and quality—helping teams ship faster without lowering standards.</p>\n<h2 id=\"what-will-you-build-next\">What will you build next?</h2>\n<p>Qodo Command isn’t built for benchmarks – it’s built for your production environment. The same version that ranked in the global top 5 on SWE-bench Verified is available today with a single command:</p>\n<p><code>npm install -g @qodo/command</code></p>\n<p>Use it to automate your code integrity workflows, accelerate code reviews, and generate tests, docs, and feature code—all while maintaining the quality standards your team depends on. It’s the CLI agent we’ve built for ourselves and continue to improve weekly, in the open. And we’re just getting started. Don’t wait, get started today at Qodo Command <a href=\"https://www.qodo.ai/products/qodo-command/\">https://www.qodo.ai/products/qodo-command/</a></p>\n\n\t\t\t\t<div data-post-modal=\"\">\n\t\t\t\t\t<figure data-modal-figure=\"\">\n\t\t\t\t\t\t<img src=\"\" alt=\"\" data-modal-img=\"\">\n\t\t\t\t\t</figure>\n\t\t\t\t</div>\n\t\t\t</div></div>","textContent":"\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\tWe’re excited to announce that Qodo Command, our CLI agent, achieved a scored of 71.2% on SWE-bench Verified (submission pending review), the leading benchmark for evaluating AI agents on real-world software engineering tasks.\n\nThis achievement is a strong signal that Qodo’s agents are built for the realities of production development. For use cases like reviewing code, writing tests, fixing bugs, and generating features, our CLI agent goes beyond autocomplete to deliver thoughtful, context-aware, and high-integrity code. \nOne-Shot, Real-World Execution\nMost AI benchmarks evaluate agents in isolated, simplified environments. However, SWE-bench Verified tests coding agents in messy, complex, real-world software engineering scenarios. Each test case in SWE-bench is built from a real GitHub issue in one of 12 widely-used, open-source Python repositories. Agents are given the GitHub issue and the codebase in the state it was in when the issue was opened, and must reason, plan, and edit code, iterating over many turns as a developer would – without shortcutting the problem.\nQodo Command scored 71.2% using a single run of the production version of Qodo Command—no finetuning or benchmark-specific adjustments—exactly the way any developer would by running it out-of-the-box with the simple install package: npm install -g @qodo/command. \nLLM Model Flexibility & Claude Partnership\nWhile Qodo Command is designed to support all top-tier LLMs, Claude 4 emerged as our model of choice for SWE Bench Verified results. Thanks to a strong partnership with Anthropic—Qodo is a “Powered by Claude” solution, we’re collaboratively building the world’s most adaptive and learning-oriented coding agents, leveraging one of the most advanced language models available today.\nThe Architecture Behind Our 71.2% SWE-bench success\nAchieving high performance on SWE-bench wasn’t about optimizing for the benchmark–it was the natural result of engineering Qodo Command to excel at real-world software engineering challenges. Here’s how our architectural decisions directly contributed to its performance:\nContext Summarization\nSWE-bench Verified tests AI on complex, muli-file codebases where understanding interdependencies is crucial for success. Succeeding in this environment requires more than feeding raw files to an LLM, pattern-matching or autocomplete.\nQodo Command solves this by distilling multi-layered code into precise, high-signal summaries—ensuring that language models receive only the most relevant, structured context at every step. This enables deep reasoning, accurate generation, and high-quality reviews without hitting context limits or losing essential detail.\nExecution Planning\nQodo’s default plan-first approach ensures that implementation begins only after a structured breakdown of user intent. Instead of rushing into action, we first deeply analyze the user’s goal, then decompose it into clear, actionable subtasks arranged for optimal execution. This creates a roadmap for the LLM, enabling precise task tracking and reliable validation. Completion is judged not just by output but by strict adherence to the original plan—gaps trigger feedback and retry loops until full alignment is achieved\nRetry and Fall-back Mechanisms\nWhen a tool call fails, Qodo agents don’t stop—they adapt. The system extracts error feedback, invokes the LLM to diagnose the failure, and intelligently adjusts the tool parameters or structure. Agents are empowered to retry up to three times if needed, refining their calls each round. If resolution isn’t possible through retries, the agent pivots to alternative strategies, ensuring progress continues despite initial breakdowns.\nPowered by LangGraph\nQodo Command uses LangGraph, a framework for agents and agentic workflows that require structure, modularity and state management, giving Qodo Command modularity and speed. LangGraph enables graph-based orchestration, where each step is a configurable node. This foundation allowed us to reuse and extend proven components from Qodo Gen, our IDE extension—including code analysis, summarization and security scanning—while giving us the flexibility to split, extend and repurpose workflows effortlessly.\nAgent Tools\nQodo Command combines agentic reasoning with a powerful set of execution tools. These tools allow Qodo’s agents to operate more like expert developers—interacting with real environments, scanning large codebases, and thinking in structured steps.\n\nFileSystem: Standard tools for reading, writing, and editing files and directories. Since even state-of-the-art (SOTA) LLMs may produce errors when using exact matches with the edit file tool, we have implemented a fallback mechanism that allows fuzzy matching to improve the tool’s success rate.\nShell Tool: executing like a real developer, Qodo agents can interact with the system shell to run build scripts and linters, execute test suites and validate hypotheses in real-time\nRipgrep: for deep codebase understanding, Qodo Command is natively designed for optimized usage of ripgrep recursive search tool to locate relevant code across large repositories\nSequential Thinking: structured agent reasoning helped contribute to the benchmark results by breaking down tasks into actionable steps. This shows the importance of interacting with ai coding agents in a step-by-step iteration, and how well structured tickets or PRDs can yield better code results. While this tool is not enabled by default, it can be easily added via MCP to any custom agent with Qodo Command.\nWeb Search: this tool has been disabled for SWE-bench run to prevent data leakage in solutions\n\nWhat Makes Qodo Command Exceptional for Complex Codebases Code Quality\nWe recently announced Qodo Command and it’s already transforming how we develop software at Qodo. What makes Qodo Command unique is our foundational focus on automation with integrity. Here’s what you can do with Qodo Command:\nCode Integrity Automations\nSince launch, the Qodo team, our customers and community contributors have been actively building agents using Qodo Command,enabling teams to automate high-impact tasks such as:\n\nCode review automation\nTest generation\nDocumentation generation\n\nAnd many more agents that enhance code quality, which  you can explore in Qodo Command agents repository.\nUI Mode for Reviewing Code\nCode quality doesn’t stop at generation—it depends on consistent, structured review. That’s why Qodo Command includes a dedicated UI mode with Qodo Merge, our advanced code review agent, built in.\nThis integration enables developers to generate and review code in a single, streamlined flow. Every AI-assisted task is automatically routed through a review process that checks for correctness, completeness, and quality—helping teams ship faster without lowering standards.\nWhat will you build next?\nQodo Command isn’t built for benchmarks – it’s built for your production environment. The same version that ranked in the global top 5 on SWE-bench Verified is available today with a single command:\nnpm install -g @qodo/command\nUse it to automate your code integrity workflows, accelerate code reviews, and generate tests, docs, and feature code—all while maintaining the quality standards your team depends on. It’s the CLI agent we’ve built for ourselves and continue to improve weekly, in the open. And we’re just getting started. Don’t wait, get started today at Qodo Command https://www.qodo.ai/products/qodo-command/\n\n\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\n\t\t\t","length":7525,"excerpt":"Read about Qodo Command scores 71.2% on SWE-bench Verified in our blog.","siteName":"Qodo","publishedTime":"2025-08-11T00:01:46+00:00","url":"https://www.qodo.ai/blog/qodo-command-swe-bench-verified/","addedTime":"2025-08-12,08:41"}