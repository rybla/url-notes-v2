{"title":"Infographics: Operation Costs in CPU Clock Cycles - IT Hare on Soft.ware","byline":"Author:","lang":"en-US","content":"<div id=\"readability-page-1\" class=\"page\"><article itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" itemprop=\"blogPost\"><div itemprop=\"text\">\n<p><em>UPDATED: TLB and CAS/atomics (including different NUMA node) added</em></p>\n<p><a href=\"http://ithare.com/wp-content/uploads/part101_infographics_v08.png\"><img src=\"http://ithare.com/wp-content/uploads/part101_infographics_v08.png\" alt=\"Operation costs in CPU clock cycles on x86/x64 Platform\" width=\"640\" height=\"533\"></a></p>\n<p><em>Click to enlarge</em><br>\n<em>NB: scale is logarithmic!</em></p>\n\n<p><span><span><span><span>Premature Pessimization</span> <span>Easy on yourself, easy on the code: All other things being equal, notably code complexity and readability, certain efficient design patterns and coding idioms should just flow naturally from your fingertips and are no harder to write than the pessimized alternatives. This is not premature optimization; it is avoiding gratuitous pessimization.</span></span><span>‚Äî Herb Sutter, Andrei Alexandrescu ‚Äî</span></span></span>Whenever we need to optimise the code, we should profile it, plain and simple. However, sometimes it makes sense just to know ballpark numbers for relative costs of some popular operations, so you won‚Äôt do grossly inefficient things from the very beginning (and hopefully won‚Äôt need to profile the program later üôÇ ).</p>\n<p>So, here it goes ‚Äì an infographics&nbsp;which should help to estimate costs of certain operations in CPU clocks cycles ‚Äì and answer the questions such as&nbsp;‚Äúhey, how much L2 read usually costs?‚Äù. While answers to all these questions are more or less known, I don‚Äôt know of a single place where all of them are listed and put into perspective.&nbsp;Let‚Äôs also note&nbsp;that while the listed numbers, strictly speaking, apply&nbsp;only to modern x86/x64 CPUs, similar patterns of relative operation costs&nbsp;are expected to&nbsp;be observed on other modern CPUs with large multi-level caches (such as ARM Cortex A, or SPARC); on the other hand, MCUs (including ARM Cortex M) are different enough so some of the patterns may be different there.</p>\n<p>Last but not least, a word of caution: all the estimates here are just indications of the order of magnitude; however, given the scale of differences between different operations, these indications may&nbsp;still be of use&nbsp;(at least to be kept in mind to avoid ‚Äúpremature pessimisation‚Äù).</p>\n<p>On the other hand, I am still sure that such a diagram is useful to avoid saying things ‚Äúhey, virtual function calls cost nothing‚Äù ‚Äì which may or may not be true depending on how often you call them. Instead, using the infographics above ‚Äì you‚Äôll be able to see that</p>\n<div><p>if you call your virtual function 100K times per second on a 3GHz CPU ‚Äì it probably won‚Äôt cost you more than 0.2% of your CPU total; however, if you‚Äôre calling the same virtual function 10M times per second ‚Äì it can easily mean that&nbsp;virtualisation eats up double-digit percentages of your CPU core.</p></div>\n<p>Another way of approaching the same question&nbsp;is to say that ‚Äúhey, I‚Äôm calling virtual function once per piece of code which is like 10000 cycles, so virtualisation won‚Äôt eat more than 1% of the program time‚Äù ‚Äì but you still need some kind of way to see an order of magnitude for the related costs ‚Äì and the diagram above will still come in handy üôÅ .</p>\n<p>Preliminaries aside, let‚Äôs take a closer look at those items on our infographics above.</p>\n<h2>ALU/FPU Operations</h2>\n<p>For our purposes, when speaking about ALU operations, we will consider only register-register ones. If memory is involved, the costs can be VERY different ‚Äì and will depend on ‚Äúhow bad the cache miss was‚Äù for the memory access, as discussed below.</p>\n<h3>‚ÄúSimple‚Äù Operations</h3>\n<p>These days (and on modern CPUs), ‚Äúsimple‚Äù operations such as ADD/MOV/OR/‚Ä¶ can easily have costs of less than 1 CPU cycle. This doesn‚Äôt mean that the operation will be literally performed in half a cycle. Instead&nbsp;‚Äì</p>\n<div><p>while all operations are still performed in a whole&nbsp;number of cycles, some of them can be performed in parallel</p></div>\n<p>In <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner4\">[Agner4]</a> (which BTW is IMO the best reference guide on CPU operation costs), this phenomenon is represented by each operation having two associated numbers ‚Äì one is <em>latency</em> (which is always a whole number of cycles), and another is&nbsp;<em>throughput</em>. It should be noted, however, that in real-world, when going beyond&nbsp;<em>order of magnitude&nbsp;</em>estimates, exact timing will depend a lot on the nature of your program, and on the order in which the compiler has put seemingly-unrelated instructions; in short ‚Äì whenever you need something better than an order-of-magnitude guesstimate, you need to profile your specific program, compiled with your specific compiler (and ideally ‚Äì on a specific target CPU too).</p>\n<p>Further discussion of such&nbsp;techniques (known as ‚Äúout of order execution‚Äù), while being Really Interesting, is going to be way too hardware-related (what about ‚Äúregister renaming‚Äù which happens under the hood of CPU to reduce dependencies which prevent out-of-order from working efficiently?), and is clearly out of our scope at the moment üôÅ .</p>\n<h3>Integer Multiplication/Division</h3>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0006b.png\" alt=\"Surprised hare:\"><span><span>‚Äú</span>Integer multiplication/division is quite expensive compared to 'simple' operations above.</span></span>Integer multiplication/division is quite expensive compared to ‚Äúsimple‚Äù operations above.&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner4\">[Agner4]</a> gives cost of 32/64-bit multiplication (MUL/IMUL in x86/x64 world) at between 1-7 cycles (in practice, I‚Äôve observed more narrow range of values, such as 3-6 cycles), and cost of 32/64-bit division (known as DIV/IDIV on x86/64) ‚Äì at between 12-44 cycles.</p>\n<h3>Floating-Point Operations</h3>\n<p>Costs of floating-point operations are taken from <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner4\">[Agner4]</a>, and range from 1-3 CPU cycles for addition (FADD/FSUB) and 2-5 cycles for multiplication (FMUL), to 37-39 cycles for division (FDIV).</p>\n<p>If using SSE scalar operations (which apparently every compiler and its dog&nbsp;does these days), the numbers will go down to 0.5-5 cycles for multiplication (MULSS/MULSD), and to 1-4o&nbsp;cycles for division (DIVSS/DIVSD); in practice, however, you should expect more like 10-40 cycles for division (1 cycle is ‚Äúreciprocal throughput‚Äù, which is rarely achievable in practice).</p>\n<h3>128-bit Vector Operations</h3>\n<p>For quite a few years, CPUs are providing ‚Äúvector‚Äù operations (more precisely ‚Äì Single Instruction Multiple Data a.k.a. SIMD operations); in Intel world they‚Äôre known as SSE and AVX and in ARM world ‚Äì as ARM Neon. One funny thing about them is that they operate on ‚Äúvectors‚Äù of data, with data being of the same size (128 bit for SSE2-SSE4, 256 bit for AVX and AVX2, and 512 bits for upcoming AVX-512) ‚Äì but interpretations of these bits being different. For example, 128-bit SSE2 register can be interpreted as (a) two doubles, (b) four floats, (c) two 64-bit integers, (d) four 32-bit integers, (e) eight 16-bit integers, (f) 16 8-bit integers.</p>\n<p><a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner4\">[Agner4]</a> gives the costs of integer addition over 128-bit vector at &lt; 1 cycle if the vector is interpreted as 4√ó32-bit integers, and at 4 cycles if it is 2√ó64-bit integers; multiplication (4√ó32 bits) goes at 1-5 cycles ‚Äì and last time I checked, there were no integer division vector operations in x86/x64 instruction set. For floating-point operations over 128-bit vectors, the numbers go from 1-3 CPU cycles for addition and 1-7 CPU cycles for multiplication, to 17-69 cycles for division.</p>\n<h3>Bypass Delays</h3>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0027b.png\" alt=\"Wtf hare:\"><span><span>‚Äú</span>switching between integer and floating-point instructions is not free</span></span>One not-so-obvious thing&nbsp;related to calculation costs, is that switching between integer and floating-point instructions is not free. <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner3\">[Agner3]</a> gives this cost (known as ‚Äúbypass delay‚Äù) at 0-3 CPU cycles depending on the CPU. Actually, the problem&nbsp;is more generic than that, and (depending on CPU) there can also be penalties for switching between vector (SSE) integer instructions and usual (scalar) integer instructions.</p>\n<p><em>Optimisation hint:&nbsp;</em>in performance-critical code, avoid mixing floating-point and integer calculations.</p>\n<h2>Branching</h2>\n<p>The next thing which we‚Äôll be discussing, is branching code. Branch (an&nbsp;<em>if </em>within your program) is essentially a comparison, plus a change in the program counter. While both these things are simple, there can be a significant cost associated with branching. Discussing why it is the case, is once again way too hardware-related (in particular, pipelining and speculative execution are two things involved here), but from the software developer‚Äôs perspective it looks as follows:</p>\n<ul>\n<li>if the CPU guesses correctly&nbsp;where the execution will go (that‚Äôs before actually calculating condition of&nbsp;<em>if)</em>, then cost of branching operation is about 1-2 CPU cycles.</li>\n<li>however, if the CPU makes an incorrect guess ‚Äì it results in CPU effectively ‚Äústalling‚Äù</li>\n</ul>\n<p>The amount of this stall is estimated at 10-20 CPU cycles <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Wikipedia.BranchPredictor\">[Wikipedia.BranchPredictor]</a>, for recent Intel CPUs ‚Äì around 15-20 CPU cycles <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner3\">[Agner3]</a>.</p>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0010b.png\" alt=\"Inquisitive hare:\"><span><span>‚Äú</span>on modern Intel CPUs branch prediction is always dynamic (or at least dominated by dynamic decisions)</span></span>Let‚Äôs note that while GCC‚Äôs __builtin_expect() macro is widely believed to affect branch prediction ‚Äì and it used to work this way just 15 years ago, it is no longer the case at least for Intel CPUs anymore (since Core 2 or so). As described in&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner3\">[Agner3]</a>, &nbsp;on modern Intel CPUs branch prediction is always dynamic (or at least dominated by dynamic decisions); this in turn, implies that __builtin_expect()-induced&nbsp;differences in the code are not expected to have any effect on branch prediction (on modern&nbsp;Intel CPUs, that is). However, __builtin_expect() still has effect on the way code is generated, as described in ‚ÄúMemory Access‚Äù section below.</p>\n<h2>Memory Access</h2>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0016b.png\" alt=\"Dreaming hare:\"><span><span>‚Äú</span>Back in 80s, it was possible to calculate the speed of the program just by looking at assembly.</span></span>Back in 80s, CPU speed was comparable with memory latency (for example, Z80 CPU, running at 4MHz, spent 4 cycles on a register-register instruction, and 6 cycles on a register-memory instruction). At that time, it was possible to calculate the speed of the program just by looking at assembly.</p>\n<p>Since that point, speeds of CPUs have grown by 3 orders of magnitude, while memory latency has improved only 10-30-fold or so. To deal with remaining 30x+ discrepancy, all kinds of caches were introduced. Modern CPU usually has 3 levels of caches. As a result, speed of memory access depends very significantly on the answer to ‚Äúwhere the data we‚Äôre trying to read, is residing?‚Äù The lower the cache level where your request was found ‚Äì the faster you can get it.</p>\n<p>L1 and L2 cache access times can be found in official documents such as <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Intel.Skylake\">[Intel.Skylake]</a>; it lists access L1/L2/L3 times at 4/12/44 CPU cycles respectively (NB: these numbers vary slightly from one CPU model to another one). Actually, as mentioned in <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Levinthal\">[Levinthal]</a>, L3 access times can go as high as 75 cycles if the cache line is shared with another core.</p>\n<p>However, what is more difficult to find, is information about main RAM access times.&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Levinthal\">[Levinthal]</a> gives it at 60ns (~180 cycles if CPU is running at 3GHz).</p>\n<p><em>Optimisation hint:&nbsp;</em>DO improve data locality. For more discussion on it, see, for example, <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-NoBugs\">[NoBugs]</a>.</p>\n<p>Besides&nbsp;memory reads, there are also memory writes. While intuitively write is perceived to be more expensive than read, most often it is not; the reason for it is simple ‚Äì CPU doesn‚Äôt need to wait for the write to complete before going forward (instead, it just starts writing ‚Äì and goes ahead with the other business). This means that most of the time, CPU can perform memory write in ~1 cycle; this is consistent with my experience, and&nbsp;<em>seems&nbsp;</em>to correlate with <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner4\">[Agner4]</a> reasonably well. On the other hand, if your system happens to be memory-bandwidth-bound, numbers can get EXTREMELY high; still, from what I‚Äôve seen, having memory bandwidth overloaded by&nbsp;<em>writes</em>&nbsp;is a very rare occurrence, so I didn‚Äôt reflect it on the diagram.</p>\n<p>And besides data, there is also code.</p>\n<p><em>Another optimisation hint:</em>&nbsp;try to improve code locality too. This one is less obvious (and usually has less drastic effects on performance than poor data locality). Discussion on the ways to improve code locality&nbsp;can be found in <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Drepper\">[Drepper]</a>; these ways include such things as inlining, and __builtin_expect().</p>\n<p>Let‚Äôs note that while __builtin_expect(), as mentioned above, doesn‚Äôt have effect on branch prediction on Intel CPUs anymore, it still has an effect on the code layout, which in turn&nbsp;impacts code spatial locality. As a result, __builtin_expect() doesn‚Äôt have effects which are too pronounced on modern Intel CPUs (on ARM ‚Äì I have no idea TBH), but still can affect a thing or three performance-wise. Also there were reports that under MSVC, swapping&nbsp;<em>if&nbsp;</em>and&nbsp;<em>else&nbsp;</em>branches of&nbsp;<em>if&nbsp;</em>statement has effects which are similar to __builtin_expect() ones (with ‚Äúlikely‚Äù branch being the&nbsp;<em>if&nbsp;</em>branch of two-handed&nbsp;<em>if</em>), but make sure to take it with a good pinch of salt.</p>\n<h3>NUMA</h3>\n<p>One further thing which is related to memory accesses and performance, is rarely observed on desktops (as it requires multi-socket machines ‚Äì not to be confused with multi-core ones). As such, it is mostly server-land; however, it does affect memory access times significantly.</p>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0008b.png\" alt=\"Hare pointing out:\"><span><span>‚Äú</span>When multiple sockets are involved, modern CPUs tend to implement so-called NUMA architecture, with each processor having its own RAM</span></span>When multiple sockets are involved, modern CPUs tend to implement so-called NUMA architecture, with each processor (where ‚Äúprocessor‚Äù = ‚Äúthat thing inserted into a socket‚Äù) having its own RAM (opposed to earlier-age FSB architecture with shared FSB a.k.a. Front-Side Bus, and shared RAM). In spite of each of the CPUs having its own RAM, CPUs&nbsp;share RAM address space ‚Äì and whenever one needs access to RAM physically located within another one ‚Äì it is done by sending a request to another socket via ultra-fast protocol such as QPI or Hypertransport.</p>\n<p>Surprisingly, this doesn‚Äôt take as long as you might have expected ‚Äì&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Levinthal\">[Levinthal]</a> gives the numbers of&nbsp;100-300 CPU cycles if the data was in the remote CPU L3 cache, and of&nbsp;100ns (~=300 cycles) if the data wasn‚Äôt there, and remote CPU needed to go to its own main RAM for this data.</p>\n<h3>CAS</h3>\n<p>Sometimes (in particular, in non-blocking algorithms and while implementing mutexes), we want to use so-called atomic operations. In academy, only one atomic operation, known as&nbsp;CAS (Compare-And-Swap) is usually considered (on the grounds that everything else can be implemented via CAS); in real-world, there are usually more of them&nbsp;(see, for example, std::atomic in C++11, Interlocked*() functions in Windows, or __sync_*_and_*() in GCC/Linux). These operations are quite weird beasts ‚Äì in particular, they require special CPU support to work properly. On x86/x64, appropriate ASM instructions are&nbsp;characterised by having LOCK prefix, so CAS on x86/x64 is usually written as LOCK CMPXCHG.</p>\n<p>What matters from our current perspective is that these CAS-like operations are going to take significantly longer than usual memory access (to provide atomic guarantees, CPU needs to synchronise things at least between different cores ‚Äì or in case of multi-socket configurations, also between different sockets).</p>\n<p><a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-AlBahra\">[AlBahra]</a> gives the cost of CAS operations at about 15-30 clock cycles (with little difference between x86 and IBM Power families). Let‚Äôs note that this number is valid only when two assumptions stand: (a) we‚Äôre working with a&nbsp;single-core configuration, and (b) that CAS-ed memory is already in L1 cache.</p>\n<p>As for CAS costs in multi-socket NUMA configurations ‚Äì I wasn‚Äôt able to find the data about CAS, so I will need to speculate for the time being üôÅ . On the one hand, IMO it will be next-to-impossible to have latencies of CAS operation on ‚Äúremote‚Äù memory less than round-trip of HyperTransport between the sockets, which in turn is comparable to the cost of NUMA L3 cache read. On the other hand, I don‚Äôt really see the reasons to go higher&nbsp;than that :-). As a result, I am guesstimating the cost of NUMA different-socket CAS (and CAS-like) operations at around 100-300 CPU clock cycles.</p>\n<h3>TLB</h3>\n<p>Whenever we‚Äôre working with modern CPUs and modern OS‚Äôs, at app-level we are usually dealing with ‚Äúvirtual‚Äù address space; in other words ‚Äì if we run 10 processes, <em>each&nbsp;</em>of these processes can (and probably will) have its own address 0x00000000. To support&nbsp;such isolation, CPUs implement&nbsp;so-called ‚Äúvirtual memory‚Äù. In x86 world ‚Äì it was first implemented via ‚Äúprotected mode‚Äù introduced as early as 1982 in 80286.</p>\n<p>Usually, ‚Äúvirtual memory‚Äù works on per-page basis (for x86 each page is either 4K or 2M or at least in theory ‚Äì even 1G(!) in size), with CPU being aware of the current process being run (!), and re-mapping virtual addresses into physical addresses ‚Äì on each memory access, that is. Note that this re-mapping occurs completely behind the scenes, in a sense that all CPU registers (except for those specifically dealing with mapping) contain all the pointers in ‚Äúvirtual memory‚Äù format.</p>\n<p>And as soon as we said ‚Äúmapping‚Äù ‚Äì well, the information about this mapping needs to be stored somewhere. Moreover, as this mapping (from virtual addresses into physical addresses) happens&nbsp;<em>on each and every memory access ‚Äì&nbsp;</em>it needs to be Damn Fast. To help with it, a special kind of cache, known as Translation Lookaside Buffer (TLB) is normally used.</p>\n<p>As for any type of cache, there is a cost of missing TLB; for x64 it is reported between 7-21 CPU cycles <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-7cpu\">[7cpu]</a>. Overall, TLBs are quite difficult to affect; however, a few recommendations can still be made in this regard:</p>\n<ul>\n<li>once again ‚Äì improving overall memory locality helps to reduce TLB misses too; the more local your data is ‚Äì the less your chances are to get out of TLB.</li>\n<li>consider using ‚Äúhuge pages‚Äù (those 2M pages on x64). The larger pages are ‚Äì the less entries&nbsp;in TLB you‚Äôll need; on the other hand, using ‚Äúhuge pages‚Äù&nbsp;comes with some caveats, and as a result ‚Äì is a two-edged sword. Which means that you need to&nbsp;make sure to test it for your specific app.</li>\n<li>consider turning off ASLR (=‚ÄùAddress Space Layout Randomization‚Äù). As discussed in <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Drepper\">[Drepper]</a>, enabling ASLR, while being good for security,&nbsp;hits performance, and exactly because of TLB misses too üôÅ .</li>\n</ul>\n<h2>Software Primitives</h2>\n<p>Now we‚Äôre done with those things which are&nbsp;directly hardware-related, and will be speaking about certain things which are more software-related; still, they‚Äôre really ubiquitous, so let‚Äôs see how much we spend every time we‚Äôre using them.</p>\n<h3>C/C++ Function Calls</h3>\n<p>First, let‚Äôs see the cost of C/C++ function call. Actually, C/C++ caller&nbsp;does a damn lot of stuff before making a call ‚Äì and callee makes another few things too.</p>\n<p><a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Efficient C++\">[Efficient C++]</a> estimates CPU costs for a function call at 25-250 CPU cycles depending on number of parameters; however, it is quite an old book, and I don‚Äôt have a better reference of the same caliber üôÅ . On the other hand, from my experience, for a function with a reasonably small number of parameters, it is more like 15-30 cycles; this also seems to apply to non-Intel CPUs as measured by&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-eruskin\">[eruskin]</a>.</p>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0003b.png\" alt=\"Hare with hopeless face:\"><span><span>‚Äú</span>keep in mind that these days compilers tend to ignore <em>inline </em>specifications more often than not</span></span><em>Optimisation hint: </em>Use&nbsp;<em>inline&nbsp;</em>functions where applicable. However, keep in mind that these days compilers tend to ignore&nbsp;<em>inline&nbsp;</em>specifications more often than not üôÅ . Therefore, for really time-critical pieces of code you may want to consider&nbsp;<em>__attribute__((always_inline))</em> for GCC, and&nbsp;<em>__forceinline&nbsp;</em>for MSVC compilers to make them do what you need. However, do NOT overuse this forced-inline stuff for not-so-critical pieces of code, it can&nbsp;make things worse rather easily.</p>\n<p>BTW, in many&nbsp;cases gains from inlining can exceed simple removal of call costs.&nbsp;This happens because inlining enables quite a few additional optimisations (including those related to reordering to achieve the proper use of hardware pipeline). Also let‚Äôs not forget that inlining improves spatial locality for the code ‚Äì which tends to help a bit too (see, for example, <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Drepper\">[Drepper]</a>).</p>\n<h4>Indirect and Virtual Calls</h4>\n<p>Discussion above was related to usual (‚Äúdirect‚Äù) function calls. Costs of indirect and virtual calls are known to be higher, and there is pretty much a consensus on that indirect call causes branching (however, as&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Agner1\">[Agner1]</a> notes, as long as you happen to call the same function from the same point in code, branch predictors of modern CPUs are able to predict it pretty good; otherwise ‚Äì you‚Äôll get a misprediction penalty of 10-30 cycles). As for virtual calls ‚Äì it is one extra read (reading VMT pointer), so if everything is cached at this point (which it usually is), we‚Äôre speaking about additional 4 CPU cycles or so.</p>\n<p>On the other hand, practical measurements from&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-eruskin\">[eruskin]</a> show that the cost of virtual functions is roughly double of the direct call costs for small functions; within our margin of error (which is ‚Äúan order of magnitude‚Äù) this is quite consistent with the analysis above.</p>\n<p><span><span><span><span><a href=\"https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern\">Curiously recurring template pattern</a></span> <span>The curiously recurring template pattern (CRTP) is an idiom in C++ in which a class X derives from a class template instantiation using X itself as template argument</span></span><span>‚Äî Wikipedia ‚Äî</span></span></span><em>Optimisation hint:&nbsp;</em>IF your virtual calls are expensive ‚Äì in C++ you may want to think about using templates instead (implementing so-called compile-time polymorphism); CRTP is one (though not the only one) way of doing it.</p>\n<h3>Allocations</h3>\n<p>These days, allocators as such can be quite fast; in particular, tcmalloc and ptmalloc2 allocators can take as little as 200-500 CPU cycles for allocation/deallocation of a small object&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-TCMalloc\">[TCMalloc]</a>.</p>\n<p>However, there is a significant caveat related to allocation ‚Äì and adding to indirect costs of using allocations: allocation, as a Big Fat rule of thumb, reduces memory locality, which in turn&nbsp;adversely affects performance (due to uncached memory accesses described above). Just to illustrate how bad this can be in practice, we can take a look at a 20-line program in <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-NoBugs\">[NoBugs]</a>;&nbsp;this program, when using&nbsp;<em>vector&lt;&gt;,&nbsp;</em>happens to be from 100x to 780x faster (depending on compiler and specific box) than an equivalent program using&nbsp;<em>list&lt;&gt;&nbsp;</em>‚Äì all because of poor memory locality of the latter :-(.</p>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0023b.png\" alt=\"Hare with an idea:\"><span><span>‚Äú</span>In some real-world cases flattening your data structures can speed up your program as much as 5x.</span></span><em>Optimisation hint:&nbsp;</em>DO think about reducing number of allocations within&nbsp;your programs ‚Äì especially if there is a stage when lots of work is done on a&nbsp;read-only data.&nbsp;In some real-world cases flattening your data structures (i.e. replacing allocated objects with packed ones) can speed up your program&nbsp;as much as 5x. A real-world story in this regard. Once upon a time, there was a program which used some gigabytes of RAM, which was deemed too much; ok, I rewrote it to a ‚Äúflattened‚Äù version (i.e. each node was first constructed dynamically, and then an equivalent ‚Äúflattened‚Äù read-only object was created in memory); the idea of ‚Äúflattening‚Äù was to reduce memory footprint. When we ran the program, we observed that not only memory footprint was reduced by 2x (which was what we expected), but that also ‚Äì as a very nice side effect ‚Äì execution speed went up by 5x.</p>\n<h3>Kernel Calls</h3>\n<p>If our program runs under an operating system,<a href=\"#rabbitfootnote-1\"><sup>1</sup></a> then we have a whole bunch of system APIs. In practice,<a href=\"#rabbitfootnote-2\"><sup>2</sup></a> quite a few of those system calls cause kernel calls, which involve switches to kernel mode and back; this includes switching between different ‚Äúprotection rings‚Äù (on Intel CPU ‚Äì usually between ‚Äúring 3‚Äù and ‚Äúring 0‚Äù). While this&nbsp;CPU-level switching back and forth itself takes only ~100 CPU cycles, other related overheads tend to make kernel calls much more expensive, so usual kernel call takes at least 1000-1500 CPU cycles üôÅ <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Wikipedia.ProtectionRing\">[Wikipedia.ProtectionRing]</a>.</p>\n<hr><p><sup>1</sup> yes, there are still programs which run without it</p><p><sup>2</sup> at least if we‚Äôre speaking about more or less conventional OS</p>\n\n<h3>C++ Exceptions</h3>\n<p>These days, C++ exceptions are said to be zero-cost until thrown. Whether it is really zero ‚Äì is still not 100% clear (IMO it is even unclear whether such a question can be asked at all), but it is certainly very close.</p>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotion_0005b.png\" alt=\"Hare with omg face:\"><span><span>‚Äú</span>these 'zero-cost until thrown' exception implementations come at the cost of a <em>huge</em> pile of work which needs to be done whenever an exception <em>is </em>thrown</span></span>However, these ‚Äúzero-cost until thrown‚Äù implementations come at the cost of a <em>huge</em> pile of work which needs to be done whenever an exception&nbsp;<em>is&nbsp;</em>thrown. Everybody agrees that the cost of exception thrown is huge, however (as usual) experimental data is scarce. Still, an experiment by <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-Ongaro\">[Ongaro]</a> gives us a ballpark number of&nbsp;around 5000 CPU&nbsp;cycles (sic!).&nbsp;Moreover, in more complicated cases, I would expect it to take even more.</p>\n<h3>Return Error and Check</h3>\n<p>An ages-old alternative to exceptions is returning error codes and checking them at each level. While I don‚Äôt have references for performance measurements of this kind of things, we already know enough to make a reasonable guesstimate. Let‚Äôs take a closer look at it (we don‚Äôt care much about performance in the case when error arises, so will concentrate on costs when everything is fine).</p>\n<p>Basically, cost of return-and-check consists of three separate costs. The first one is the cost of conditional jump itself ‚Äì and we can safely assume that 99+% of the time it will be predicted correctly; which means the cost of conditional jump in this case is around 1-2 cycles. The second cost is the cost of copying the error code around ‚Äì and as long as&nbsp;it stays within the registers, it is a simple MOV ‚Äì which is, given the circumstances, is 0 to 1 cycles (0 cycles means that MOV&nbsp;has no additional cost, as it is performed in parallel with some other stuff). The third cost is much less obvious ‚Äì it is a cost of the extra register necessary to carry the error code; if we‚Äôre out of registers ‚Äì we‚Äôll need PUSH/POP pair (or a reasonable facsimile), which is in turn a write + L1 read, or 1+4 cycles. On the other hand, let‚Äôs keep in mind that&nbsp;&nbsp;chances of PUSH/POP being necessary, vary from one platform to another one; for example, on x86 any realistic function would require them almost for sure; however, on x64 (which has double number of registers) ‚Äì chances of PUSH/POP being necessary, go down significantly (and in quite a few&nbsp;cases, even if register is not completely free, making it available may be done by compiler cheaper than a dumb PUSH/POP).</p>\n<p>Adding all three costs together, I‚Äôd guesstimate costs of return-error-code-and-check (in normal case) at anywhere between 1 and 7 CPU cycles. Which in turn means that if we have one exception per 10000 function calls ‚Äì we‚Äôre likely to be better with exceptions; however, if we have one exception per 100 function calls ‚Äì we‚Äôre likely to be better with error codes. In other words, we‚Äôve just reconfirmed a very well-known best practice ‚Äì ‚Äúuse exceptions only for abnormal situations‚Äù üôÇ .</p>\n<h3>Thread Context Switches</h3>\n<p>Last but certainly not least, we need to speak about costs of thread context switches. One&nbsp;problem with estimating them is that, well, it is very difficult to figure them out. Common wisdom tells that they‚Äôre ‚Äúdamn expensive‚Äù (hey, there should be a reason why nginx outperforms Apache) ‚Äì but how much this ‚Äúdamn expensive‚Äù is?</p>\n<p>From my personal observations, the costs were <em>at least</em> 10000 CPU cycles; however, there are lots of sources which are giving MUCH lower numbers. In fact, however, it is all about ‚Äúwhat exactly we‚Äôre trying to measure‚Äù. As noted in <a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-LiEtAl\">[LiEtAl]</a>, there are two different costs with relation to context switches.</p>\n<ul>\n<li>The first cost is direct costs of thread context switching ‚Äì and these are measured at about 2000 CPU cycles<a href=\"#rabbitfootnote-3\"><sup>3</sup></a></li>\n<li>However, the second cost is MUCH higher; it is related to cache invalidation by the thread; according to&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-LiEtAl\">[LiEtAl]</a>, it can be as large as 3M CPU clocks. In theory, with completely random access pattern, modern CPU with 12M of L3 cache (and taking penalty of the order of 50 cycles per access) can take&nbsp;a penalty of up to 10M cycles per context switch; still, in practice the penalties are usually somewhat lower than that, so the number of 1M from&nbsp;<a onclick=\"rabbit_show_references(); return true;\" href=\"#rabbitref-LiEtAl\">[LiEtAl]</a> makes sense. This ‚Äúmuch higher‚Äù estimate is also consistent with the number of spinlocks on x64 (which defaults to 4000 at least for Windows/x64): if it is usually beneficial to wait for 4000 iterations (amounting&nbsp;<em>at the very least&nbsp;</em>to 15-20K CPU cycles, and more like 40-50K cycles from what I‚Äôve seen) reading that variable-which-is-currently-locked within a busy loop ‚Äì just&nbsp;<em>in hope&nbsp;</em>that the variable will unlock before 4000 iterations is over, all of this trouble and CPU cycles merely to avoid a context switch ‚Äì it means that the cost of the context switch is usually&nbsp;<em>much higher&nbsp;</em>than those tens-of-thousands-of-CPU-cycles-we‚Äôre-ready-to-spend-in-a-busy-loop-doing-nothing-useful.</li>\n</ul>\n<hr><p><sup>3</sup> that is, if my math is correct when converting from microseconds&nbsp;into cycles</p>\n\n<h2>Wrapping it Up</h2>\n<p><span><img width=\"170\" height=\"170\" src=\"http://ithare.com/wp-content/uploads/BB_emotionM_0001b.png\" alt=\"Tired hare:\"><span></span></span>Phew, it was quite a bit of work to find&nbsp;references for all these more-or-less known observations.</p>\n<p>Also please note that while I‚Äôve honestly tried to collect all the related costs&nbsp;in one place (checking 3rd-party findings against my own experiences in the process), it is just a very first attempt at this, so if you find reasonably compelling evidence that certain item is wrong ‚Äì please let me know, I will be happy&nbsp;to make the diagram more accurate.</p>\n\n<p><span>Don't like this post?&nbsp;<b></b>.</span> <span>You do?! Please share: <a href=\"http://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fithare.com%2Finfographics-operation-costs-in-cpu-clock-cycles%2F&amp;title=Infographics%3A+Operation+Costs+in+CPU+Clock+Cycles&amp;summary=Infographics%3A+operation+cost+in+cycles+%28from+%3C1+cycle+for+ADD%2FXOR%2F...+to+up+to+a+million+for+a+thread+context+switch%29\" onclick=\"return !window.open(this.href, '_blank', 'width=640,height=535,top='+(screen.height-535)/2+',left='+(screen.width-640)/2)\" target=\"_blank\"><img width=\"26\" height=\"26\" src=\"http://ithare.com/wp-content/uploads/socialnet_linkedin.png\" alt=\"...on LinkedIn\" title=\"Spill Some Sarcasm over LinkedIn\"></a><a href=\"http://reddit.com/submit?url=http%3A%2F%2Fithare.com%2Finfographics-operation-costs-in-cpu-clock-cycles%2F&amp;title=Infographics%3A+operation+costs+in+CPU+clock+cycles\" onclick=\"return !window.open(this.href, '_blank', 'width=860,height=860,top='+(screen.height-860)/2+',left='+(screen.width-860)/2)\" target=\"_blank\"><img width=\"26\" height=\"26\" src=\"http://ithare.com/wp-content/uploads/socialnet_reddit1.png\" alt=\"...on Reddit\" title=\"Stir Controversy on Reddit\"></a><a href=\"http://twitter.com/share?url=http%3A%2F%2Fithare.com%2Finfographics-operation-costs-in-cpu-clock-cycles%2F&amp;text=%23Op+costs+in+%23CPU+%23cycles+%28from+%3C1+for+%23XOR+to+1M+for+%23Thread+%23ContextSwitch%29\" onclick=\"return !window.open(this.href, '_blank', 'width=640,height=440,top='+(screen.height-440)/2+',left='+(screen.width-640)/2)\" target=\"_blank\"><img width=\"26\" height=\"26\" src=\"http://ithare.com/wp-content/uploads/socialnet_twitter1.png\" alt=\"...on Twitter\" title=\"Put your 140 characters worth on Twitter\"></a><a href=\"http://www.facebook.com/sharer.php?u=http%3A%2F%2Fithare.com%2Finfographics-operation-costs-in-cpu-clock-cycles%2F\" onclick=\"return !window.open(this.href, '_blank', 'width=640,height=480,top='+(screen.height-480)/2+',left='+(screen.width-640)/2)\" target=\"_blank\"><img width=\"26\" height=\"26\" src=\"http://ithare.com/wp-content/uploads/socialnet_facebook.png\" alt=\"...on Facebook\" title=\"Share Ingenuity (or not) on Facebook\"></a></span></p><div><h3><span>[<span>+</span>]</span>References</h3></div><h3>Acknowledgement</h3><p>Cartoons by Sergey Gordeev<sup><a href=\"http://ithare.com/real-people-behind-the-hare#sergey-gordeev\"><img width=\"16\" height=\"16\" src=\"http://ithare.com/wp-content/uploads/irl-link.png\" alt=\"IRL\"></a></sup> from <a href=\"http://gagltd.eu/\">Gordeev Animation Graphics</a>, Prague.</p>\n<!--<rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n\t\t\txmlns:dc=\"http://purl.org/dc/elements/1.1/\"\n\t\t\txmlns:trackback=\"http://madskills.com/public/xml/rss/module/trackback/\">\n\t\t<rdf:Description rdf:about=\"http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/\"\n    dc:identifier=\"http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/\"\n    dc:title=\"Infographics: Operation Costs in CPU Clock Cycles\"\n    trackback:ping=\"http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/trackback/\" />\n</rdf:RDF>-->\n</div></article></div>","textContent":"\nUPDATED: TLB and CAS/atomics (including different NUMA node) added\n\nClick to enlarge\nNB: scale is logarithmic!\n\nPremature Pessimization Easy on yourself, easy on the code: All other things being equal, notably code complexity and readability, certain efficient design patterns and coding idioms should just flow naturally from your fingertips and are no harder to write than the pessimized alternatives. This is not premature optimization; it is avoiding gratuitous pessimization.‚Äî Herb Sutter, Andrei Alexandrescu ‚ÄîWhenever we need to optimise the code, we should profile it, plain and simple. However, sometimes it makes sense just to know ballpark numbers for relative costs of some popular operations, so you won‚Äôt do grossly inefficient things from the very beginning (and hopefully won‚Äôt need to profile the program later üôÇ ).\nSo, here it goes ‚Äì an infographics¬†which should help to estimate costs of certain operations in CPU clocks cycles ‚Äì and answer the questions such as¬†‚Äúhey, how much L2 read usually costs?‚Äù. While answers to all these questions are more or less known, I don‚Äôt know of a single place where all of them are listed and put into perspective.¬†Let‚Äôs also note¬†that while the listed numbers, strictly speaking, apply¬†only to modern x86/x64 CPUs, similar patterns of relative operation costs¬†are expected to¬†be observed on other modern CPUs with large multi-level caches (such as ARM Cortex A, or SPARC); on the other hand, MCUs (including ARM Cortex M) are different enough so some of the patterns may be different there.\nLast but not least, a word of caution: all the estimates here are just indications of the order of magnitude; however, given the scale of differences between different operations, these indications may¬†still be of use¬†(at least to be kept in mind to avoid ‚Äúpremature pessimisation‚Äù).\nOn the other hand, I am still sure that such a diagram is useful to avoid saying things ‚Äúhey, virtual function calls cost nothing‚Äù ‚Äì which may or may not be true depending on how often you call them. Instead, using the infographics above ‚Äì you‚Äôll be able to see that\nif you call your virtual function 100K times per second on a 3GHz CPU ‚Äì it probably won‚Äôt cost you more than 0.2% of your CPU total; however, if you‚Äôre calling the same virtual function 10M times per second ‚Äì it can easily mean that¬†virtualisation eats up double-digit percentages of your CPU core.\nAnother way of approaching the same question¬†is to say that ‚Äúhey, I‚Äôm calling virtual function once per piece of code which is like 10000 cycles, so virtualisation won‚Äôt eat more than 1% of the program time‚Äù ‚Äì but you still need some kind of way to see an order of magnitude for the related costs ‚Äì and the diagram above will still come in handy üôÅ .\nPreliminaries aside, let‚Äôs take a closer look at those items on our infographics above.\nALU/FPU Operations\nFor our purposes, when speaking about ALU operations, we will consider only register-register ones. If memory is involved, the costs can be VERY different ‚Äì and will depend on ‚Äúhow bad the cache miss was‚Äù for the memory access, as discussed below.\n‚ÄúSimple‚Äù Operations\nThese days (and on modern CPUs), ‚Äúsimple‚Äù operations such as ADD/MOV/OR/‚Ä¶ can easily have costs of less than 1 CPU cycle. This doesn‚Äôt mean that the operation will be literally performed in half a cycle. Instead¬†‚Äì\nwhile all operations are still performed in a whole¬†number of cycles, some of them can be performed in parallel\nIn [Agner4] (which BTW is IMO the best reference guide on CPU operation costs), this phenomenon is represented by each operation having two associated numbers ‚Äì one is latency (which is always a whole number of cycles), and another is¬†throughput. It should be noted, however, that in real-world, when going beyond¬†order of magnitude¬†estimates, exact timing will depend a lot on the nature of your program, and on the order in which the compiler has put seemingly-unrelated instructions; in short ‚Äì whenever you need something better than an order-of-magnitude guesstimate, you need to profile your specific program, compiled with your specific compiler (and ideally ‚Äì on a specific target CPU too).\nFurther discussion of such¬†techniques (known as ‚Äúout of order execution‚Äù), while being Really Interesting, is going to be way too hardware-related (what about ‚Äúregister renaming‚Äù which happens under the hood of CPU to reduce dependencies which prevent out-of-order from working efficiently?), and is clearly out of our scope at the moment üôÅ .\nInteger Multiplication/Division\n‚ÄúInteger multiplication/division is quite expensive compared to 'simple' operations above.Integer multiplication/division is quite expensive compared to ‚Äúsimple‚Äù operations above.¬†[Agner4] gives cost of 32/64-bit multiplication (MUL/IMUL in x86/x64 world) at between 1-7 cycles (in practice, I‚Äôve observed more narrow range of values, such as 3-6 cycles), and cost of 32/64-bit division (known as DIV/IDIV on x86/64) ‚Äì at between 12-44 cycles.\nFloating-Point Operations\nCosts of floating-point operations are taken from [Agner4], and range from 1-3 CPU cycles for addition (FADD/FSUB) and 2-5 cycles for multiplication (FMUL), to 37-39 cycles for division (FDIV).\nIf using SSE scalar operations (which apparently every compiler and its dog¬†does these days), the numbers will go down to 0.5-5 cycles for multiplication (MULSS/MULSD), and to 1-4o¬†cycles for division (DIVSS/DIVSD); in practice, however, you should expect more like 10-40 cycles for division (1 cycle is ‚Äúreciprocal throughput‚Äù, which is rarely achievable in practice).\n128-bit Vector Operations\nFor quite a few years, CPUs are providing ‚Äúvector‚Äù operations (more precisely ‚Äì Single Instruction Multiple Data a.k.a. SIMD operations); in Intel world they‚Äôre known as SSE and AVX and in ARM world ‚Äì as ARM Neon. One funny thing about them is that they operate on ‚Äúvectors‚Äù of data, with data being of the same size (128 bit for SSE2-SSE4, 256 bit for AVX and AVX2, and 512 bits for upcoming AVX-512) ‚Äì but interpretations of these bits being different. For example, 128-bit SSE2 register can be interpreted as (a) two doubles, (b) four floats, (c) two 64-bit integers, (d) four 32-bit integers, (e) eight 16-bit integers, (f) 16 8-bit integers.\n[Agner4] gives the costs of integer addition over 128-bit vector at < 1 cycle if the vector is interpreted as 4√ó32-bit integers, and at 4 cycles if it is 2√ó64-bit integers; multiplication (4√ó32 bits) goes at 1-5 cycles ‚Äì and last time I checked, there were no integer division vector operations in x86/x64 instruction set. For floating-point operations over 128-bit vectors, the numbers go from 1-3 CPU cycles for addition and 1-7 CPU cycles for multiplication, to 17-69 cycles for division.\nBypass Delays\n‚Äúswitching between integer and floating-point instructions is not freeOne not-so-obvious thing¬†related to calculation costs, is that switching between integer and floating-point instructions is not free. [Agner3] gives this cost (known as ‚Äúbypass delay‚Äù) at 0-3 CPU cycles depending on the CPU. Actually, the problem¬†is more generic than that, and (depending on CPU) there can also be penalties for switching between vector (SSE) integer instructions and usual (scalar) integer instructions.\nOptimisation hint:¬†in performance-critical code, avoid mixing floating-point and integer calculations.\nBranching\nThe next thing which we‚Äôll be discussing, is branching code. Branch (an¬†if within your program) is essentially a comparison, plus a change in the program counter. While both these things are simple, there can be a significant cost associated with branching. Discussing why it is the case, is once again way too hardware-related (in particular, pipelining and speculative execution are two things involved here), but from the software developer‚Äôs perspective it looks as follows:\n\nif the CPU guesses correctly¬†where the execution will go (that‚Äôs before actually calculating condition of¬†if), then cost of branching operation is about 1-2 CPU cycles.\nhowever, if the CPU makes an incorrect guess ‚Äì it results in CPU effectively ‚Äústalling‚Äù\n\nThe amount of this stall is estimated at 10-20 CPU cycles [Wikipedia.BranchPredictor], for recent Intel CPUs ‚Äì around 15-20 CPU cycles [Agner3].\n‚Äúon modern Intel CPUs branch prediction is always dynamic (or at least dominated by dynamic decisions)Let‚Äôs note that while GCC‚Äôs __builtin_expect() macro is widely believed to affect branch prediction ‚Äì and it used to work this way just 15 years ago, it is no longer the case at least for Intel CPUs anymore (since Core 2 or so). As described in¬†[Agner3], ¬†on modern Intel CPUs branch prediction is always dynamic (or at least dominated by dynamic decisions); this in turn, implies that __builtin_expect()-induced¬†differences in the code are not expected to have any effect on branch prediction (on modern¬†Intel CPUs, that is). However, __builtin_expect() still has effect on the way code is generated, as described in ‚ÄúMemory Access‚Äù section below.\nMemory Access\n‚ÄúBack in 80s, it was possible to calculate the speed of the program just by looking at assembly.Back in 80s, CPU speed was comparable with memory latency (for example, Z80 CPU, running at 4MHz, spent 4 cycles on a register-register instruction, and 6 cycles on a register-memory instruction). At that time, it was possible to calculate the speed of the program just by looking at assembly.\nSince that point, speeds of CPUs have grown by 3 orders of magnitude, while memory latency has improved only 10-30-fold or so. To deal with remaining 30x+ discrepancy, all kinds of caches were introduced. Modern CPU usually has 3 levels of caches. As a result, speed of memory access depends very significantly on the answer to ‚Äúwhere the data we‚Äôre trying to read, is residing?‚Äù The lower the cache level where your request was found ‚Äì the faster you can get it.\nL1 and L2 cache access times can be found in official documents such as [Intel.Skylake]; it lists access L1/L2/L3 times at 4/12/44 CPU cycles respectively (NB: these numbers vary slightly from one CPU model to another one). Actually, as mentioned in [Levinthal], L3 access times can go as high as 75 cycles if the cache line is shared with another core.\nHowever, what is more difficult to find, is information about main RAM access times.¬†[Levinthal] gives it at 60ns (~180 cycles if CPU is running at 3GHz).\nOptimisation hint:¬†DO improve data locality. For more discussion on it, see, for example, [NoBugs].\nBesides¬†memory reads, there are also memory writes. While intuitively write is perceived to be more expensive than read, most often it is not; the reason for it is simple ‚Äì CPU doesn‚Äôt need to wait for the write to complete before going forward (instead, it just starts writing ‚Äì and goes ahead with the other business). This means that most of the time, CPU can perform memory write in ~1 cycle; this is consistent with my experience, and¬†seems¬†to correlate with [Agner4] reasonably well. On the other hand, if your system happens to be memory-bandwidth-bound, numbers can get EXTREMELY high; still, from what I‚Äôve seen, having memory bandwidth overloaded by¬†writes¬†is a very rare occurrence, so I didn‚Äôt reflect it on the diagram.\nAnd besides data, there is also code.\nAnother optimisation hint:¬†try to improve code locality too. This one is less obvious (and usually has less drastic effects on performance than poor data locality). Discussion on the ways to improve code locality¬†can be found in [Drepper]; these ways include such things as inlining, and __builtin_expect().\nLet‚Äôs note that while __builtin_expect(), as mentioned above, doesn‚Äôt have effect on branch prediction on Intel CPUs anymore, it still has an effect on the code layout, which in turn¬†impacts code spatial locality. As a result, __builtin_expect() doesn‚Äôt have effects which are too pronounced on modern Intel CPUs (on ARM ‚Äì I have no idea TBH), but still can affect a thing or three performance-wise. Also there were reports that under MSVC, swapping¬†if¬†and¬†else¬†branches of¬†if¬†statement has effects which are similar to __builtin_expect() ones (with ‚Äúlikely‚Äù branch being the¬†if¬†branch of two-handed¬†if), but make sure to take it with a good pinch of salt.\nNUMA\nOne further thing which is related to memory accesses and performance, is rarely observed on desktops (as it requires multi-socket machines ‚Äì not to be confused with multi-core ones). As such, it is mostly server-land; however, it does affect memory access times significantly.\n‚ÄúWhen multiple sockets are involved, modern CPUs tend to implement so-called NUMA architecture, with each processor having its own RAMWhen multiple sockets are involved, modern CPUs tend to implement so-called NUMA architecture, with each processor (where ‚Äúprocessor‚Äù = ‚Äúthat thing inserted into a socket‚Äù) having its own RAM (opposed to earlier-age FSB architecture with shared FSB a.k.a. Front-Side Bus, and shared RAM). In spite of each of the CPUs having its own RAM, CPUs¬†share RAM address space ‚Äì and whenever one needs access to RAM physically located within another one ‚Äì it is done by sending a request to another socket via ultra-fast protocol such as QPI or Hypertransport.\nSurprisingly, this doesn‚Äôt take as long as you might have expected ‚Äì¬†[Levinthal] gives the numbers of¬†100-300 CPU cycles if the data was in the remote CPU L3 cache, and of¬†100ns (~=300 cycles) if the data wasn‚Äôt there, and remote CPU needed to go to its own main RAM for this data.\nCAS\nSometimes (in particular, in non-blocking algorithms and while implementing mutexes), we want to use so-called atomic operations. In academy, only one atomic operation, known as¬†CAS (Compare-And-Swap) is usually considered (on the grounds that everything else can be implemented via CAS); in real-world, there are usually more of them¬†(see, for example, std::atomic in C++11, Interlocked*() functions in Windows, or __sync_*_and_*() in GCC/Linux). These operations are quite weird beasts ‚Äì in particular, they require special CPU support to work properly. On x86/x64, appropriate ASM instructions are¬†characterised by having LOCK prefix, so CAS on x86/x64 is usually written as LOCK CMPXCHG.\nWhat matters from our current perspective is that these CAS-like operations are going to take significantly longer than usual memory access (to provide atomic guarantees, CPU needs to synchronise things at least between different cores ‚Äì or in case of multi-socket configurations, also between different sockets).\n[AlBahra] gives the cost of CAS operations at about 15-30 clock cycles (with little difference between x86 and IBM Power families). Let‚Äôs note that this number is valid only when two assumptions stand: (a) we‚Äôre working with a¬†single-core configuration, and (b) that CAS-ed memory is already in L1 cache.\nAs for CAS costs in multi-socket NUMA configurations ‚Äì I wasn‚Äôt able to find the data about CAS, so I will need to speculate for the time being üôÅ . On the one hand, IMO it will be next-to-impossible to have latencies of CAS operation on ‚Äúremote‚Äù memory less than round-trip of HyperTransport between the sockets, which in turn is comparable to the cost of NUMA L3 cache read. On the other hand, I don‚Äôt really see the reasons to go higher¬†than that :-). As a result, I am guesstimating the cost of NUMA different-socket CAS (and CAS-like) operations at around 100-300 CPU clock cycles.\nTLB\nWhenever we‚Äôre working with modern CPUs and modern OS‚Äôs, at app-level we are usually dealing with ‚Äúvirtual‚Äù address space; in other words ‚Äì if we run 10 processes, each¬†of these processes can (and probably will) have its own address 0x00000000. To support¬†such isolation, CPUs implement¬†so-called ‚Äúvirtual memory‚Äù. In x86 world ‚Äì it was first implemented via ‚Äúprotected mode‚Äù introduced as early as 1982 in 80286.\nUsually, ‚Äúvirtual memory‚Äù works on per-page basis (for x86 each page is either 4K or 2M or at least in theory ‚Äì even 1G(!) in size), with CPU being aware of the current process being run (!), and re-mapping virtual addresses into physical addresses ‚Äì on each memory access, that is. Note that this re-mapping occurs completely behind the scenes, in a sense that all CPU registers (except for those specifically dealing with mapping) contain all the pointers in ‚Äúvirtual memory‚Äù format.\nAnd as soon as we said ‚Äúmapping‚Äù ‚Äì well, the information about this mapping needs to be stored somewhere. Moreover, as this mapping (from virtual addresses into physical addresses) happens¬†on each and every memory access ‚Äì¬†it needs to be Damn Fast. To help with it, a special kind of cache, known as Translation Lookaside Buffer (TLB) is normally used.\nAs for any type of cache, there is a cost of missing TLB; for x64 it is reported between 7-21 CPU cycles [7cpu]. Overall, TLBs are quite difficult to affect; however, a few recommendations can still be made in this regard:\n\nonce again ‚Äì improving overall memory locality helps to reduce TLB misses too; the more local your data is ‚Äì the less your chances are to get out of TLB.\nconsider using ‚Äúhuge pages‚Äù (those 2M pages on x64). The larger pages are ‚Äì the less entries¬†in TLB you‚Äôll need; on the other hand, using ‚Äúhuge pages‚Äù¬†comes with some caveats, and as a result ‚Äì is a two-edged sword. Which means that you need to¬†make sure to test it for your specific app.\nconsider turning off ASLR (=‚ÄùAddress Space Layout Randomization‚Äù). As discussed in [Drepper], enabling ASLR, while being good for security,¬†hits performance, and exactly because of TLB misses too üôÅ .\n\nSoftware Primitives\nNow we‚Äôre done with those things which are¬†directly hardware-related, and will be speaking about certain things which are more software-related; still, they‚Äôre really ubiquitous, so let‚Äôs see how much we spend every time we‚Äôre using them.\nC/C++ Function Calls\nFirst, let‚Äôs see the cost of C/C++ function call. Actually, C/C++ caller¬†does a damn lot of stuff before making a call ‚Äì and callee makes another few things too.\n[Efficient C++] estimates CPU costs for a function call at 25-250 CPU cycles depending on number of parameters; however, it is quite an old book, and I don‚Äôt have a better reference of the same caliber üôÅ . On the other hand, from my experience, for a function with a reasonably small number of parameters, it is more like 15-30 cycles; this also seems to apply to non-Intel CPUs as measured by¬†[eruskin].\n‚Äúkeep in mind that these days compilers tend to ignore inline specifications more often than notOptimisation hint: Use¬†inline¬†functions where applicable. However, keep in mind that these days compilers tend to ignore¬†inline¬†specifications more often than not üôÅ . Therefore, for really time-critical pieces of code you may want to consider¬†__attribute__((always_inline)) for GCC, and¬†__forceinline¬†for MSVC compilers to make them do what you need. However, do NOT overuse this forced-inline stuff for not-so-critical pieces of code, it can¬†make things worse rather easily.\nBTW, in many¬†cases gains from inlining can exceed simple removal of call costs.¬†This happens because inlining enables quite a few additional optimisations (including those related to reordering to achieve the proper use of hardware pipeline). Also let‚Äôs not forget that inlining improves spatial locality for the code ‚Äì which tends to help a bit too (see, for example, [Drepper]).\nIndirect and Virtual Calls\nDiscussion above was related to usual (‚Äúdirect‚Äù) function calls. Costs of indirect and virtual calls are known to be higher, and there is pretty much a consensus on that indirect call causes branching (however, as¬†[Agner1] notes, as long as you happen to call the same function from the same point in code, branch predictors of modern CPUs are able to predict it pretty good; otherwise ‚Äì you‚Äôll get a misprediction penalty of 10-30 cycles). As for virtual calls ‚Äì it is one extra read (reading VMT pointer), so if everything is cached at this point (which it usually is), we‚Äôre speaking about additional 4 CPU cycles or so.\nOn the other hand, practical measurements from¬†[eruskin] show that the cost of virtual functions is roughly double of the direct call costs for small functions; within our margin of error (which is ‚Äúan order of magnitude‚Äù) this is quite consistent with the analysis above.\nCuriously recurring template pattern The curiously recurring template pattern (CRTP) is an idiom in C++ in which a class X derives from a class template instantiation using X itself as template argument‚Äî Wikipedia ‚ÄîOptimisation hint:¬†IF your virtual calls are expensive ‚Äì in C++ you may want to think about using templates instead (implementing so-called compile-time polymorphism); CRTP is one (though not the only one) way of doing it.\nAllocations\nThese days, allocators as such can be quite fast; in particular, tcmalloc and ptmalloc2 allocators can take as little as 200-500 CPU cycles for allocation/deallocation of a small object¬†[TCMalloc].\nHowever, there is a significant caveat related to allocation ‚Äì and adding to indirect costs of using allocations: allocation, as a Big Fat rule of thumb, reduces memory locality, which in turn¬†adversely affects performance (due to uncached memory accesses described above). Just to illustrate how bad this can be in practice, we can take a look at a 20-line program in [NoBugs];¬†this program, when using¬†vector<>,¬†happens to be from 100x to 780x faster (depending on compiler and specific box) than an equivalent program using¬†list<>¬†‚Äì all because of poor memory locality of the latter :-(.\n‚ÄúIn some real-world cases flattening your data structures can speed up your program as much as 5x.Optimisation hint:¬†DO think about reducing number of allocations within¬†your programs ‚Äì especially if there is a stage when lots of work is done on a¬†read-only data.¬†In some real-world cases flattening your data structures (i.e. replacing allocated objects with packed ones) can speed up your program¬†as much as 5x. A real-world story in this regard. Once upon a time, there was a program which used some gigabytes of RAM, which was deemed too much; ok, I rewrote it to a ‚Äúflattened‚Äù version (i.e. each node was first constructed dynamically, and then an equivalent ‚Äúflattened‚Äù read-only object was created in memory); the idea of ‚Äúflattening‚Äù was to reduce memory footprint. When we ran the program, we observed that not only memory footprint was reduced by 2x (which was what we expected), but that also ‚Äì as a very nice side effect ‚Äì execution speed went up by 5x.\nKernel Calls\nIf our program runs under an operating system,1 then we have a whole bunch of system APIs. In practice,2 quite a few of those system calls cause kernel calls, which involve switches to kernel mode and back; this includes switching between different ‚Äúprotection rings‚Äù (on Intel CPU ‚Äì usually between ‚Äúring 3‚Äù and ‚Äúring 0‚Äù). While this¬†CPU-level switching back and forth itself takes only ~100 CPU cycles, other related overheads tend to make kernel calls much more expensive, so usual kernel call takes at least 1000-1500 CPU cycles üôÅ [Wikipedia.ProtectionRing].\n1 yes, there are still programs which run without it2 at least if we‚Äôre speaking about more or less conventional OS\n\nC++ Exceptions\nThese days, C++ exceptions are said to be zero-cost until thrown. Whether it is really zero ‚Äì is still not 100% clear (IMO it is even unclear whether such a question can be asked at all), but it is certainly very close.\n‚Äúthese 'zero-cost until thrown' exception implementations come at the cost of a huge pile of work which needs to be done whenever an exception is thrownHowever, these ‚Äúzero-cost until thrown‚Äù implementations come at the cost of a huge pile of work which needs to be done whenever an exception¬†is¬†thrown. Everybody agrees that the cost of exception thrown is huge, however (as usual) experimental data is scarce. Still, an experiment by [Ongaro] gives us a ballpark number of¬†around 5000 CPU¬†cycles (sic!).¬†Moreover, in more complicated cases, I would expect it to take even more.\nReturn Error and Check\nAn ages-old alternative to exceptions is returning error codes and checking them at each level. While I don‚Äôt have references for performance measurements of this kind of things, we already know enough to make a reasonable guesstimate. Let‚Äôs take a closer look at it (we don‚Äôt care much about performance in the case when error arises, so will concentrate on costs when everything is fine).\nBasically, cost of return-and-check consists of three separate costs. The first one is the cost of conditional jump itself ‚Äì and we can safely assume that 99+% of the time it will be predicted correctly; which means the cost of conditional jump in this case is around 1-2 cycles. The second cost is the cost of copying the error code around ‚Äì and as long as¬†it stays within the registers, it is a simple MOV ‚Äì which is, given the circumstances, is 0 to 1 cycles (0 cycles means that MOV¬†has no additional cost, as it is performed in parallel with some other stuff). The third cost is much less obvious ‚Äì it is a cost of the extra register necessary to carry the error code; if we‚Äôre out of registers ‚Äì we‚Äôll need PUSH/POP pair (or a reasonable facsimile), which is in turn a write + L1 read, or 1+4 cycles. On the other hand, let‚Äôs keep in mind that¬†¬†chances of PUSH/POP being necessary, vary from one platform to another one; for example, on x86 any realistic function would require them almost for sure; however, on x64 (which has double number of registers) ‚Äì chances of PUSH/POP being necessary, go down significantly (and in quite a few¬†cases, even if register is not completely free, making it available may be done by compiler cheaper than a dumb PUSH/POP).\nAdding all three costs together, I‚Äôd guesstimate costs of return-error-code-and-check (in normal case) at anywhere between 1 and 7 CPU cycles. Which in turn means that if we have one exception per 10000 function calls ‚Äì we‚Äôre likely to be better with exceptions; however, if we have one exception per 100 function calls ‚Äì we‚Äôre likely to be better with error codes. In other words, we‚Äôve just reconfirmed a very well-known best practice ‚Äì ‚Äúuse exceptions only for abnormal situations‚Äù üôÇ .\nThread Context Switches\nLast but certainly not least, we need to speak about costs of thread context switches. One¬†problem with estimating them is that, well, it is very difficult to figure them out. Common wisdom tells that they‚Äôre ‚Äúdamn expensive‚Äù (hey, there should be a reason why nginx outperforms Apache) ‚Äì but how much this ‚Äúdamn expensive‚Äù is?\nFrom my personal observations, the costs were at least 10000 CPU cycles; however, there are lots of sources which are giving MUCH lower numbers. In fact, however, it is all about ‚Äúwhat exactly we‚Äôre trying to measure‚Äù. As noted in [LiEtAl], there are two different costs with relation to context switches.\n\nThe first cost is direct costs of thread context switching ‚Äì and these are measured at about 2000 CPU cycles3\nHowever, the second cost is MUCH higher; it is related to cache invalidation by the thread; according to¬†[LiEtAl], it can be as large as 3M CPU clocks. In theory, with completely random access pattern, modern CPU with 12M of L3 cache (and taking penalty of the order of 50 cycles per access) can take¬†a penalty of up to 10M cycles per context switch; still, in practice the penalties are usually somewhat lower than that, so the number of 1M from¬†[LiEtAl] makes sense. This ‚Äúmuch higher‚Äù estimate is also consistent with the number of spinlocks on x64 (which defaults to 4000 at least for Windows/x64): if it is usually beneficial to wait for 4000 iterations (amounting¬†at the very least¬†to 15-20K CPU cycles, and more like 40-50K cycles from what I‚Äôve seen) reading that variable-which-is-currently-locked within a busy loop ‚Äì just¬†in hope¬†that the variable will unlock before 4000 iterations is over, all of this trouble and CPU cycles merely to avoid a context switch ‚Äì it means that the cost of the context switch is usually¬†much higher¬†than those tens-of-thousands-of-CPU-cycles-we‚Äôre-ready-to-spend-in-a-busy-loop-doing-nothing-useful.\n\n3 that is, if my math is correct when converting from microseconds¬†into cycles\n\nWrapping it Up\nPhew, it was quite a bit of work to find¬†references for all these more-or-less known observations.\nAlso please note that while I‚Äôve honestly tried to collect all the related costs¬†in one place (checking 3rd-party findings against my own experiences in the process), it is just a very first attempt at this, so if you find reasonably compelling evidence that certain item is wrong ‚Äì please let me know, I will be happy¬†to make the diagram more accurate.\n\nDon't like this post?¬†. You do?! Please share: [+]ReferencesAcknowledgementCartoons by Sergey Gordeev from Gordeev Animation Graphics, Prague.\n\n","length":28682,"excerpt":"Quote:\"Back in 80s, it was possible to calculate the speed of the program just by looking at assembly.\"Another Quote:\"keep in mind that these days compilers tend to ignore inline specifications more often than not\"[‚Üí]","siteName":"IT Hare on Soft.ware","publishedTime":"2016-09-12T12:10:11+00:00","url":"http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/"}