{"title":"MCCC: Quasi-Monte Carlo","byline":"Max Slater","dir":"ltr","lang":"en-us","content":"<div id=\"readability-page-1\" class=\"page\"><div><main><h2>Monte Carlo Crash Course</h2><ul><li><a href=\"https://thenumb.at/Probability\">Continuous Probability</a></li><li><a href=\"https://thenumb.at/Monte-Carlo\">Exponentially Better Integration</a></li><li><a href=\"https://thenumb.at/Sampling\">Sampling</a></li><li><a href=\"https://thenumb.at/Rendering\">Case Study: Rendering</a></li><li><strong><a href=\"https://thenumb.at/QMC\">Quasi-Monte Carlo</a></strong></li><li><em>Coming Soon…</em></li></ul><hr><p>We’ve learned how to define and apply Monte Carlo integration—fundamentally, it’s the only tool we need.\nIn&nbsp;the remaining chapters, we’ll explore ways to reduce variance and successfully sample difficult distributions.</p><ul><li><a href=\"#variance--correlation\">Variance &amp; Correlation</a></li><li><a href=\"#stratified-sampling\">Stratified Sampling</a></li><li><a href=\"#adaptive-sampling\">Adaptive Sampling</a></li><li><a href=\"#latin-hypercube\">Latin Hypercube</a></li><li><a href=\"#quasi-monte-carlo-1\">Quasi-Monte Carlo</a></li><li><a href=\"#low-discrepancy-sequences\">Low-Discrepancy Sequences</a></li></ul><h2 id=\"variance--correlation\">Variance &amp; Correlation</h2><p>In <a href=\"https://thenumb.at/Monte-Carlo/#escaping-the-curse\">chapter two</a>, we determined that the variance of a Monte Carlo estimator is inversely proportional to its sample count.\nEmpirically, we confirmed that our integrators’ expected error scaled with $$\\frac{1}{\\sqrt{N}}$$ in any dimension.</p><p>Although dramatically faster than <em>exponential</em>, if we want a very accurate result, $$\\frac{1}{\\sqrt{N}}$$ may still be too slow.\nIn&nbsp;practice, we can only scale sample count quadratically so many times.</p><p><img src=\"https://thenumb.at/QMC/error.svg\"></p><p>We also assumed that our samples are independent, so their variance is additive.\nHowever, our proof that Monte Carlo integration is unbiased didn’t rely on independence—so what if we relaxed that assumption?</p><p>\\[\\begin{align*}\n\\mathrm{Var}[X + Y] = \\mathrm{Var}[X] + \\mathrm{Var}[Y] + 2\\mathrm{Cov}[X,Y]\n\\end{align*}\\]</p><p>If $$X$$ and $$Y$$ are <em>negatively</em> correlated, $$\\mathrm{Cov}[X,Y] &lt; 0$$, decreasing the variance of $$X+Y$$.\nIf we can assure that our samples are negatively correlated, our Monte Carlo estimator might converge faster than $$\\frac{1}{\\sqrt{N}}$$.</p><h3 id=\"poisson-disk-sampling\">Poisson Disk Sampling</h3><p>Perceptually, negatively correlated samples look “more random” than uncorrelated samples.</p><div><p><img src=\"https://thenumb.at/QMC/uncorrelated.svg\"></p><p><img src=\"https://thenumb.at/QMC/ncorrelated.svg\"></p></div><p>That’s because uncorrelated samples often appear in clusters and may leave significant chunks of the domain entirely unsampled.\nNegative correlation causes the opposite behavior: the more samples an area contains, the less likely it is to be sampled, and vice versa.</p><p>So, how can we generate negatively correlated samples?\nOne approach is rejection sampling: simply discard samples that fall too close to any previous sample.\nThis algorithm is known as <em>Poisson disk sampling</em>.<sup id=\"fnref:1\"><a href=\"#fn:1\" role=\"doc-noteref\">1</a></sup></p><p>Poisson disk sampling is useful for pre-generating samples with a minimum separation distance, but isn’t always applicable to Monte Carlo integration, where we need a progressive sampler that eventually covers the entire domain.</p><h2 id=\"stratified-sampling\">Stratified Sampling</h2><p>If we don’t need a minimum separation distance, a faster way to generate negatively correlated samples is <em>stratified sampling</em>.\nStratification will let us combine the strengths of <a href=\"https://thenumb.at/Monte-Carlo/#bias-and-consistency\">quadrature</a> and Monte Carlo integration.</p><p>Instead of generating $$N$$ independent samples of the entire domain, a stratified sampler partitions the domain into $$M$$ equal-sized regions and takes $$\\frac{N}{M}$$ independent samples of each one.\nSince no more than $$\\frac{N}{M}$$ samples can occur in any particular region, the samples are negatively correlated.<sup id=\"fnref:2\"><a href=\"#fn:2\" role=\"doc-noteref\">2</a></sup></p><p>Let’s consider a Monte Carlo estimator that uses $$N$$ stratified samples of $$\\Omega$$.\nGrouping samples by region lets us rearrange the estimator into a collection of $$\\frac{N}{M}$$-sample estimators for each region $$\\Omega_m$$:</p><p>\\[\\begin{align*}\nF_\\text{Stratified} &amp;= \\sum_{n=0}^N \\frac{f(\\Omega_n)}{p(\\Omega_n)}\\\\\n&amp;= \\sum_{m=0}^M \\sum_{n=0}^\\frac{N}{M} \\frac{f(\\Omega_{m,n})}{p(\\Omega_{m,n})}\\\\\n&amp;= \\sum_{m=0}^M F_{\\Omega_m}\n\\end{align*}\\]</p><p>Intuitively, stratification partitions our integral across the regions $$\\Omega_m$$ and computes an independent $$\\frac{N}{M}$$-sample estimate of each term.\nHence, linearity of expectation implies that our stratified estimator is unbiased.</p><p>\\[\\begin{align*}\n\\mathbb{E}[F_\\text{Stratified}]\n&amp;= \\mathbb{E}[F_{\\Omega_0} + F_{\\Omega_1} + \\dots]\\\\\n&amp;= \\mathbb{E}[F_{\\Omega_0}] + \\mathbb{E}[F_{\\Omega_1}] + \\dots\\\\\n&amp;= \\int_{\\Omega_0} f(\\omega)\\, d\\omega + \\int_{\\Omega_1} f(\\omega)\\, d\\omega + \\dots\\\\\n&amp;= \\int_\\Omega f(\\omega)\\, d\\omega\n\\end{align*}\\]</p><p>But did stratification reduce variance?\nLet’s try dividing our familiar circular estimator into $$M=64$$ regions:</p><p>We’ll find that the stratified estimator has fairly low error, especially when $$N$$ is small.<sup id=\"fnref:3\"><a href=\"#fn:3\" role=\"doc-noteref\">3</a></sup></p><p>Precisely how much stratification decreases variance depends on the behavior of $$f$$, but we may prove that stratification at least never <em>increases</em> variance.</p><h3 id=\"why-stratify\">Why Stratify?</h3><p>Let’s compare an $$N$$-sample uniform estimator on $$\\Omega$$ against a stratified estimator that uniformly samples partitions $$A$$ and $$B$$.\nRecalling <a href=\"https://thenumb.at/Monte-Carlo/#escaping-the-curse\">chapter two</a>, we can compute these estimators’ variances:</p><p>\\[\\begin{align*}\n\\mathrm{Var}[F_\\text{Uniform}] &amp;= \\vphantom{\\Bigg|}\\frac{|\\Omega|^2}{N}\\mathrm{Var}[f(\\Omega)]\\\\\n\\mathrm{Var}[F_\\text{Stratified}] &amp;= \\mathrm{Var}[F_A] + \\mathrm{Var}[F_B] \\tag{$F_A,F_B$ indep.}\\\\\n&amp;= \\frac{|\\Omega|^2}{2N}\\left(\\vphantom{\\big|}\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]\\right)\n\\end{align*}\\]</p><p>\\[\\begin{align*}\n\\mathrm{Var}[F_\\text{Uniform}] &amp;= \\vphantom{\\Bigg|}\\frac{|\\Omega|^2}{N}\\mathrm{Var}[f(\\Omega)]\\\\\n\\mathrm{Var}[F_\\text{Stratified}] &amp;= \\mathrm{Var}[F_A] + \\mathrm{Var}[F_B] \\\\&amp;\\tag{$F_A,F_B$ indep.}\\\\\n&amp;= \\frac{|\\Omega|^2}{2N}\\left(\\vphantom{\\big|}\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]\\right)\n\\end{align*}\\]</p><p>To relate these quantities, we may condition $$\\mathrm{Var}[f(\\Omega)]$$ on the sampled region.\nWe will write $$\\mu_\\mathcal{X}$$ to denote the expected value $$\\mathbb{E}[f(\\mathcal{X})]$$.</p><p>\\[\\begin{align*}\n\\mathrm{Var}[f(\\Omega)] &amp;= \\mathrm{Var}[f(\\Omega)\\ |\\ A]\\cdot\\mathbb{P}\\{A\\} + \\mathrm{Var}[f(\\Omega)\\ |\\ B]\\cdot\\mathbb{P}\\{B\\}\\\\\n&amp;= \\frac{1}{2}\\left(\\mathbb{E}[(f(\\Omega)-\\mu_\\Omega)^2\\ |\\ A] + \\mathbb{E}[(f(\\Omega)-\\mu_\\Omega)^2\\ |\\ B]\\right)\\\\\n&amp;= \\frac{1}{2}\\left(\\mathbb{E}[f(A)]^2+\\mathbb{E}[f(B)]^2-2\\left(\\frac{\\mu_A+\\mu_B}{2}\\right)^2\\right)\\\\\n&amp;= \\frac{1}{2}\\left(\\mathbb{E}[f(A)]^2-\\mu_A^2+\\mathbb{E}[f(B)]^2-\\mu_B^2+\\frac{1}{2}\\left(\\mu_A^2-2\\mu_A\\mu_B+\\mu_B^2\\right)\\right)\\\\\n&amp;= \\frac{\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]}{2}+\\frac{(\\mu_A-\\mu_B)^2}{4}\\\\\n&amp;\\ge \\frac{\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]}{2}\\\\\n\\end{align*}\\]</p><p>The squared term is never negative, so we know that $$\\mathrm{Var}[f(A)] + \\mathrm{Var}[f(B)]$$ is at most $$2\\cdot\\mathrm{Var}[f(\\Omega)]$$.\nHence, $$F_\\text{Stratified}$$ cannot have higher variance than $$F_\\text{Uniform}$$, and has lower variance when $$\\mu_A \\neq \\mu_B$$.</p><p><img src=\"https://thenumb.at/QMC/avg.svg\"></p><p>This result makes some intuitive sense—if $$f$$ has a different average on $$A$$ and $$B$$, samples constrained to $$A$$ or $$B$$ must have locally lower variance than $$f$$ as a whole.</p><h3 id=\"dynamic-stratification\">Dynamic Stratification</h3><p>So, should we be using stratified sampling everywhere?\nOften, yes—partitioning the domain can only reduce variance—but note that stratifying across a fixed $$64$$ regions did not reduce variance <em>asymptotically</em>.</p><p>Even when $$f$$ is <a href=\"https://en.wikipedia.org/wiki/Bounded_variation\">sufficiently nice</a>, stratification only reduces error in inverse proportion to the regions’ volume.\nIn&nbsp;a $$d$$-dimensional domain, we should expect our estimator to converge with the following expression:<sup id=\"fnref:4\"><a href=\"#fn:4\" role=\"doc-noteref\">4</a></sup></p><div><p><img src=\"https://thenumb.at/QMC/curse.svg\"></p><p>\\[\\begin{align*}\n\\sigma_\\text{Stratified} &amp;\\propto \\frac{1}{\\sqrt{N}\\sqrt[d]{M}} \\\\ &amp;= N^{-\\frac{1}{2}}M^{-\\frac{1}{d}}\n\\end{align*}\\]</p></div><p>That is, $$M$$ is subject to the <a href=\"https://thenumb.at/Monte-Carlo/#the-curse-of-dimensionality\">curse of dimensionality</a>.\nPlus, we can’t use more regions than our sample count, so $$M \\le N$$.\nNonetheless, we can find an algorithmic improvement by dynamically scaling $$M \\propto \\sqrt{N}$$:<sup id=\"fnref:5\"><a href=\"#fn:5\" role=\"doc-noteref\">5</a></sup></p><p>\\[\\begin{align*}\n\\sigma_\\text{Stratified} &amp;\\propto N^{-\\frac{1}{2}}{\\sqrt{N}}^{-\\frac{1}{d}} \\\\ &amp;= N^{-\\frac{d+1}{2d}}\n\\end{align*}\\]</p><p>In one dimension ($$d = 1$$), dynamic stratification produces $$\\sigma \\propto N^{-1}$$.\nBack in <a href=\"https://thenumb.at/Monte-Carlo/#quadrature\">chapter two</a>, we used quadrature to integrate $$\\sqrt{x}$$ with error proportional to $$N^{-1}$$.\nUsing dynamic stratification, we get an <em>unbiased</em> estimator with the same convergence rate!</p><p>In two dimensions, dynamic stratification results in $$\\sigma \\propto N^{-\\frac{3}{4}}$$, which converges faster than naive Monte Carlo, but in higher dimensions, we rapidly approach our existing result of $$\\sigma \\propto N^{-\\frac{1}{2}}$$.\nHence, dynamic stratification is usually only worthwhile in a small number of dimensions.</p><h2 id=\"adaptive-sampling\">Adaptive Sampling</h2><p>Another extension of stratified sampling is <em>adaptive sampling</em>.\nInstead of assigning the same number of samples to each region, adaptive sampling uses more samples in regions with higher variance.</p><p>Above, we determined that the variance of a stratified estimator is a weighted sum, where $$N_A+N_B=N$$:</p><p>\\[\\begin{align*}\n\\mathrm{Var}[F_\\text{Stratified}] &amp;= \\mathrm{Var}[F_A] + \\mathrm{Var}[F_B]\\\\\n&amp;\\propto \\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B}\n\\end{align*}\\]</p><p>We also assumed $$N_A = N_B$$, but that wasn’t required to show that our stratified estimator was unbiased.\nSo,&nbsp;if $$\\sigma_A^2&gt;\\sigma_B^2$$, using $$N_A&gt;N_B$$ would decrease the total.\nTo find the optimal sample distribution, we may minimize this sum using a <a href=\"https://en.wikipedia.org/wiki/Lagrange_multiplier\">Lagrange multiplier</a>:</p><p>\\[\\begin{align*}\n&amp;&amp;&amp; \\min_{N_A+N_B=N} \\left\\{\\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B}\\right\\}\\\\\n&amp;\\implies&amp;&amp; \\begin{cases}\n\\frac{\\partial}{\\partial N_A}\\left(\\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B} - \\lambda(N_A+N_B-N)\\right) = 0 \\\\\n\\frac{\\partial}{\\partial N_B}\\left(\\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B} - \\lambda(N_A+N_B-N)\\right) = 0\n\\end{cases}\\\\\n&amp;\\implies&amp;&amp;\nN_A = N\\cdot\\frac{\\sigma_A}{\\sigma_A+\\sigma_B} \\\\\n&amp;&amp;&amp;\nN_B = N\\cdot\\frac{\\sigma_B}{\\sigma_A+\\sigma_B} \\tag{Algebra}\n\\end{align*}\\]</p><p>As you might expect, we should partition samples between $$F_A$$ and $$F_B$$ in proportion to their standard deviation.\nLet’s attempt to implement this improvement in our circular estimator.\nIntuitively, we should be able to ignore regions where $$f$$ is constant (i.e. $$F$$ has zero deviation):</p><p><img src=\"https://thenumb.at/QMC/adapt.svg\"></p><p>However, we can’t assume to know the standard deviation of each region beforehand, so we must estimate it during integration.\nTo guide adaptive sampling, each region $$\\Omega_m$$ will track its sample count $$N_m$$, as well as estimates of $$\\mathbb{E}[f(\\Omega_m)]$$ and $$\\mathbb{E}[f(\\Omega_m)^2]$$.<sup id=\"fnref:6\"><a href=\"#fn:6\" role=\"doc-noteref\">6</a></sup></p><p>\\[\\begin{align*}\n\\sigma_{\\Omega_m} &amp;= \\sqrt{\\mathrm{Var}[F_{\\Omega_m}]} \\\\\n&amp;= \\sqrt{\\frac{\\mathbb{E}[f(\\Omega_m)^2] - \\mathbb{E}[f(\\Omega_m)]^2}{N_m}}\n\\end{align*}\\]</p><p>Unfortunately, this estimate can be very imprecise, so using it for adaptive sampling isn’t always easy.\nOne&nbsp;approach is to randomly select the next region to sample weighted by estimated deviation (drawn in red):</p><p>This strategy works, but even when a region has zero estimated deviation, we must choose it with non-zero probability, since the true value may not be zero.\nWe can see this situation play out when our estimator discovers that a mostly-covered region actually has non-zero variance.</p><p>The core ideas of adaptive sampling and dynamic stratification—assigning samples where they’re needed and progressively refining the sampling pattern—can be combined to form the <a href=\"https://en.wikipedia.org/wiki/Multilevel_Monte_Carlo_method\">multi-level Monte Carlo</a> method, which we won’t explore in this chapter.</p><h2 id=\"latin-hypercube\">Latin Hypercube</h2><p>Any way we slice it, stratified sampling will eventually run up against the curse of dimensionality.\nIn practice, we may need computationally cheaper methods of generating negatively correlated samples.\nOne such method is known as <em>Latin hypercube</em> sampling.<sup id=\"fnref:7\"><a href=\"#fn:7\" role=\"doc-noteref\">7</a></sup></p><p>Instead of attempting to distribute samples across an exponential number of regions, a Latin hypercube sampler stratifies each dimension <em>independently</em>.\nFor example, in two dimensions, we start by generating two lists of one-dimensional samples stratified across $$D$$ regions:</p><img src=\"https://thenumb.at/QMC/indep.svg\"><p>We then shuffle $$\\mathbf{x}$$ and $$\\mathbf{y}$$, randomizing their order.\nFinally, to produce samples of $$\\Omega$$, we simply take each pair $$(\\mathbf{x}_i,\\mathbf{y}_i)$$ from the shuffled lists.<sup id=\"fnref:8\"><a href=\"#fn:8\" role=\"doc-noteref\">8</a></sup>\nThis procedure is typically performed in batches of $$D$$ samples.<sup id=\"fnref:9\"><a href=\"#fn:9\" role=\"doc-noteref\">9</a></sup></p><p>Since we’re implicitly partitioning $$\\Omega$$ into $$D^d$$ regions, we can think of a Latin hypercube sampler as a sparse approximation of a stratified sampler over this much larger space.\nHowever, perfect stratification would require placing at most $$\\frac{N}{D^d}$$ samples in each region, which our sampler does not achieve.</p><p>Given any particular region, we know exactly $$\\frac{N}{D}$$ samples have a matching position along each dimension, so up to $$\\frac{N}{D}$$ samples could occur in this region.\nThis bound is exponentially weaker than full stratification, but still leads to negative correlation.</p><p>Note that regardless of correlation, Latin hypercube samples are uniform, and by similar logic as stratified sampling, may be used for Monte Carlo integration.</p><p>In our circular estimator, we’ll find that Latin hypercube samples tend to reduce error at small $$N$$, but they’re clearly less effective than stratified samples.\nLatin hypercube samples become more useful when full stratification is infeasible, e.g. in four or more dimensions.</p><p>The Latin hypercube approach can also be combined with stratified sampling to create multi-level samplers, such as <a href=\"https://graphics.pixar.com/library/MultiJitteredSampling/paper.pdf\">correlated multi-jittered sampling</a>.</p><h2 id=\"quasi-monte-carlo-1\">Quasi-Monte Carlo</h2><p>All of the sampling algorithms we’ve examined so far are fundamentally random, but with additional restrictions that introduce negative correlation.\nAlternatively, if we’re willing to introduce <a href=\"https://thenumb.at/Monte-Carlo/#bias-and-consistency\">bias</a>, we can do away with randomness entirely!</p><p>In <a href=\"https://thenumb.at/Sampling/#pseudo-random-numbers\">chapter three</a>, we discussed pseudo-random number generators (PRNGs), which deterministically compute a sequence of samples that “look uniformly random.”\nOur only source of non-determinism was the seed: given the same initial state, a PRNG always produces the same sequence.\nHence, for a <strong>fixed seed</strong>, a PRNG is just a particular sequence of numbers $$x_i$$—but we can still use it for Monte Carlo integration.</p><p>\\[\nF_\\text{QMC} = \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n\\]</p><p>The result is known as a <em>Quasi-Monte Carlo</em> estimator.<sup id=\"fnref:10\"><a href=\"#fn:10\" role=\"doc-noteref\">10</a></sup>\nQMC estimators are <a href=\"https://thenumb.at/Monte-Carlo/#bias-and-consistency\">biased</a>: they always return the same value, so averaging multiple runs does not increase accuracy.\nFortunately, they are also <a href=\"https://thenumb.at/Monte-Carlo/#bias-and-consistency\">consistent</a>: increasing the sample count $$N$$ causes the estimator to converge to the exact result.<sup id=\"fnref:11\"><a href=\"#fn:11\" role=\"doc-noteref\">11</a></sup></p><p><img src=\"https://thenumb.at/QMC/consistent.svg\"></p><h3 id=\"discrepancy\">Discrepancy</h3><p>Empirically, we’ve already confirmed that QMC estimators are consistent—most of the interactive figures use a PRNG with a fixed seed.\nBut what is it about pseudo-random sequences that make QMC work?\nLacking the tools of probability, how can we relate the accuracy of an estimator to the quantity and quality of its samples?</p><p>This is the purpose of the <em>Koksma–Hlawka inequality</em>, which we will present without proof:</p><p>\\[\n\\Bigg|\\, \\frac{1}{N}\\sum_{i=1}^N f(x_i) - \\int_\\Omega f(\\omega)\\, d\\omega\\, \\Bigg| \\le V(f) D^*_N(x_1,\\dots,x_N)\n\\]</p><p>Here, $$x_i$$ is our sample sequence, $$f$$ has <a href=\"https://en.wikipedia.org/wiki/Bounded_variation\">bounded variation</a> $$V(f)$$, and $$D^*_N$$ is the “star-discrepancy” of $$x$$.\nBounded variation is morally the same constraint that our Monte Carlo estimator for $$f$$ had finite variance.</p><p>Importantly, our estimator’s error is (at worst) proportional to this property $$D^*_N$$, which only depends on $$x$$.</p><p>\\[\n\\bigg|\\, F_\\text{QMC} - \\int_\\Omega f(\\omega)\\, d\\omega\\, \\bigg| \\propto D^*_N(x_1,\\dots,x_N)\n\\]</p><p>Intuitively, we can think of star-discrepancy as a deterministic equivalent of negative correlation.\nTaking $$\\Omega$$ to be the unit square, $$D^*_N$$ is defined as the worst-case difference between the ratio of samples falling inside a rectangle $$\\mathcal{R}$$ and the volume of $$\\mathcal{R}$$.\n$$\\mathcal{R}$$ must also include the origin as its bottom-left corner.<sup id=\"fnref:12\"><a href=\"#fn:12\" role=\"doc-noteref\">12</a></sup></p><p>\\[\nD^*_N(x_1,\\dots,x_N) = \\sup_{\\mathcal{R}\\in\\Omega} \\Bigg|\\, \\frac{\\text{Samples}(\\mathcal{R})}{N} - ||\\mathcal{R}|| \\,\\Bigg|\n\\]</p><p>We can see that discrepancy penalizes regions that have more (or fewer) samples than they should:</p><p><img src=\"https://thenumb.at/QMC/discrepancy.svg\"></p><p>Uniformly random point sets—as well as those produced by PRNGs—have the property that $$D^*_N$$ tends to zero with increasing sample count.\nHence, using such sequences in a QMC estimator will cause it to converge for all $$f$$ with bounded variation.</p><h2 id=\"low-discrepancy-sequences\">Low-Discrepancy Sequences</h2><p>To justify using a biased estimator, it should exhibit a faster convergence rate than unbiased alternatives.\nBut&nbsp;simply fixing a PRNG seed surely doesn’t accelerate convergence, right?\nIndeed, in $$d$$ dimensions, the star-discrepancy of a uniformly random point set only converges with the following expression:</p><div><p>\\[\nD^*_N(\\text{Uniform}) \\propto \\sqrt{\\frac{\\log^dN}{N}}\n\\]</p></div><p>The Koksma–Hlawka inequality therefore doesn’t tell us anything—we already knew uniform Monte Carlo converges with $$\\frac{1}{\\sqrt{N}}$$.\nHowever, there’s no reason we have to use uniformly random points. Instead, we can make use of certain <em>low-discrepancy sequences</em>, which provide near-linear convergence.</p><p>\\[\nD^*_N(\\text{Low-Discrepancy}) \\propto \\frac{\\log^dN}{N}\n\\]</p><p>While this convergence rate is always asymptotically faster than $$\\frac{1}{\\sqrt{N}}$$, in high dimensions, the associated constant factor (which depends on $$d$$) can still make the approach impractical.</p><h3 id=\"the-halton-sequence\">The Halton Sequence</h3><p>There are many low-discrepancy sequences, each making use of different mathematical tools.\nOne&nbsp;popular choice is the <a href=\"https://en.wikipedia.org/wiki/Halton_sequence\">Halton sequence</a>.\nTo compute a one-dimensional Halton sequence $$g_b(n)$$, first choose a base $$b$$ and find the base-$$b$$ digits of $$n$$:</p><p>\\[\nn = \\sum_k \\text{Digit}_k(n)b^k\n\\]</p><p>Then mirror the digits about the decimal place.\nIn base ten, we would have $$g_{10}(1234) = 0.4321$$.</p><p>\\[\ng_b(n) = \\sum_k \\text{Digit}_k(n)b^{-k-1}\n\\]</p><p>This operation is also known as the <em>radical inverse</em> $$\\Psi_b(n)$$.\nTo create a $$d$$-dimensional Halton sequence, simply join several one-dimensional sequences with co-prime bases $$b_1\\dots b_d$$.</p><p>\\[\ng(n) = (g_{b_1}(n),\\dots,g_{b_d}(n))\n\\]</p><p>Halton sequences exhibit star-discrepancy proportional to $$\\frac{\\log^d N}{N}$$, but proving this result is beyond the scope of this chapter.\nInstead, let’s examine the two-dimensional $$(2,3)$$ Halton sequence:</p><p>Naturally, we can use Halton samples in our circular estimator:</p><p>We’ll find that convergence is effectively linear, significantly outpacing our earlier estimators.<sup id=\"fnref:13\"><a href=\"#fn:13\" role=\"doc-noteref\">13</a></sup></p><p>Does this result imply we should abandon randomness and use low-discrepancy sequences for everything?\nIn&nbsp;few dimensions, QMC can be ideal—but working with high-dimensional low-discrepancy sequences turns out to be difficult.</p><h3 id=\"scrambling\">Scrambling</h3><p>By definition, a $$d$$-dimensional Halton sequence uses $$d$$ different co-prime bases.\nTo illustrate the difficulties encountered in higher dimensions, let’s consider only the points generated by bases 29 and 31.</p><p>\\[\ng(n) = (\\dots,g_{29}(n),g_{31}(n),\\dots)\n\\]</p><p>Note $$g_{29}(n),g_{31}(n)$$ is also the projection of $$g(n)$$ onto the corresponding axes.\nAlthough this sequence is still “low-discrepancy,” its absolute discrepancy starts out quite high:</p><p>Using sequences like $$(29,31)$$ in QMC estimators can be fraught, since reducing bias to an acceptable level requires many samples.\nThis problem gets worse as we increase dimensionality.</p><p>Fortunately, there’s another technique (known as <em>scrambling</em>) that makes higher-dimensional bases significantly more usable.\nScrambling introduces an extra step to the radical inverse:</p><p>\\[\ng_b(n) = \\sum_k \\rho(\\text{Digit}_k(n))b^{-k-1}\n\\]</p><p>Where $$\\rho$$ is a permutation of the digits $$0\\dots b$$, typically chosen randomly.\nScrambling dramatically reduces discrepancy at low sample counts, making the resulting bias less objectionable.</p><p>The Halton sequence succinctly exemplifies the benefits and pitfalls of QMC, but it’s certainly not the only useful low-discrepancy sequence.\nIn&nbsp;particular, <a href=\"https://pbr-book.org/3ed-2018/Sampling_and_Reconstruction/(0,_2)-Sequence_Sampler\">Sobol’ sequences</a> are widely used in practice, as they can achieve a kind of optimal discrepancy while admitting an efficient implementation.</p><hr><h2 id=\"footnotes\">Footnotes</h2><div role=\"doc-endnotes\"><hr><ol><li id=\"fn:1\"><p>Note checking for sample intersection can be done in constant time using a background grid. In fact, generating $$N$$ samples can be done in (more or less) <a href=\"https://www.cs.ubc.ca/~rbridson/docs/bridson-siggraph07-poissondisk.pdf\">linear work</a>.&nbsp;<a href=\"#fnref:1\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:2\"><p>Technically, sample $$i$$ is positively correlated with sample $$i+M$$, but it’s negatively correlated with all of the samples in between, so the overall effect is negative.&nbsp;<a href=\"#fnref:2\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:3\"><p>But not when $$N$$ is <em>too</em> small: using fewer than $$M$$ samples would result in a biased estimate.&nbsp;<a href=\"#fnref:3\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:4\"><p>Precisely explaining this point is beyond the scope of this chapter, but intuitively, if $$f$$ has bounded variation (and is not a constant), there’s some resolution at which $$f$$ must have a different average in different regions, and this resolution may be required along every dimension.&nbsp;<a href=\"#fnref:4\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:5\"><p>To see why $$\\sqrt{N}$$ is a good choice, compute the convergence rate for $$M = \\log N$$ and $$M = N$$.&nbsp;<a href=\"#fnref:5\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:6\"><p>Also known as the first and second moments of $$f(\\Omega_m)$$. Interestingly, in stochastic optimization problems (such as <a href=\"https://rgl.epfl.ch/publications\">inverse rendering</a>), the <a href=\"https://arxiv.org/pdf/1412.6980\">Adam</a> optimizer naturally estimates these moments, which can be used for adaptive sampling.&nbsp;<a href=\"#fnref:6\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:7\"><p>In two dimensions, the Latin hypercube pattern is also known as <em>N-rooks</em>: if each batch of samples were rooks on a chessboard, they wouldn’t threaten each other.&nbsp;<a href=\"#fnref:7\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:8\"><p>Joining one sample from each dimension can also be interpreted as picking a vertex of a dimension-$$d$$ length-$$N$$ lattice, leading to the “hypercube” name. The “Latin” part comes from <a href=\"https://en.wikipedia.org/wiki/Latin_square\">Latin squares</a>.&nbsp;<a href=\"#fnref:8\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:9\"><p>Allowing batch sizes linear in $$D$$ is another benefit of Latin hypercube sampling: for full stratification to be unbiased, we must use at least $$D^d$$ samples, which may be infeasible in of itself.&nbsp;<a href=\"#fnref:9\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:10\"><p>Technically, QMC also implies the use of a low-discrepancy sequence, since (outside of pedagogy) it’s not particularly useful to consider PRNG-based estimators part of the QMC framework.&nbsp;<a href=\"#fnref:10\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:11\"><p>This distinction may sound academic, but it also has computational implications. To increase accuracy, we must use additional samples <em>from the same sequence</em>, so we can’t easily parallelize sample generation.&nbsp;<a href=\"#fnref:11\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:12\"><p>In other words, $$D^*_N$$ measures how well $$x_i$$ integrates all possible rectangular indicator functions.&nbsp;<a href=\"#fnref:12\" role=\"doc-backlink\">↩︎</a></p></li><li id=\"fn:13\"><p>These error plots should be log-log; they’re difficult to interpret otherwise. Maybe they will be someday.&nbsp;<a href=\"#fnref:13\" role=\"doc-backlink\">↩︎</a></p></li></ol></div><p>Written on <time datetime=\"2025-08-02T00:00:00+00:00\">August 2, 2025</time></p></main></div></div>","textContent":"Monte Carlo Crash CourseContinuous ProbabilityExponentially Better IntegrationSamplingCase Study: RenderingQuasi-Monte CarloComing Soon…We’ve learned how to define and apply Monte Carlo integration—fundamentally, it’s the only tool we need.\nIn the remaining chapters, we’ll explore ways to reduce variance and successfully sample difficult distributions.Variance & CorrelationStratified SamplingAdaptive SamplingLatin HypercubeQuasi-Monte CarloLow-Discrepancy SequencesVariance & CorrelationIn chapter two, we determined that the variance of a Monte Carlo estimator is inversely proportional to its sample count.\nEmpirically, we confirmed that our integrators’ expected error scaled with $$\\frac{1}{\\sqrt{N}}$$ in any dimension.Although dramatically faster than exponential, if we want a very accurate result, $$\\frac{1}{\\sqrt{N}}$$ may still be too slow.\nIn practice, we can only scale sample count quadratically so many times.We also assumed that our samples are independent, so their variance is additive.\nHowever, our proof that Monte Carlo integration is unbiased didn’t rely on independence—so what if we relaxed that assumption?\\[\\begin{align*}\n\\mathrm{Var}[X + Y] = \\mathrm{Var}[X] + \\mathrm{Var}[Y] + 2\\mathrm{Cov}[X,Y]\n\\end{align*}\\]If $$X$$ and $$Y$$ are negatively correlated, $$\\mathrm{Cov}[X,Y] < 0$$, decreasing the variance of $$X+Y$$.\nIf we can assure that our samples are negatively correlated, our Monte Carlo estimator might converge faster than $$\\frac{1}{\\sqrt{N}}$$.Poisson Disk SamplingPerceptually, negatively correlated samples look “more random” than uncorrelated samples.That’s because uncorrelated samples often appear in clusters and may leave significant chunks of the domain entirely unsampled.\nNegative correlation causes the opposite behavior: the more samples an area contains, the less likely it is to be sampled, and vice versa.So, how can we generate negatively correlated samples?\nOne approach is rejection sampling: simply discard samples that fall too close to any previous sample.\nThis algorithm is known as Poisson disk sampling.1Poisson disk sampling is useful for pre-generating samples with a minimum separation distance, but isn’t always applicable to Monte Carlo integration, where we need a progressive sampler that eventually covers the entire domain.Stratified SamplingIf we don’t need a minimum separation distance, a faster way to generate negatively correlated samples is stratified sampling.\nStratification will let us combine the strengths of quadrature and Monte Carlo integration.Instead of generating $$N$$ independent samples of the entire domain, a stratified sampler partitions the domain into $$M$$ equal-sized regions and takes $$\\frac{N}{M}$$ independent samples of each one.\nSince no more than $$\\frac{N}{M}$$ samples can occur in any particular region, the samples are negatively correlated.2Let’s consider a Monte Carlo estimator that uses $$N$$ stratified samples of $$\\Omega$$.\nGrouping samples by region lets us rearrange the estimator into a collection of $$\\frac{N}{M}$$-sample estimators for each region $$\\Omega_m$$:\\[\\begin{align*}\nF_\\text{Stratified} &= \\sum_{n=0}^N \\frac{f(\\Omega_n)}{p(\\Omega_n)}\\\\\n&= \\sum_{m=0}^M \\sum_{n=0}^\\frac{N}{M} \\frac{f(\\Omega_{m,n})}{p(\\Omega_{m,n})}\\\\\n&= \\sum_{m=0}^M F_{\\Omega_m}\n\\end{align*}\\]Intuitively, stratification partitions our integral across the regions $$\\Omega_m$$ and computes an independent $$\\frac{N}{M}$$-sample estimate of each term.\nHence, linearity of expectation implies that our stratified estimator is unbiased.\\[\\begin{align*}\n\\mathbb{E}[F_\\text{Stratified}]\n&= \\mathbb{E}[F_{\\Omega_0} + F_{\\Omega_1} + \\dots]\\\\\n&= \\mathbb{E}[F_{\\Omega_0}] + \\mathbb{E}[F_{\\Omega_1}] + \\dots\\\\\n&= \\int_{\\Omega_0} f(\\omega)\\, d\\omega + \\int_{\\Omega_1} f(\\omega)\\, d\\omega + \\dots\\\\\n&= \\int_\\Omega f(\\omega)\\, d\\omega\n\\end{align*}\\]But did stratification reduce variance?\nLet’s try dividing our familiar circular estimator into $$M=64$$ regions:We’ll find that the stratified estimator has fairly low error, especially when $$N$$ is small.3Precisely how much stratification decreases variance depends on the behavior of $$f$$, but we may prove that stratification at least never increases variance.Why Stratify?Let’s compare an $$N$$-sample uniform estimator on $$\\Omega$$ against a stratified estimator that uniformly samples partitions $$A$$ and $$B$$.\nRecalling chapter two, we can compute these estimators’ variances:\\[\\begin{align*}\n\\mathrm{Var}[F_\\text{Uniform}] &= \\vphantom{\\Bigg|}\\frac{|\\Omega|^2}{N}\\mathrm{Var}[f(\\Omega)]\\\\\n\\mathrm{Var}[F_\\text{Stratified}] &= \\mathrm{Var}[F_A] + \\mathrm{Var}[F_B] \\tag{$F_A,F_B$ indep.}\\\\\n&= \\frac{|\\Omega|^2}{2N}\\left(\\vphantom{\\big|}\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]\\right)\n\\end{align*}\\]\\[\\begin{align*}\n\\mathrm{Var}[F_\\text{Uniform}] &= \\vphantom{\\Bigg|}\\frac{|\\Omega|^2}{N}\\mathrm{Var}[f(\\Omega)]\\\\\n\\mathrm{Var}[F_\\text{Stratified}] &= \\mathrm{Var}[F_A] + \\mathrm{Var}[F_B] \\\\&\\tag{$F_A,F_B$ indep.}\\\\\n&= \\frac{|\\Omega|^2}{2N}\\left(\\vphantom{\\big|}\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]\\right)\n\\end{align*}\\]To relate these quantities, we may condition $$\\mathrm{Var}[f(\\Omega)]$$ on the sampled region.\nWe will write $$\\mu_\\mathcal{X}$$ to denote the expected value $$\\mathbb{E}[f(\\mathcal{X})]$$.\\[\\begin{align*}\n\\mathrm{Var}[f(\\Omega)] &= \\mathrm{Var}[f(\\Omega)\\ |\\ A]\\cdot\\mathbb{P}\\{A\\} + \\mathrm{Var}[f(\\Omega)\\ |\\ B]\\cdot\\mathbb{P}\\{B\\}\\\\\n&= \\frac{1}{2}\\left(\\mathbb{E}[(f(\\Omega)-\\mu_\\Omega)^2\\ |\\ A] + \\mathbb{E}[(f(\\Omega)-\\mu_\\Omega)^2\\ |\\ B]\\right)\\\\\n&= \\frac{1}{2}\\left(\\mathbb{E}[f(A)]^2+\\mathbb{E}[f(B)]^2-2\\left(\\frac{\\mu_A+\\mu_B}{2}\\right)^2\\right)\\\\\n&= \\frac{1}{2}\\left(\\mathbb{E}[f(A)]^2-\\mu_A^2+\\mathbb{E}[f(B)]^2-\\mu_B^2+\\frac{1}{2}\\left(\\mu_A^2-2\\mu_A\\mu_B+\\mu_B^2\\right)\\right)\\\\\n&= \\frac{\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]}{2}+\\frac{(\\mu_A-\\mu_B)^2}{4}\\\\\n&\\ge \\frac{\\mathrm{Var}[f(A)]+\\mathrm{Var}[f(B)]}{2}\\\\\n\\end{align*}\\]The squared term is never negative, so we know that $$\\mathrm{Var}[f(A)] + \\mathrm{Var}[f(B)]$$ is at most $$2\\cdot\\mathrm{Var}[f(\\Omega)]$$.\nHence, $$F_\\text{Stratified}$$ cannot have higher variance than $$F_\\text{Uniform}$$, and has lower variance when $$\\mu_A \\neq \\mu_B$$.This result makes some intuitive sense—if $$f$$ has a different average on $$A$$ and $$B$$, samples constrained to $$A$$ or $$B$$ must have locally lower variance than $$f$$ as a whole.Dynamic StratificationSo, should we be using stratified sampling everywhere?\nOften, yes—partitioning the domain can only reduce variance—but note that stratifying across a fixed $$64$$ regions did not reduce variance asymptotically.Even when $$f$$ is sufficiently nice, stratification only reduces error in inverse proportion to the regions’ volume.\nIn a $$d$$-dimensional domain, we should expect our estimator to converge with the following expression:4\\[\\begin{align*}\n\\sigma_\\text{Stratified} &\\propto \\frac{1}{\\sqrt{N}\\sqrt[d]{M}} \\\\ &= N^{-\\frac{1}{2}}M^{-\\frac{1}{d}}\n\\end{align*}\\]That is, $$M$$ is subject to the curse of dimensionality.\nPlus, we can’t use more regions than our sample count, so $$M \\le N$$.\nNonetheless, we can find an algorithmic improvement by dynamically scaling $$M \\propto \\sqrt{N}$$:5\\[\\begin{align*}\n\\sigma_\\text{Stratified} &\\propto N^{-\\frac{1}{2}}{\\sqrt{N}}^{-\\frac{1}{d}} \\\\ &= N^{-\\frac{d+1}{2d}}\n\\end{align*}\\]In one dimension ($$d = 1$$), dynamic stratification produces $$\\sigma \\propto N^{-1}$$.\nBack in chapter two, we used quadrature to integrate $$\\sqrt{x}$$ with error proportional to $$N^{-1}$$.\nUsing dynamic stratification, we get an unbiased estimator with the same convergence rate!In two dimensions, dynamic stratification results in $$\\sigma \\propto N^{-\\frac{3}{4}}$$, which converges faster than naive Monte Carlo, but in higher dimensions, we rapidly approach our existing result of $$\\sigma \\propto N^{-\\frac{1}{2}}$$.\nHence, dynamic stratification is usually only worthwhile in a small number of dimensions.Adaptive SamplingAnother extension of stratified sampling is adaptive sampling.\nInstead of assigning the same number of samples to each region, adaptive sampling uses more samples in regions with higher variance.Above, we determined that the variance of a stratified estimator is a weighted sum, where $$N_A+N_B=N$$:\\[\\begin{align*}\n\\mathrm{Var}[F_\\text{Stratified}] &= \\mathrm{Var}[F_A] + \\mathrm{Var}[F_B]\\\\\n&\\propto \\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B}\n\\end{align*}\\]We also assumed $$N_A = N_B$$, but that wasn’t required to show that our stratified estimator was unbiased.\nSo, if $$\\sigma_A^2>\\sigma_B^2$$, using $$N_A>N_B$$ would decrease the total.\nTo find the optimal sample distribution, we may minimize this sum using a Lagrange multiplier:\\[\\begin{align*}\n&&& \\min_{N_A+N_B=N} \\left\\{\\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B}\\right\\}\\\\\n&\\implies&& \\begin{cases}\n\\frac{\\partial}{\\partial N_A}\\left(\\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B} - \\lambda(N_A+N_B-N)\\right) = 0 \\\\\n\\frac{\\partial}{\\partial N_B}\\left(\\frac{\\sigma_A^2}{N_A} + \\frac{\\sigma_B^2}{N_B} - \\lambda(N_A+N_B-N)\\right) = 0\n\\end{cases}\\\\\n&\\implies&&\nN_A = N\\cdot\\frac{\\sigma_A}{\\sigma_A+\\sigma_B} \\\\\n&&&\nN_B = N\\cdot\\frac{\\sigma_B}{\\sigma_A+\\sigma_B} \\tag{Algebra}\n\\end{align*}\\]As you might expect, we should partition samples between $$F_A$$ and $$F_B$$ in proportion to their standard deviation.\nLet’s attempt to implement this improvement in our circular estimator.\nIntuitively, we should be able to ignore regions where $$f$$ is constant (i.e. $$F$$ has zero deviation):However, we can’t assume to know the standard deviation of each region beforehand, so we must estimate it during integration.\nTo guide adaptive sampling, each region $$\\Omega_m$$ will track its sample count $$N_m$$, as well as estimates of $$\\mathbb{E}[f(\\Omega_m)]$$ and $$\\mathbb{E}[f(\\Omega_m)^2]$$.6\\[\\begin{align*}\n\\sigma_{\\Omega_m} &= \\sqrt{\\mathrm{Var}[F_{\\Omega_m}]} \\\\\n&= \\sqrt{\\frac{\\mathbb{E}[f(\\Omega_m)^2] - \\mathbb{E}[f(\\Omega_m)]^2}{N_m}}\n\\end{align*}\\]Unfortunately, this estimate can be very imprecise, so using it for adaptive sampling isn’t always easy.\nOne approach is to randomly select the next region to sample weighted by estimated deviation (drawn in red):This strategy works, but even when a region has zero estimated deviation, we must choose it with non-zero probability, since the true value may not be zero.\nWe can see this situation play out when our estimator discovers that a mostly-covered region actually has non-zero variance.The core ideas of adaptive sampling and dynamic stratification—assigning samples where they’re needed and progressively refining the sampling pattern—can be combined to form the multi-level Monte Carlo method, which we won’t explore in this chapter.Latin HypercubeAny way we slice it, stratified sampling will eventually run up against the curse of dimensionality.\nIn practice, we may need computationally cheaper methods of generating negatively correlated samples.\nOne such method is known as Latin hypercube sampling.7Instead of attempting to distribute samples across an exponential number of regions, a Latin hypercube sampler stratifies each dimension independently.\nFor example, in two dimensions, we start by generating two lists of one-dimensional samples stratified across $$D$$ regions:We then shuffle $$\\mathbf{x}$$ and $$\\mathbf{y}$$, randomizing their order.\nFinally, to produce samples of $$\\Omega$$, we simply take each pair $$(\\mathbf{x}_i,\\mathbf{y}_i)$$ from the shuffled lists.8\nThis procedure is typically performed in batches of $$D$$ samples.9Since we’re implicitly partitioning $$\\Omega$$ into $$D^d$$ regions, we can think of a Latin hypercube sampler as a sparse approximation of a stratified sampler over this much larger space.\nHowever, perfect stratification would require placing at most $$\\frac{N}{D^d}$$ samples in each region, which our sampler does not achieve.Given any particular region, we know exactly $$\\frac{N}{D}$$ samples have a matching position along each dimension, so up to $$\\frac{N}{D}$$ samples could occur in this region.\nThis bound is exponentially weaker than full stratification, but still leads to negative correlation.Note that regardless of correlation, Latin hypercube samples are uniform, and by similar logic as stratified sampling, may be used for Monte Carlo integration.In our circular estimator, we’ll find that Latin hypercube samples tend to reduce error at small $$N$$, but they’re clearly less effective than stratified samples.\nLatin hypercube samples become more useful when full stratification is infeasible, e.g. in four or more dimensions.The Latin hypercube approach can also be combined with stratified sampling to create multi-level samplers, such as correlated multi-jittered sampling.Quasi-Monte CarloAll of the sampling algorithms we’ve examined so far are fundamentally random, but with additional restrictions that introduce negative correlation.\nAlternatively, if we’re willing to introduce bias, we can do away with randomness entirely!In chapter three, we discussed pseudo-random number generators (PRNGs), which deterministically compute a sequence of samples that “look uniformly random.”\nOur only source of non-determinism was the seed: given the same initial state, a PRNG always produces the same sequence.\nHence, for a fixed seed, a PRNG is just a particular sequence of numbers $$x_i$$—but we can still use it for Monte Carlo integration.\\[\nF_\\text{QMC} = \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n\\]The result is known as a Quasi-Monte Carlo estimator.10\nQMC estimators are biased: they always return the same value, so averaging multiple runs does not increase accuracy.\nFortunately, they are also consistent: increasing the sample count $$N$$ causes the estimator to converge to the exact result.11DiscrepancyEmpirically, we’ve already confirmed that QMC estimators are consistent—most of the interactive figures use a PRNG with a fixed seed.\nBut what is it about pseudo-random sequences that make QMC work?\nLacking the tools of probability, how can we relate the accuracy of an estimator to the quantity and quality of its samples?This is the purpose of the Koksma–Hlawka inequality, which we will present without proof:\\[\n\\Bigg|\\, \\frac{1}{N}\\sum_{i=1}^N f(x_i) - \\int_\\Omega f(\\omega)\\, d\\omega\\, \\Bigg| \\le V(f) D^*_N(x_1,\\dots,x_N)\n\\]Here, $$x_i$$ is our sample sequence, $$f$$ has bounded variation $$V(f)$$, and $$D^*_N$$ is the “star-discrepancy” of $$x$$.\nBounded variation is morally the same constraint that our Monte Carlo estimator for $$f$$ had finite variance.Importantly, our estimator’s error is (at worst) proportional to this property $$D^*_N$$, which only depends on $$x$$.\\[\n\\bigg|\\, F_\\text{QMC} - \\int_\\Omega f(\\omega)\\, d\\omega\\, \\bigg| \\propto D^*_N(x_1,\\dots,x_N)\n\\]Intuitively, we can think of star-discrepancy as a deterministic equivalent of negative correlation.\nTaking $$\\Omega$$ to be the unit square, $$D^*_N$$ is defined as the worst-case difference between the ratio of samples falling inside a rectangle $$\\mathcal{R}$$ and the volume of $$\\mathcal{R}$$.\n$$\\mathcal{R}$$ must also include the origin as its bottom-left corner.12\\[\nD^*_N(x_1,\\dots,x_N) = \\sup_{\\mathcal{R}\\in\\Omega} \\Bigg|\\, \\frac{\\text{Samples}(\\mathcal{R})}{N} - ||\\mathcal{R}|| \\,\\Bigg|\n\\]We can see that discrepancy penalizes regions that have more (or fewer) samples than they should:Uniformly random point sets—as well as those produced by PRNGs—have the property that $$D^*_N$$ tends to zero with increasing sample count.\nHence, using such sequences in a QMC estimator will cause it to converge for all $$f$$ with bounded variation.Low-Discrepancy SequencesTo justify using a biased estimator, it should exhibit a faster convergence rate than unbiased alternatives.\nBut simply fixing a PRNG seed surely doesn’t accelerate convergence, right?\nIndeed, in $$d$$ dimensions, the star-discrepancy of a uniformly random point set only converges with the following expression:\\[\nD^*_N(\\text{Uniform}) \\propto \\sqrt{\\frac{\\log^dN}{N}}\n\\]The Koksma–Hlawka inequality therefore doesn’t tell us anything—we already knew uniform Monte Carlo converges with $$\\frac{1}{\\sqrt{N}}$$.\nHowever, there’s no reason we have to use uniformly random points. Instead, we can make use of certain low-discrepancy sequences, which provide near-linear convergence.\\[\nD^*_N(\\text{Low-Discrepancy}) \\propto \\frac{\\log^dN}{N}\n\\]While this convergence rate is always asymptotically faster than $$\\frac{1}{\\sqrt{N}}$$, in high dimensions, the associated constant factor (which depends on $$d$$) can still make the approach impractical.The Halton SequenceThere are many low-discrepancy sequences, each making use of different mathematical tools.\nOne popular choice is the Halton sequence.\nTo compute a one-dimensional Halton sequence $$g_b(n)$$, first choose a base $$b$$ and find the base-$$b$$ digits of $$n$$:\\[\nn = \\sum_k \\text{Digit}_k(n)b^k\n\\]Then mirror the digits about the decimal place.\nIn base ten, we would have $$g_{10}(1234) = 0.4321$$.\\[\ng_b(n) = \\sum_k \\text{Digit}_k(n)b^{-k-1}\n\\]This operation is also known as the radical inverse $$\\Psi_b(n)$$.\nTo create a $$d$$-dimensional Halton sequence, simply join several one-dimensional sequences with co-prime bases $$b_1\\dots b_d$$.\\[\ng(n) = (g_{b_1}(n),\\dots,g_{b_d}(n))\n\\]Halton sequences exhibit star-discrepancy proportional to $$\\frac{\\log^d N}{N}$$, but proving this result is beyond the scope of this chapter.\nInstead, let’s examine the two-dimensional $$(2,3)$$ Halton sequence:Naturally, we can use Halton samples in our circular estimator:We’ll find that convergence is effectively linear, significantly outpacing our earlier estimators.13Does this result imply we should abandon randomness and use low-discrepancy sequences for everything?\nIn few dimensions, QMC can be ideal—but working with high-dimensional low-discrepancy sequences turns out to be difficult.ScramblingBy definition, a $$d$$-dimensional Halton sequence uses $$d$$ different co-prime bases.\nTo illustrate the difficulties encountered in higher dimensions, let’s consider only the points generated by bases 29 and 31.\\[\ng(n) = (\\dots,g_{29}(n),g_{31}(n),\\dots)\n\\]Note $$g_{29}(n),g_{31}(n)$$ is also the projection of $$g(n)$$ onto the corresponding axes.\nAlthough this sequence is still “low-discrepancy,” its absolute discrepancy starts out quite high:Using sequences like $$(29,31)$$ in QMC estimators can be fraught, since reducing bias to an acceptable level requires many samples.\nThis problem gets worse as we increase dimensionality.Fortunately, there’s another technique (known as scrambling) that makes higher-dimensional bases significantly more usable.\nScrambling introduces an extra step to the radical inverse:\\[\ng_b(n) = \\sum_k \\rho(\\text{Digit}_k(n))b^{-k-1}\n\\]Where $$\\rho$$ is a permutation of the digits $$0\\dots b$$, typically chosen randomly.\nScrambling dramatically reduces discrepancy at low sample counts, making the resulting bias less objectionable.The Halton sequence succinctly exemplifies the benefits and pitfalls of QMC, but it’s certainly not the only useful low-discrepancy sequence.\nIn particular, Sobol’ sequences are widely used in practice, as they can achieve a kind of optimal discrepancy while admitting an efficient implementation.FootnotesNote checking for sample intersection can be done in constant time using a background grid. In fact, generating $$N$$ samples can be done in (more or less) linear work. ↩︎Technically, sample $$i$$ is positively correlated with sample $$i+M$$, but it’s negatively correlated with all of the samples in between, so the overall effect is negative. ↩︎But not when $$N$$ is too small: using fewer than $$M$$ samples would result in a biased estimate. ↩︎Precisely explaining this point is beyond the scope of this chapter, but intuitively, if $$f$$ has bounded variation (and is not a constant), there’s some resolution at which $$f$$ must have a different average in different regions, and this resolution may be required along every dimension. ↩︎To see why $$\\sqrt{N}$$ is a good choice, compute the convergence rate for $$M = \\log N$$ and $$M = N$$. ↩︎Also known as the first and second moments of $$f(\\Omega_m)$$. Interestingly, in stochastic optimization problems (such as inverse rendering), the Adam optimizer naturally estimates these moments, which can be used for adaptive sampling. ↩︎In two dimensions, the Latin hypercube pattern is also known as N-rooks: if each batch of samples were rooks on a chessboard, they wouldn’t threaten each other. ↩︎Joining one sample from each dimension can also be interpreted as picking a vertex of a dimension-$$d$$ length-$$N$$ lattice, leading to the “hypercube” name. The “Latin” part comes from Latin squares. ↩︎Allowing batch sizes linear in $$D$$ is another benefit of Latin hypercube sampling: for full stratification to be unbiased, we must use at least $$D^d$$ samples, which may be infeasible in of itself. ↩︎Technically, QMC also implies the use of a low-discrepancy sequence, since (outside of pedagogy) it’s not particularly useful to consider PRNG-based estimators part of the QMC framework. ↩︎This distinction may sound academic, but it also has computational implications. To increase accuracy, we must use additional samples from the same sequence, so we can’t easily parallelize sample generation. ↩︎In other words, $$D^*_N$$ measures how well $$x_i$$ integrates all possible rectangular indicator functions. ↩︎These error plots should be log-log; they’re difficult to interpret otherwise. Maybe they will be someday. ↩︎Written on August 2, 2025","length":21565,"excerpt":"We’ve learned how to define and apply Monte Carlo integration—fundamentally, it’s the only tool we need.\nIn the remaining chapters, we’ll explore ways to reduce variance and successfully sample difficult distributions.","siteName":null,"publishedTime":null}