{"title":"GitHub - synthetic-lab/octofriend: An open-source coding helper. Very friendly!","byline":"synthetic-lab","dir":null,"lang":"en","content":"<div id=\"readability-page-1\" class=\"page\"><div data-hpc=\"true\"><article itemprop=\"text\"><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://raw.githubusercontent.com/synthetic-lab/octofriend/main/octofriend.png\"><img src=\"https://raw.githubusercontent.com/synthetic-lab/octofriend/main/octofriend.png\" alt=\"octofriend\"></a></p>\n<p dir=\"auto\"><h2 tabindex=\"-1\" dir=\"auto\">Get Started</h2><a id=\"user-content-get-started\" aria-label=\"Permalink: Get Started\" href=\"#get-started\"></a></p>\n<div dir=\"auto\" data-snippet-clipboard-copy-content=\"npm install --global octofriend\"><pre>npm install --global octofriend</pre></div>\n<p dir=\"auto\">And then:</p>\n<div dir=\"auto\" data-snippet-clipboard-copy-content=\"octofriend\"><pre>octofriend</pre></div>\n<p dir=\"auto\"><h2 tabindex=\"-1\" dir=\"auto\">About</h2><a id=\"user-content-about\" aria-label=\"Permalink: About\" href=\"#about\"></a></p>\n<p dir=\"auto\">Octo is a small, helpful, cephalopod-flavored coding assistant that works with\nany OpenAI-compatible or Anthropic-compatible LLM API, and allows you to switch\nmodels at will mid-conversation when a particular model gets stuck. Octo can\noptionally use (and we recommend using) ML models we custom-trained and\nopen-sourced (<a href=\"https://huggingface.co/syntheticlab/diff-apply\" rel=\"nofollow\">1</a>,\n<a href=\"https://huggingface.co/syntheticlab/fix-json\" rel=\"nofollow\">2</a>) to automatically handle tool\ncall and code edit failures from the main coding models you're working with:\nthe autofix models work with any coding LLM. Octo wants to help you because\nOcto is your friend.</p>\n<p dir=\"auto\">Octo works great with GPT-5, Claude 4, GLM-4.5, and Kimi K2 (although you can\nuse it with pretty much anything!). Correctly handling multi-turn responses,\nespecially with thinking models like GPT-5 and Claude 4 (whose content may\neven be encrypted), can be tricky. Octo carefully manages thinking tokens to\nensure it's always as smart as it can be. We think it's the best multi-LLM tool\nout there at managing thinking tokens, and you'll feel how much smarter it is.</p>\n<p dir=\"auto\">Octo has zero telemetry. Using Octo with a privacy-focused LLM provider (may we\nselfishly recommend <a href=\"https://synthetic.new/\" rel=\"nofollow\">Synthetic</a>?) means your code stays\nyours. But you can also use it with any OpenAI-compatible API provider, with\nAnthropic, or with local LLMs you run on your own machine.</p>\n<p dir=\"auto\">Octo has helped write some of its own source code, but the codebase is\nhuman-first: Octo is meant to be a friendly little helper rather than a\ncompletely hands-free author, and that's how I use it. But if you want to live\ndangerously, you can always run <code>octofriend --unchained</code>, and skip all tool and\nedit confirmations.</p>\n<p dir=\"auto\"><h2 tabindex=\"-1\" dir=\"auto\">Demo</h2><a id=\"user-content-demo\" aria-label=\"Permalink: Demo\" href=\"#demo\"></a></p>\n<p dir=\"auto\"><a href=\"https://asciinema.org/a/728456\" rel=\"nofollow\"><img src=\"https://raw.githubusercontent.com/synthetic-lab/octofriend/main/octo-asciicast.svg\" alt=\"Octo asciicast\"></a></p>\n<p dir=\"auto\"><h2 tabindex=\"-1\" dir=\"auto\">Rules</h2><a id=\"user-content-rules\" aria-label=\"Permalink: Rules\" href=\"#rules\"></a></p>\n<p dir=\"auto\">Octo will look for instruction files named like so:</p>\n<ul dir=\"auto\">\n<li><code>OCTO.md</code></li>\n<li><code>CLAUDE.md</code></li>\n<li><code>AGENTS.md</code></li>\n</ul>\n<p dir=\"auto\">Octo uses the <em>first</em> one of those it finds: so if you want to have different\ninstructions for Octo than for Claude, just have an <code>OCTO.md</code> and a\n<code>CLAUDE.md</code>, and Octo will ignore your <code>CLAUDE.md</code>.</p>\n<p dir=\"auto\">Octo will search the current directory for rules, and every parent directory,\nup until (inclusive of) your home directory. All rule files will be merged: so\nif you want project-specific rules as well as general rules to apply\neverywhere, you can add an <code>OCTO.md</code> to your project, as well as a global\n<code>OCTO.md</code> in your home directory.</p>\n<p dir=\"auto\">If you don't want to clutter your home directory, you can also add a global\nrules file in <code>~/.config/octofriend/OCTO.md</code>.</p>\n<p dir=\"auto\"><h2 tabindex=\"-1\" dir=\"auto\">Connecting Octo to MCP servers</h2><a id=\"user-content-connecting-octo-to-mcp-servers\" aria-label=\"Permalink: Connecting Octo to MCP servers\" href=\"#connecting-octo-to-mcp-servers\"></a></p>\n<p dir=\"auto\">Octo can do a lot out of the box — pretty much anything is possible with enough\nBash — but if you want access to rich data from an MCP server, it'll help Octo\nout a lot to just provide the MCP server directly instead of trying to contort\nits tentacles into crafting the right Bash-isms. After you run <code>octofriend</code> for\nthe first time, you'll end up with a config file in\n<code>~/.config/octofriend/octofriend.json5</code>. To hook Octo up to your favorite MCP\nserver, add the following to the config file:</p>\n<div dir=\"auto\" data-snippet-clipboard-copy-content=\"mcpServers: {\n  serverName: {\n    command: &quot;command-string&quot;,\n    arguments: [\n      &quot;arguments&quot;,\n      &quot;to&quot;,\n      &quot;pass&quot;,\n    ],\n  },\n},\"><pre><span>mcpServers</span>: <span>{</span>\n  <span>serverName</span>: <span>{</span>\n    <span>command</span>: <span>\"command-string\"</span><span>,</span>\n    <span>arguments</span>: <span>[</span>\n      <span>\"arguments\"</span><span>,</span>\n      <span>\"to\"</span><span>,</span>\n      <span>\"pass\"</span><span>,</span>\n    <span>]</span><span>,</span>\n  <span>}</span><span>,</span>\n<span>}</span><span>,</span></pre></div>\n<p dir=\"auto\">For example, to plug Octo into your Linear workspace:</p>\n<div dir=\"auto\" data-snippet-clipboard-copy-content=\"mcpServers: {\n  linear: {\n    command: &quot;npx&quot;,\n    arguments: [ &quot;-y&quot;, &quot;mcp-remote&quot;, &quot;https://mcp.linear.app/sse&quot; ],\n  },\n},\"><pre><span>mcpServers</span>: <span>{</span>\n  <span>linear</span>: <span>{</span>\n    <span>command</span>: <span>\"npx\"</span><span>,</span>\n    <span>arguments</span>: <span>[</span> <span>\"-y\"</span><span>,</span> <span>\"mcp-remote\"</span><span>,</span> <span>\"https://mcp.linear.app/sse\"</span> <span>]</span><span>,</span>\n  <span>}</span><span>,</span>\n<span>}</span><span>,</span></pre></div>\n</article></div></div>","textContent":"\nGet Started\nnpm install --global octofriend\nAnd then:\noctofriend\nAbout\nOcto is a small, helpful, cephalopod-flavored coding assistant that works with\nany OpenAI-compatible or Anthropic-compatible LLM API, and allows you to switch\nmodels at will mid-conversation when a particular model gets stuck. Octo can\noptionally use (and we recommend using) ML models we custom-trained and\nopen-sourced (1,\n2) to automatically handle tool\ncall and code edit failures from the main coding models you're working with:\nthe autofix models work with any coding LLM. Octo wants to help you because\nOcto is your friend.\nOcto works great with GPT-5, Claude 4, GLM-4.5, and Kimi K2 (although you can\nuse it with pretty much anything!). Correctly handling multi-turn responses,\nespecially with thinking models like GPT-5 and Claude 4 (whose content may\neven be encrypted), can be tricky. Octo carefully manages thinking tokens to\nensure it's always as smart as it can be. We think it's the best multi-LLM tool\nout there at managing thinking tokens, and you'll feel how much smarter it is.\nOcto has zero telemetry. Using Octo with a privacy-focused LLM provider (may we\nselfishly recommend Synthetic?) means your code stays\nyours. But you can also use it with any OpenAI-compatible API provider, with\nAnthropic, or with local LLMs you run on your own machine.\nOcto has helped write some of its own source code, but the codebase is\nhuman-first: Octo is meant to be a friendly little helper rather than a\ncompletely hands-free author, and that's how I use it. But if you want to live\ndangerously, you can always run octofriend --unchained, and skip all tool and\nedit confirmations.\nDemo\n\nRules\nOcto will look for instruction files named like so:\n\nOCTO.md\nCLAUDE.md\nAGENTS.md\n\nOcto uses the first one of those it finds: so if you want to have different\ninstructions for Octo than for Claude, just have an OCTO.md and a\nCLAUDE.md, and Octo will ignore your CLAUDE.md.\nOcto will search the current directory for rules, and every parent directory,\nup until (inclusive of) your home directory. All rule files will be merged: so\nif you want project-specific rules as well as general rules to apply\neverywhere, you can add an OCTO.md to your project, as well as a global\nOCTO.md in your home directory.\nIf you don't want to clutter your home directory, you can also add a global\nrules file in ~/.config/octofriend/OCTO.md.\nConnecting Octo to MCP servers\nOcto can do a lot out of the box — pretty much anything is possible with enough\nBash — but if you want access to rich data from an MCP server, it'll help Octo\nout a lot to just provide the MCP server directly instead of trying to contort\nits tentacles into crafting the right Bash-isms. After you run octofriend for\nthe first time, you'll end up with a config file in\n~/.config/octofriend/octofriend.json5. To hook Octo up to your favorite MCP\nserver, add the following to the config file:\nmcpServers: {\n  serverName: {\n    command: \"command-string\",\n    arguments: [\n      \"arguments\",\n      \"to\",\n      \"pass\",\n    ],\n  },\n},\nFor example, to plug Octo into your Linear workspace:\nmcpServers: {\n  linear: {\n    command: \"npx\",\n    arguments: [ \"-y\", \"mcp-remote\", \"https://mcp.linear.app/sse\" ],\n  },\n},\n","length":3229,"excerpt":"An open-source coding helper. Very friendly! Contribute to synthetic-lab/octofriend development by creating an account on GitHub.","siteName":"GitHub","publishedTime":null}